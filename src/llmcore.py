import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from matplotlib import pyplot as plt
import time
import pandas as pd
import urllib.request
import os
from collections import OrderedDict

def get_device(force_cpu=False):
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„è®¾å¤‡"""
    if force_cpu:
        device = torch.device("cpu")
        print("å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = torch.device("cpu")
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
    return device

def get_default_config(force_cpu=False):
    """è·å–é»˜è®¤é…ç½®"""
    device = get_device(force_cpu=force_cpu)
    return {
        'batch_size': 32, # ä¸€æ¬¡batchä¼šæœ‰å¤šå°‘ä¸ªéšæœºç‚¹
        'context_window': 16, # æ»‘åŠ¨çª—å£é‡‡æ ·ï¼Œè®¾ç½®é‡‡æ ·å¤§å°
        'vocab_size': 4325, # å’±ä»¬çš„è¥¿æ¸¸è®°æ•°æ®é›†ï¼Œä¸€å…±åŒ…å«4325ä¸ªä¸é‡å¤çš„æ±‰å­—ï¼Œæ ‡ç‚¹ç¬¦å·
        'd_model': 128, #æ¨¡å‹ä¸º128ç»´çš„embedding
        'epochs': 5000, # è®­ç»ƒè½®æ¬¡ - å‡å°‘åˆ°5000è½®ï¼Œé…åˆæ—©åœæœºåˆ¶
        'log_interval': 10, # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡log
        'n_heads': 8, # 32ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œæˆ‘ä»¬æ¥8ä¸ªå§
        'n_layers': 4, # æ ¹æ®ä¼ å…¥çš„å †å å±‚æ•°ï¼Œåˆ›å»ºLlamaåŠŸèƒ½å—ï¼Œæ³¨æ„OrderedDictä¸ºä¸€ç§ç‰¹æ®Šç±»å‹çš„å­—å…¸æ•°æ®ï¼Œä¿ç•™å­—å…¸å†™å…¥çš„é¡ºåºï¼Œå…ˆæ’å…¥çš„æ•°æ®åœ¨å‰ï¼Œåæ’å…¥çš„æ•°æ®åœ¨åã€‚
        # è¿™é‡Œï¼Œæˆ‘ä»¬å°†llamaçš„åŠŸèƒ½å—å †å 4å±‚
        'device': device, # è®¡ç®—è®¾å¤‡
    }

def download_and_prepare_data(data_file="xiyouji.txt", force_download=False):
    """ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†"""
    if not os.path.exists(data_file) or force_download:
        print(f"æ­£åœ¨ä¸‹è½½æ•°æ®é›†åˆ° {data_file}...")
        url = "https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt"
        urllib.request.urlretrieve(url, data_file)
        print("æ•°æ®é›†ä¸‹è½½å®Œæˆ")
    
    # è¯»å–æ•°æ®
    with open(data_file, 'r', encoding='utf-8') as f:
        lines = f.read()
    
    # åˆ›å»ºè¯è¡¨
    vocab = sorted(list(set(lines)))
    print(f'è¯è¡¨å¤§å°: {len(vocab)}')
    
    # åˆ›å»ºç¼–ç æ˜ å°„
    itos = {i: ch for i, ch in enumerate(vocab)}
    stoi = {ch: i for i, ch in enumerate(vocab)}
    
    # ç¼–ç æ•´ä¸ªæ•°æ®é›†
    dataset = torch.tensor([stoi[ch] for ch in lines], dtype=torch.int16)
    print(f'æ•°æ®é›†å½¢çŠ¶: {dataset.shape}')
    
    return dataset, vocab, itos, stoi

def encode_text(text, stoi):
    """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—"""
    return [stoi[ch] for ch in text]

def decode_text(indices, itos):
    """è§£ç æ•°å­—åºåˆ—ä¸ºæ–‡æœ¬"""
    return ''.join([itos[i] for i in indices])

# æ„å»ºbatch
def get_batches(data, split, batch_size, context_window, device=None):
    # åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸ºï¼Œè®­ç»ƒ80%ï¼ŒéªŒè¯10%ï¼Œæµ‹è¯•10%
    train = data[:int(0.8 * len(data))]
    val = data[int(0.8 * len(data)): int(0.9 * len(data))]
    test = data[int(0.9 * len(data)):]

    # å°†å…¨éƒ¨çš„è®­ç»ƒæ•°æ®ä½œä¸ºbatchï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ä¹Ÿæ¢ä¸ªå˜é‡å­˜å‚¨ï¼ˆå•çº¯ä¸ºäº†æ–¹ä¾¿çœ‹ï¼‰
    batch_data = train
    if split == 'val':
        batch_data = val
    if split == 'test':
        batch_data = test

    # å¦‚æœä½¿ç”¨GPUï¼Œå…ˆå°†æ•°æ®ç§»åŠ¨åˆ°GPUä¸Šï¼Œé¿å…é‡å¤ä¼ è¾“
    if device is not None and device.type == 'cuda' and batch_data.device != device:
        batch_data = batch_data.to(device)

    # åœ¨GPUä¸Šç”Ÿæˆéšæœºç´¢å¼•ï¼Œé¿å…CPUåˆ°GPUçš„ä¼ è¾“
    device_for_randint = device if device is not None else torch.device('cpu')
    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,), device=device_for_randint)

    # ä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡åŒ–ç´¢å¼•æ–¹å¼ï¼Œé¿å…Pythonå¾ªç¯
    # åˆ›å»ºç´¢å¼•çŸ©é˜µï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰batchæ•°æ®
    batch_indices = ix.unsqueeze(1) + torch.arange(context_window, device=ix.device).unsqueeze(0)
    target_indices = batch_indices + 1
    
    x = batch_data[batch_indices].long()
    y = batch_data[target_indices].long()

    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if device is not None:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)

    # è¿”å›ç‰¹å¾å€¼ï¼Œç›®æ ‡å€¼
    return x, y



# æ„é€ ä¸€ä¸ªè¯„ä¼°å‡½æ•°
@torch.no_grad()
def evaluate_loss(model, dataset, config):
    # è¯„ä¼°ç»“æœå­˜å‚¨å˜é‡
    out = {}

    # å°†æ¨¡å‹ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # åˆ†åˆ«ä¼šåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†é‡Œé€šè¿‡get_batchs()å‡½æ•°å–è¯„ä¼°æ•°æ®
    for split in ["train", "val"]:

        losses = []

        # è¯„ä¼°10ä¸ªbatch
        for _ in range(10):
            # æ‹¿åˆ°ç‰¹å¾å€¼ï¼ˆè¾“å…¥æ•°æ®ï¼‰ï¼Œä»¥åŠç›®æ ‡å€¼ï¼ˆè¾“å‡ºæ•°æ®ï¼‰
            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'], config.get('device'))

            # æŠŠæ‹¿åˆ°çš„æ•°æ®ä¸¢è¿›æ¨¡å‹ï¼Œå¾—åˆ°losså€¼
            _, loss = model(xb, yb)

            # æ›´æ–°losså­˜å‚¨
            losses.append(loss.item())

        # è¿™é‡Œå°±æ˜¯å¤§å®¶ç»å¸¸åœ¨æ§åˆ¶å°çœ‹åˆ°çš„ "train_loss"  "valid_loss"ç”±æ¥
        out[split] = np.mean(losses)

    # è¯„ä¼°å®Œäº†ï¼Œåˆ«å¿˜äº†æŠŠæ¨¡å‹å†ç½®å›è®­ç»ƒçŠ¶æ€ï¼Œä¸‹ä¸€ä¸ªepochè¿˜è¦ç»§ç»­è®­ç»ƒå‘¢
    model.train()

    return out



# åˆ›å»ºä¸€ä¸ªAdamä¼˜åŒ–å™¨ï¼ŒåŸºç¡€çŸ¥è¯†ï¼Œ
# optimizer = torch.optim.Adam(
#     model.parameters(),      # ä¼˜åŒ–å™¨æ‰§è¡Œä¼˜åŒ–å…¨éƒ¨çš„æ¨¡å‹å‚æ•°
# )

# æ„å»ºè®­ç»ƒå‡½æ•°
def train(model, optimizer, dataset, config, scheduler=None, print_logs=False):
    # losså­˜å‚¨
    losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 50  # æ—©åœè€å¿ƒå€¼

    # è®­ç»ƒæ—¶é—´è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # å¾ªç¯è®­ç»ƒæŒ‡å®šepochçš„è½®æ•°
    for epoch in range(config['epochs']):
        # ä¼˜åŒ–å™¨è¦åˆå§‹åŒ–å•Šï¼Œå¦åˆ™æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯åŸºäºä¸Šä¸€æ¬¡è®­ç»ƒç»“æœè¿›è¡Œä¼˜åŒ–ï¼Œæ•ˆæœç”šå¾®
        optimizer.zero_grad()

        # è·å–è®­ç»ƒæ•°æ®
        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'], config.get('device'))

        # å‰å‘ä¼ æ’­è®¡ç®—æ¦‚ç‡çŸ©é˜µä¸loss
        logits, loss = model(xs, targets=ys)

        # åå‘ä¼ æ’­æ›´æ–°æƒé‡å‚æ•°ï¼Œæ›´æ–°å­¦ä¹ ç‡ä¼˜åŒ–å™¨
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()

        # å¦‚æœæä¾›å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé‚£ä¹ˆå­¦ä¹ ç‡ä¼šé€šè¿‡è°ƒåº¦å™¨è¿›è¡Œä¿®æ”¹
        if scheduler:
            # æ¯ä¸ªbatchéƒ½æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

        # æ‰“å°log
        if epoch % config['log_interval'] == 0:
            # è®­ç»ƒæ—¶é—´
            batch_time = time.time() - start_time

            # æ‰§è¡Œè¯„ä¼°å‡½æ•°ï¼Œåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè®¡ç®—loss
            x = evaluate_loss(model, dataset, config)

            # Store the validation loss
            losses += [x]
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']

            # æ‰“å°è¿›åº¦æ—¥å¿—
            if print_logs:
                print(f"Epoch {epoch:5d} | train loss {x['train']:.4f} | val loss {x['val']:.4f} | lr {current_lr:.2e} | Time {batch_time:.2f}s | ETA {batch_time * (config['epochs'] - epoch)/config['log_interval']:.1f}s")

            # å°†éªŒè¯lossä¼ é€’ç»™è°ƒåº¦å™¨è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´
            if scheduler and hasattr(scheduler, 'step'):
                # å¯¹äºè‡ªå®šä¹‰è°ƒåº¦å™¨ï¼Œä¼ é€’éªŒè¯loss
                if hasattr(scheduler, 'plateau_patience'):
                    scheduler.step(val_loss=x['val'])
            
            # æ—©åœæœºåˆ¶
            if x['val'] < best_val_loss:
                best_val_loss = x['val']
                patience_counter = 0
                if print_logs:
                    print(f"  âœ“ æ–°çš„æœ€ä½³éªŒè¯loss: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= max_patience:
                    print(f"  âš  æ—©åœè§¦å‘ï¼éªŒè¯lossè¿ç»­{max_patience}è½®æœªæ”¹å–„")
                    break

            # é‡ç½®å¼€å§‹æ—¶é—´ï¼Œç”¨äºè®¡ç®—ä¸‹ä¸€è½®çš„è®­ç»ƒæ—¶é—´
            start_time = time.time()

    # ä¸Šé¢æ‰€æœ‰epochè®­ç»ƒç»“æŸï¼Œæ‰“å°æœ€ç»ˆçš„ç»“æœ
    print(f"\n=== è®­ç»ƒç»“æŸ ===")
    print(f"æœ€ä½³éªŒè¯loss: {best_val_loss:.4f}")
    print(f"æœ€ç»ˆéªŒè¯loss: {losses[-1]['val']:.4f}")

    # è¿”è¿˜æ¯ä¸€æ­¥losså€¼çš„åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬è¦ç”»å›¾ï¼Œè¿”è¿˜çš„æ˜¯lossè¿­ä»£çš„å›¾åƒ
    return pd.DataFrame(losses).plot()

# å¯åŠ¨è®­ç»ƒ
# train(model, optimizer)

# æ¨ç†å‡½æ•°
def generate(model, itos, config, max_new_tokens=20):
    # ç”Ÿæˆéšæœºæ•°ï¼Œä½œä¸ºè¾“å…¥æ•°æ®,5è¡Œä¸€åˆ—ï¼Œä»£è¡¨è¾“å…¥5ä¸ªå­—ç¬¦ã€‚ è¿™ä¸ªåœ°æ–¹å¯ä»¥è‡ªè¡Œæ›¿æ¢å…¶ä»–éšæœºæ•°æµ‹è¯•ã€‚
    device = config.get('device', torch.device('cpu'))
    idx = torch.randint(1, 4000, (5, 1)).long().to(device)
    print("éšæœºç”Ÿæˆçš„å­—ç¬¦:", [itos[i.item()] for i in idx])
    print(idx[:, -config['context_window']:])
    for _ in range(max_new_tokens):
        # å› ä¸ºæ¨ç†çš„æ—¶å€™ï¼Œä¾èµ–åé¢çš„nä¸ªtokenï¼Œæ‰€ä»¥æ»‘åŠ¨çª—å£è¦ä»åå¾€å‰é€‰æ‹©è¾“å…¥æ•°æ®çš„å€’æ•°å‡ ä¸ªtokenï¼Œè¿™ä¸ªæ˜¯è¶…è¿‡å­—ç¬¦æ•°é‡ä¼šå¯¹è¾“å…¥è¿›è¡Œæˆªæ–­ï¼Œåªé€‰å–æœ€åå‡ ä¸ªtokenï¼šidx[:, -config['context_window']:]
        logits = model(idx[:, -config['context_window']:])
        # print(logits.size())
        # å¾—åˆ°æ¨¡å‹è¾“å‡ºçš„ç»“æœï¼Œè¿›è¡Œè§£ç ï¼Œè¿™é‡Œlogits[:, -1, :]æŒºæŠ½è±¡çš„ï¼Œå®é™…ä¸Šç¬¬ä¸€ç»´åº¦æ˜¯è¾“å…¥çš„å­—ç¬¦æ•°ï¼Œç¬¬äºŒç»´åº¦æ˜¯æ—¶é—´æ­¥ï¼Œç¬¬ä¸‰ç»´åº¦æ˜¯è¯è¡¨
        # å³ï¼Œå¯¹æ¯ä¸€æ­¥çš„è§£ç ç»“æœï¼Œå–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„æ•°æ®ï¼Œä½œä¸ºè¾“å‡ºçš„æ•°æ®ã€‚è§£ç çš„è¿‡ç¨‹æ˜¯ç¬¬ä¸€æ¬¡è§£ç ï¼Œè¾“å…¥5ä¸ªtokenï¼Œç¬¬äºŒæ¬¡è§£ç ä¾èµ–çš„æ˜¯åŸæ¥5ä¸ªtokençš„æœ€å4ä¸ªï¼ŒåŠ ä¸Šä¸Šä¸€æ­¥è§£ç ç”Ÿæˆçš„ä¸€ä¸ªï¼Œä¹Ÿæ˜¯5ä¸ªtokenï¼Œå¦‚æ­¤å¾ªç¯ã€‚
        last_time_step_logits = logits[:, -1, :]
        # print('last_time_step_logits')
        # print(last_time_step_logits.shape)
        # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
        p = F.softmax(last_time_step_logits, dim=-1)
        # print('p_shape')
        # print(p.shape)
        # æ ¹æ®æ¦‚ç‡åˆ†å¸ƒè®¡ç®—ä¸‹ä¸€ä¸ªtokenï¼Œè¿™é‡Œä½¿ç”¨ torch.multinomialåšçš„æ˜¯éšæœºé‡‡æ ·
        idx_next = torch.multinomial(p, num_samples=1)
        # print('idx_next_shape')
        # print(idx_next.shape)
        # å°†æ–°çš„idxé€šè¿‡å¼ é‡æ‹¼æ¥å†™å…¥åˆ°è§£ç åºåˆ—ä¸­
        idx = torch.cat([idx, idx_next], dim=-1)
    # ä½¿ç”¨ä¹‹å‰å®šä¹‰çš„è§£ç å‡½æ•°ï¼Œå°†IDè½¬æ¢ä¸ºæ±‰å­—ï¼Œæˆ‘ä»¬å¾—åˆ°çš„5è¡Œ21åˆ—çš„æ•°æ®ï¼Œæ¥æºäºæ¯ä¸€ä¸ªè¾“å…¥å­—ç¬¦ä½œä¸ºå¼€å§‹ä½ç½®ï¼Œç”Ÿæˆ20ä¸ªå­—ç¬¦ã€‚ å› ä¸º5ä¸ªè¾“å…¥éƒ½æ˜¯0ï¼Œåœ¨è¯è¡¨ä¸­ç¼–å·ä¸º0çš„æ•°æ®æ˜¯'\n'ã€‚
    print(idx.shape)
    return [decode_text(x, itos) for x in idx.tolist()]

class RMSNorm(nn.Module):
    def __init__(self, layer_shape, eps=1e-8, bias=False):
        super(RMSNorm, self).__init__()

        # torchä¸­register_parameter()åŠŸèƒ½ä¸ºï¼šå‘æˆ‘ä»¬å»ºç«‹çš„ç½‘ç»œmoduleæ·»åŠ parameter
        # å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹pytorchå®˜æ–¹å°è£…å¥½çš„RMSNormåŠŸèƒ½æ¨¡å—æ·»åŠ ä¸€ä¸ªå¯ä»¥è®­ç»ƒå‚æ•°çš„å±‚ï¼Œå‘½åä¸ºscaleï¼Œå¹¶åˆå§‹åŒ–ä¸ºå½¢çŠ¶ä¸ºlayer_shapeï¼Œæ‰€æœ‰å€¼ä¸º1çš„å¼ é‡çŸ©é˜µã€‚
        self.register_parameter("scale", nn.Parameter(torch.ones(layer_shape)))

    def forward(self, x):
        # è®¡ç®—FrobeniusèŒƒæ•°ï¼ˆçƒæŸä¸ªçŸ©é˜µä¸­æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œå†å¼€æ–¹å¾—åˆ°ï¼Œè¯¥èŒƒæ•°ç”¨æ¥è¡¡é‡çŸ©é˜µçš„å¤§å°ï¼Œè¯¦æƒ…è¯·ç™¾åº¦ï¼‰, RMS = 1/sqrt(N) * Frobenius
        # å…·ä½“æ¥è¯´ï¼Œtorch.linalg.norm(x, dim=(1, 2))è®¡ç®—äº†xåœ¨ç¬¬1å’Œç¬¬2ç»´åº¦ä¸Šçš„èŒƒæ•°ã€‚ç„¶åï¼Œå°†ç»“æœä¹˜ä»¥x[0].numel() ** -.5ã€‚x[0].numel()è¡¨ç¤ºxç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå³xçš„ç¬¬ä¸€è¡Œï¼‰çš„å…ƒç´ ä¸ªæ•°ï¼Œ** -.5è¡¨ç¤ºæ±‚å¹³æ–¹æ ¹çš„å€’æ•°ã€‚
        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5
        # print(ff_rms.shape)
        # å°†ff_rmsç®—å­åº”ç”¨äºè¾“å…¥çš„å¼ é‡xï¼Œä¾æ®å…¬å¼ï¼Œåšé™¤æ³•ï¼Œå› ä¸ºè¾“å…¥å‘é‡xæ˜¯ä¸‰ç»´çš„ï¼Œå› æ­¤éœ€è¦å¯¹ff_rmsè¿›è¡Œå‡ä¸¤ç»´ï¼Œä¹Ÿå˜æˆä¸‰ç»´çš„å¼ é‡ã€‚è¿™æ ·å¯ä»¥è¿›è¡Œå…ƒç´ ä¹‹é—´çš„è®¡ç®—ã€‚
        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)
        # print(raw.shape)
        # è¿”å›scaleç¼©æ”¾åå½’ä¸€åŒ–çš„å¼ é‡
        # print(self.scale[:x.shape[1], :].unsqueeze(0) * raw)
        return self.scale[:x.shape[1], :].unsqueeze(0) * raw

def get_rotary_matrix(context_window, embedding_dim, device=None):
    # åˆå§‹åŒ–ä¸€ä¸ª0å¡«å……ï¼Œå½¢çŠ¶ä¸ºï¼ˆcontext_window, embedding_dim, embedding_dimï¼‰çš„å¼ é‡çŸ©é˜µï¼Œå…¶ä¸­context_windowä¸ºtokenæ•°é‡ï¼Œåé¢ä¸¤ä¸ªembedding_dimç»„æˆæ­£æ–¹å½¢çŸ©é˜µï¼Œä¸åé¢çš„attentionè®¡ç®—å¯¹é½æ ¼å¼
    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False, device=device)

    # éå†æ¯ä¸€ä¸ªä½ç½®çš„token
    for position in range(context_window):
        # è¿˜è®°å¾—æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­è¯´çš„ï¼Œå¯¹äºç‰¹å¾ï¼Œä¸¤ä¸¤ç»„åˆå—ï¼Œå› æ­¤éœ€è¦å¾ªç¯çš„æ¬¡æ•°ä¸ºembedding_dimé™¤ä»¥2
        for i in range(embedding_dim // 2):
            # è®¾ç½®Î¸å€¼ï¼Œé‡‡æ ·é¢‘ç‡ï¼Œæˆ–è€…è¯´æ—‹è½¬é¢‘ç‡ï¼Œæ—‹è½¬è§’éƒ½å¯ä»¥ï¼Œé™¤ä»¥embedding_dimé˜²æ­¢æ¢¯åº¦é—®é¢˜ã€‚
            theta = 10000. ** (-2. * (i - 1) / embedding_dim)
            # æ ¹æ®æ¬§æ‹‰å…¬å¼ï¼Œè®¡ç®—æ—‹è½¬çš„è§’åº¦ï¼Œåˆ†åˆ«æœ‰sin å’Œcosï¼Œå°†è®¡ç®—æ‹‰åˆ°å¤æ•°ç©ºé—´ï¼Œå¹¶å°†æ—‹è½¬è§’åº¦åº”ç”¨åœ¨ä¸Šé¢çš„0å¡«å……çš„çŸ©é˜µ
            m_theta = position * theta
            R[position, 2 * i, 2 * i] = np.cos(m_theta)
            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
            # å¾—åˆ°çš„ç»“æœæ˜¯æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µï¼Œåˆ°è¿™é‡Œè¿˜æ²¡è¦†ç›–åˆ°attention
    return R

# æ­¤ä¸ºå•å¤´æ³¨æ„åŠ›æœºåˆ¶
class RoPEMaskedAttentionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # è®¡ç®—Qæƒé‡çŸ©é˜µ
        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # è®¡ç®—Kæƒé‡çŸ©é˜µ
        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # è®¡ç®—Væƒé‡çŸ©é˜µ
        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)
        
        self.context_window = config['context_window']
        self.embedding_dim = config['d_model']
        
        # é¢„è®¡ç®—å¹¶ç¼“å­˜æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µ - ä¸ä½œä¸ºæ¨¡å‹å‚æ•°ä¿å­˜
        self.R_cache = None
        self.cache_device = None

    def _get_rotary_matrix(self, device):
        """è·å–æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è®¡ç®—"""
        # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–è®¾å¤‡ä¸åŒ¹é…ï¼Œé‡æ–°åˆ›å»º
        if self.R_cache is None or self.cache_device != device:
            R = torch.zeros((self.context_window, self.embedding_dim, self.embedding_dim), 
                          requires_grad=False, device=device)
            
            # éå†æ¯ä¸€ä¸ªä½ç½®çš„token
            for position in range(self.context_window):
                # è®¡ç®—æ—‹è½¬è§’åº¦
                for i in range(self.embedding_dim // 2):
                    theta = 10000. ** (-2. * (i - 1) / self.embedding_dim)
                    m_theta = position * theta
                    R[position, 2 * i, 2 * i] = np.cos(m_theta)
                    R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
                    R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
                    R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
            
            # ç¼“å­˜çŸ©é˜µå’Œè®¾å¤‡ä¿¡æ¯
            self.R_cache = R
            self.cache_device = device
            
        return self.R_cache

    def forward(self, x, return_attn_weights=False):
        # å‰å‘ä¼ æ’­æ—¶ï¼Œè¾“å…¥çŸ©é˜µçš„å½¢çŠ¶ä¸º(batch, sequence length, dimension)
        b, m, d = x.shape  # batch size, sequence length, dimension

        # çº¿æ€§å˜æ¢Q,K,V
        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)

        # è·å–ç¼“å­˜çš„æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µ
        R = self._get_rotary_matrix(x.device)
        
        # å°†æ—‹è½¬ä½ç½®ç¼–ç åº”ç”¨äºQå’ŒKï¼Œåªä½¿ç”¨éœ€è¦çš„éƒ¨åˆ†
        q_rotated = (torch.bmm(q.transpose(0, 1), R[:m])).transpose(0, 1)
        k_rotated = (torch.bmm(k.transpose(0, 1), R[:m])).transpose(0, 1)

        # å¯¹æ³¨æ„åŠ›æœºåˆ¶ç‚¹ç§¯è¿›è¡Œç­‰æ¯”ä¾‹ç¼©æ”¾
        activations = F.scaled_dot_product_attention(
            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True
        )
        
        # å¦‚æœéœ€è¦è¿”å›æ³¨æ„åŠ›æƒé‡
        if return_attn_weights:
            # ç”Ÿæˆä¸Šä¸‰è§’å¸ƒå°”æ©ç ï¼ˆæœªæ¥ä½ç½®ä¸ºTrueï¼‰
            attn_mask = torch.triu(torch.ones(m, m, device=q_rotated.device), diagonal=1).bool()
            # è®¡ç®—åŸå§‹æ³¨æ„åŠ›åˆ†æ•°
            attn_scores = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d)
            # é®ç›–æœªæ¥ä½ç½®ï¼ˆè®¾ä¸º-1e9ï¼‰
            attn_scores = attn_scores.masked_fill(attn_mask, -1e9)
            # Softmaxå½’ä¸€åŒ–
            attn_weights = F.softmax(attn_scores, dim=-1)
            return activations, attn_weights

        return activations

# å•å¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°å®Œæ¯•ï¼Œä¸‹é¢å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
class RoPEMaskedMultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´å¯¹è±¡æ„å»ºå®Œæ¯•äº†ï¼Œå¤šå¤´çš„ï¼Œé¦–å…ˆå¤šæ¬¡åˆ›å»ºè¿™ä¸ªå¯¹è±¡ã€‚ç”Ÿæˆå¤šä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œå¡åˆ°ä¸€ä¸ªåˆ—è¡¨é‡Œã€‚
        self.heads = nn.ModuleList([
            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])
        ])
        # åœ¨æ¨¡å‹ç»“æ„ä¸Šï¼Œåˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚ï¼ˆéšè—å±‚ï¼‰ï¼Œç”¨äºçº¿å‹è¾“å‡ºæ³¨æ„åŠ›æœºåˆ¶å¤´è¾“å‡ºçš„å¼ é‡çŸ©é˜µï¼Œå¯»æ‰¾å¤šå¤´ä¹‹é—´çš„ç‰¹å¾ï¼Œä½†æ˜¯æ›´ä¸»è¦çš„æ˜¯ï¼Œxç»è¿‡å¤šå¤´è®¡ç®—åå½¢çŠ¶æ”¹å˜äº†ï¼Œåˆ›å»ºçº¿æ€§å±‚ï¼Œè®©å¼ é‡çŸ©é˜µå˜å›åŸæ¥è¾“å…¥çš„å½¢çŠ¶ã€‚
        # åŒæ—¶ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨éšæœºç¥ç»å…ƒå¤±æ´»ï¼Œæ¯”ç‡0.1
        # çº¿æ€§å±‚è¾“å…¥å½¢çŠ¶ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ï¼Œä¹˜ä»¥çŸ©é˜µçš„ç»´åº¦ï¼Œå…³è”åˆ°ä¿ºçš„ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œå°±æ˜¯keyçŸ©é˜µï¼Œåœ¨å¤šå¤´ä¹‹é—´å…±äº«æƒé‡ï¼Œå‡å°‘è®¡ç®—çš„æ€ç»´ã€‚ è¾“å‡ºä¸ºï¼šæ¨¡å‹çš„embeddingç»´åº¦æ•°
        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        # è¾“å…¥çŸ©é˜µå½¢çŠ¶xï¼š (batch, sequence length, dimension)

        # æ¯ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œéƒ½ä¼ å…¥Xè¿›è¡Œè®¡ç®—ã€‚ï¼ˆè¿™ä¸ªåœ°æ–¹å¼€å¯å¹¶è¡Œæ‰§è¡Œä¼šä¸ä¼šå¿«ä¸€äº›ï¼Œä½†æ˜¯ä¸çŸ¥é“pytorchæ˜¯ä¸æ˜¯è‡ªåŠ¨è°ƒç”¨å¹¶è¡Œï¼‰
        heads = [h(x) for h in self.heads]  # ä¸è¿”å›æ³¨æ„åŠ›æƒé‡ï¼Œåªè¿”å›æ¿€æ´»å€¼
        # è¾“å…¥å¼ é‡xç»è¿‡å¤šä¸ªå¤´è®¡ç®—attentionï¼ˆåŒæ—¶ï¼Œattentionæ˜¯å·²ç»è¦†ç›–äº†RoPEçš„ï¼‰ï¼Œé‡æ–°æ‹¼æ¥æˆæ–°çš„çŸ©é˜µï¼Œé‡æ–°æ”¾å…¥å˜é‡xã€‚åˆ°è¿™é‡Œä½ åº”è¯¥è§‰å¾—ï¼šé‚£çŸ©é˜µå½¢çŠ¶ä¸å°±å˜äº†å—
        x = torch.cat(heads, dim=-1)

        # è¿™ä¸ï¼Œçº¿æ€§å±‚çš„ä½œç”¨æ¥äº†
        x = self.linear(x)

        # éšæœºå¤±æ´»ä¸€ä¸‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        x = self.dropout(x)
        return x

# Llama 32ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œæˆ‘ä»¬æ¥8ä¸ªå§


# æˆ‘ä»¬å·²ç»åˆ›å»ºå®Œäº†æ‰€éœ€è¦çš„ç®—å­ï¼Œ  ç°åœ¨ç§¯æœ¨å·²åˆ›å»ºå®Œæ¯•ï¼Œå°†è¿™äº›ç§¯æœ¨ç»„åˆèµ·æ¥ï¼ï¼ï¼ï¼
class RopeModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        # Embeddingå±‚
        self.embedding = nn.Embedding(config['vocab_size'], config['d_model'])

        # RMSNormå±‚
        self.rms = RMSNorm((config['context_window'], config['d_model']))

        # æ—‹è½¬ä½ç½®ç¼–ç å™¨+æ³¨æ„åŠ›æœºåˆ¶
        self.rope_attention = RoPEMaskedMultiheadAttention(config)

        # çº¿æ€§å±‚+æ¿€æ´»å‡½æ•°å˜ä¸ºéçº¿æ€§è¾“å‡ºï¼
        self.linear = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            nn.ReLU(),
        )

        # æœ€ç»ˆçš„è¾“å‡ºï¼Œå› ä¸ºéœ€è¦è§£ç ï¼Œå› ä¸ºè¾“å‡ºçš„ç»´åº¦ä¸è¯è¡¨å¤§å°ç»Ÿä¸€ï¼ï¼ï¼
        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])

        print("model params:", sum([m.numel() for m in self.parameters()]))
    # å‰å‘ä¼ æ’­
    def forward(self, idx, targets=None):
        # embeddingï¼Œä¸è§£é‡Š
        x = self.embedding(idx)
        # å½’ä¸€åŒ–æ•°å€¼ï¼Œä¸è§£é‡Š
        x = self.rms(x)
        # ç›¸åŠ ï¼Œè§£é‡Šä¸€ä¸‹ï¼Œå› ä¸ºattentionæ˜¯è¦è¦†ç›–åˆ°åŸçŸ©é˜µçš„ï¼Œæƒ³è±¡ä¸¤ä¸ªå½¢çŠ¶ä¸€æ ·çš„çŸ©é˜µä¸ºä¸¤å¼ çº¸ï¼Œå·¦æ‰‹ä¸€å¼ çº¸ï¼Œå³æ‰‹ä¸€å¼ çº¸ï¼ŒåŒæ‰‹åˆåï¼Œå•ªï¼è¦†ç›–ã€‚ ä½¿ç”¨åŠ ç®—ï¼Œå°±æ˜¯å°†ä¸¤ä¸ªçŸ©é˜µä¸­çš„å…ƒç´ æŒ‰ä½ç½®ç›¸åŠ ï¼ç›´æ¥è¦†ç›–å€¼ï¼
        x = x + self.rope_attention(x)
        # å†å½’ä¸€åŒ–ï¼
        x = self.rms(x)
        # å› ä¸ºç›´æ¥è®¡ç®—å½’ä¸€åŒ–çš„æ•°å€¼å¯èƒ½å‡ºç°æ¢¯åº¦é—®é¢˜ï¼Œå› æ­¤æŠŠå½’ä¸€åŒ–çš„å€¼ä½œä¸ºä¿®æ­£ç³»æ•°ï¼Œå†è¦†ç›–ï¼
        x = x + self.linear(x)
        # åˆ°è¿™é‡Œï¼Œæ‰æ˜¯æœ€ç»ˆè¾“å‡ºvocabæ•°é‡çš„ç¥ç»å…ƒè¾“å‡ºï¼ï¼ï¼ï¼ï¼ï¼
        logits = self.last_linear(x)

        # è®­ç»ƒé˜¶æ®µæœ‰ç›®æ ‡å€¼
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss
        # éªŒè¯æˆ–è€…æ¨ç†é˜¶æ®µï¼Œç›®æ ‡å€¼yæ²¡æœ‰ï¼åªæœ‰ç»“æœï¼Œæ²¡æœ‰lossï¼
        else:
            return logits


class SwiGLU(nn.Module):

    def __init__(self, size):
        super().__init__()
        # å®šä¹‰ä¸€ä¸ªé—¨æ§çš„çº¿æ€§å±‚ï¼Œè¾“å…¥è¾“å‡ºéƒ½æ˜¯é—¨æ§ç»“æ„çš„å°ºå¯¸
        self.linear_gate = nn.Linear(size, size)
        # é—¨æ§ç»“æ„ä¸»å¹²çº¿æ€§å±‚
        self.linear = nn.Linear(size, size)
        # åˆå§‹åŒ–ä¸€ä¸ªéšæœºæ•°ä½œä¸ºbetaç³»æ•°
        self.beta = torch.randn(1, requires_grad=True)

        # nn.Parameterç”¨äºæŒ‡å®šæŸä¸€å±‚å‚æ•°ä¸ºå¯å­¦ä¹ çš„ï¼Œå³æœ¬æ¥ä¸èƒ½é€šè¿‡è®­ç»ƒæ›´æ”¹å‚æ•°ï¼Œç°åœ¨å˜æˆäº†å¯ä»¥ç»è¿‡è®­ç»ƒæ¥æ›´æ–°çš„å‚æ•°ã€‚
        self.beta = nn.Parameter(torch.ones(1))
        # å°†éšæœºæ•°betaæŒ‡å®šä¸ºä¸€ä¸ªåä¸ºbetaçš„ç¥ç»ç½‘ç»œå±‚
        self.register_parameter("beta", self.beta)

    def forward(self, x):
        # Swishé—¨æ§ä½†æ„¿çš„è®¡ç®—ï¼šï¼ˆä»æ‹¬å·é‡Œå¼€å§‹ï¼‰å¯¹äºåŸå§‹è¾“å…¥çš„æ•°æ®å¼ é‡ï¼Œç»è¿‡çº¿æ€§å˜æ¢ä¹˜ä»¥betaç³»æ•°ï¼Œå†ç»è¿‡sigmoidå˜æ¢ä¸º0-1ä¹‹é—´çš„å€¼ï¼Œå†ä¹˜ä»¥åŸæ•°æ®ç»è¿‡é—¨æ§çº¿æ€§å˜æ¢ã€‚æ€»çš„æ¥è¯´ï¼Œçº¿å‹è¾“å‡ºç»è¿‡éçº¿æ€§å˜æ¢ï¼Œå†åº”ç”¨åˆ°çº¿æ€§å˜æ¢çš„ç»“æœï¼Œå…ƒç´ æŒ‰ä½ç½®ç›¸ä¹˜ï¼Œä¿®æ­£åŸæœ¬æ•°æ®å¼ é‡ï¼Œå°±æ˜¯è¿™ä¸ªé—¨æ§ç»“æ„åšçš„äº‹æƒ…ã€‚
        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))
        # å°†é—¨æ§ç»“æ„è¾“å‡ºçš„å€¼å†æŒ‰ä½ä¹˜ä»¥çº¿å‹è¾“å‡ºçš„åŸæ•°æ®å¼ é‡
        # ä¸ºå•¥è¿™ä¹ˆåšï¼Œæˆ‘ä¸çŸ¥é“ï¼Œä½†æ˜¯è®ºæ–‡å¤ç°çš„ä»£ç å°±æ˜¯è¿™æ ·æ»´ï¼Œæœ‰å…´è¶£å¯ä»¥ç ”ç©¶ä¸€ä¸‹ï¼Œæˆ‘æ²¡ç ”ç©¶è¿‡ã€‚
        out = swish_gate * self.linear(x)
        return out


# OKï¼ ç°åœ¨æˆ‘ä»¬æ›´æ–°ä¸€ä¸‹ï¼Œéšè—å±‚ç»´åº¦å †å å¤šå°‘å±‚ï¼Œæˆ‘ä»¬å…ˆæ¥4å±‚å°å°å’¸æ·¡ï¼ï¼ï¼ï¼

# ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰çš„ç®—å­ï¼ŒRMSï¼ŒROPE,SWIGLUï¼Œæˆ‘ä»¬æ­å»ºæˆ‘ä»¬çš„LlaMaï¼ é¦–å…ˆå®ç°LlaMaçš„åŠŸèƒ½å—ï¼Œç„¶åå †å ã€‚
# åŠŸèƒ½æ²¡ä»€ä¹ˆå¥½è®²çš„ï¼Œå¦‚æœä»”ç»†çœ‹åˆ°äº†è¿™é‡Œï¼Œä¸‹é¢çš„æ¯ä¸€è¡Œä»£ç éƒ½éš¾ä¸ä½ä½ ã€‚
class LlamaBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.rms = RMSNorm((config['context_window'], config['d_model']))
        self.attention = RoPEMaskedMultiheadAttention(config)
        self.feedforward = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
        )

    def forward(self, x):

        x = self.rms(x)
        x = x + self.attention(x)

        x = self.rms(x)
        x = x + self.feedforward(x)
        return x

# ç°åœ¨ï¼Œæˆ‘ä»¬ç»„è£…LlaMa
class Llama(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # Embeddingä¸è§£é‡Š
        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])
        # æ ¹æ®ä¼ å…¥çš„å †å å±‚æ•°ï¼Œåˆ›å»ºLlamaåŠŸèƒ½å—ï¼Œæ³¨æ„OrderedDictä¸ºä¸€ç§ç‰¹æ®Šç±»å‹çš„å­—å…¸æ•°æ®ï¼Œä¿ç•™å­—å…¸å†™å…¥çš„é¡ºåºï¼Œå…ˆæ’å…¥çš„æ•°æ®åœ¨å‰ï¼Œåæ’å…¥çš„æ•°æ®åœ¨åã€‚
        # è¿™é‡Œï¼Œæˆ‘ä»¬å°†llamaçš„åŠŸèƒ½å—å †å 4å±‚
        self.llama_blocks = nn.Sequential(
            OrderedDict([(f"llama_{i}", LlamaBlock(config)) for i in range(config['n_layers'])])
        )
        # FFNå±‚ï¼ŒåŒ…å«ï¼šçº¿æ€§å±‚ã€æ¿€æ´»å‡½æ•°éçº¿æ€§å˜æ¢ã€å†ç”¨çº¿æ€§å±‚è¾“å‡ºæœ€ç»ˆè§£ç æ•°å€¼ã€‚
        self.ffn = nn.Sequential(
            nn.Linear(config['d_model'], config['d_model']),
            SwiGLU(config['d_model']),
            nn.Linear(config['d_model'], config['vocab_size']),
        )

        # çœ‹çœ‹å’±ä»¬çš„å¤§æ¨¡å‹å¤šå°‘å‚æ•°ï¼
        print("model params:", sum([m.numel() for m in self.parameters()]))

    def forward(self, idx, targets=None):
        # embeddingåµŒå…¥
        x = self.embeddings(idx)
        # Llamaæ¨¡å‹è®¡ç®—
        x = self.llama_blocks(x)
        # FFNè®¡ç®—ï¼Œå¾—åˆ°logits
        logits = self.ffn(x)

        # æ¨ç†é˜¶æ®µæ²¡æœ‰ç›®æ ‡å€¼ï¼Œåªè¾“å‡ºç»“æœ
        if targets is None:
            return logits
        # è®­ç»ƒé˜¶æ®µï¼Œæœ‰ç›®æ ‡å€¼ï¼Œéœ€è¦è¾“å‡ºç»“æœï¼Œä»¥åŠlossï¼Œç”¨äºåå‘ä¼ æ’­æ›´æ–°æƒé‡ï¼
        else:
            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))
            return logits, loss

# ä¾¿åˆ©å‡½æ•°
def create_model(config=None):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹"""
    if config is None:
        config = get_default_config()
    
    # å¦‚æœä½¿ç”¨GPUï¼Œé¢„åˆ†é…å†…å­˜
    if config.get('device', torch.device('cpu')).type == 'cuda':
        print("é¢„åˆ†é…GPUå†…å­˜...")
        # åˆ›å»ºä¸€ä¸ªå¤§çš„ä¸´æ—¶å¼ é‡æ¥é¢„åˆ†é…å†…å­˜
        temp_tensor = torch.randn(1000, 1000, device=config['device'])
        del temp_tensor
        torch.cuda.empty_cache()
        print("GPUå†…å­˜é¢„åˆ†é…å®Œæˆ")
    
    
    model = Llama(config)
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
    if 'device' in config:
        model = model.to(config['device'])
        print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {config['device']}")
    return model

def create_optimizer(model, lr=3e-4, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-8):
    """åˆ›å»ºä¼˜åŒ–å™¨ - ä½¿ç”¨æ›´é€‚åˆå¤§æ¨¡å‹çš„å­¦ä¹ ç‡"""
    # return torch.optim.AdamW(  # ä½¿ç”¨AdamWï¼Œå¯¹æƒé‡è¡°å‡å¤„ç†æ›´å¥½
    #     model.parameters(),
    #     lr=lr,
    #     betas=betas,
    #     weight_decay=weight_decay,
    #     eps=eps
    # )
    return torch.optim.Adam(model.parameters())

class WarmupCosineScheduler:
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŒ…å«è‡ªé€‚åº”è°ƒæ•´åŠŸèƒ½"""
    def __init__(self, optimizer, warmup_steps=100, max_steps=10000, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_steps = max_steps
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_step = 0
        
        # è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
        self.plateau_patience = 10  # lossåœæ»å®¹å¿æ­¥æ•°
        self.plateau_factor = 0.5   # lossåœæ»æ—¶çš„å­¦ä¹ ç‡è¡°å‡å› å­
        self.best_loss = float('inf')
        self.plateau_count = 0
        
    def step(self, val_loss=None):
        self.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
        if val_loss is not None:
            if val_loss < self.best_loss:
                self.best_loss = val_loss
                self.plateau_count = 0
            else:
                self.plateau_count += 1
                
            # å¦‚æœlossåœæ»ï¼Œé™ä½å­¦ä¹ ç‡
            if self.plateau_count >= self.plateau_patience:
                self.base_lr *= self.plateau_factor
                self.plateau_count = 0
                print(f"  ğŸ“‰ æ£€æµ‹åˆ°lossåœæ»ï¼Œå­¦ä¹ ç‡é™ä½è‡³: {self.base_lr:.2e}")
        
        if self.current_step <= self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿
            lr = self.base_lr * (self.current_step / self.warmup_steps)
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            progress = (self.current_step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
            progress = min(progress, 1.0)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_lr(self):
        return [group['lr'] for group in self.optimizer.param_groups]
    
    def state_dict(self):
        """è¿”å›è°ƒåº¦å™¨çš„çŠ¶æ€å­—å…¸"""
        return {
            'current_step': self.current_step,
            'base_lr': self.base_lr,
            'best_loss': self.best_loss,
            'plateau_count': self.plateau_count,
            'warmup_steps': self.warmup_steps,
            'max_steps': self.max_steps,
            'min_lr': self.min_lr,
            'plateau_patience': self.plateau_patience,
            'plateau_factor': self.plateau_factor
        }
    
    def load_state_dict(self, state_dict):
        """åŠ è½½è°ƒåº¦å™¨çš„çŠ¶æ€å­—å…¸"""
        self.current_step = state_dict['current_step']
        self.base_lr = state_dict['base_lr']
        self.best_loss = state_dict['best_loss']
        self.plateau_count = state_dict['plateau_count']
        self.warmup_steps = state_dict['warmup_steps']
        self.max_steps = state_dict['max_steps']
        self.min_lr = state_dict['min_lr']
        self.plateau_patience = state_dict['plateau_patience']
        self.plateau_factor = state_dict['plateau_factor']

def create_scheduler(optimizer, warmup_steps=100, max_steps=10000, min_lr=1e-6):
    """åˆ›å»ºå¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    return WarmupCosineScheduler(optimizer, warmup_steps, max_steps, min_lr)

def initialize_training_environment(data_file="xiyouji.txt", config=None):
    """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒï¼Œè¿”å›æ‰€æœ‰å¿…è¦çš„ç»„ä»¶"""
    if config is None:
        config = get_default_config()
    
    # å‡†å¤‡æ•°æ®
    dataset, vocab, itos, stoi = download_and_prepare_data(data_file)
    
    # æ›´æ–°é…ç½®ä¸­çš„è¯è¡¨å¤§å°
    config['vocab_size'] = len(vocab)
    
    # å¦‚æœä½¿ç”¨GPUï¼Œå°†æ•°æ®é›†ç§»åŠ¨åˆ°GPUä¸Šä»¥é¿å…é‡å¤ä¼ è¾“
    device = config.get('device', torch.device('cpu'))
    if device.type == 'cuda':
        print(f"å°†æ•°æ®é›†ç§»åŠ¨åˆ°GPU: {device}")
        dataset = dataset.to(device)
        # é¢„çƒ­GPUï¼Œå‡å°‘é¦–æ¬¡è¿è¡Œçš„å¼€é”€
        torch.cuda.synchronize()
        print("GPUé¢„çƒ­å®Œæˆ")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    
    return {
        'model': model,
        'dataset': dataset,
        'vocab': vocab,
        'itos': itos,
        'stoi': stoi,
        'config': config
    }