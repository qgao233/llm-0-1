#!/usr/bin/env python3
"""
é…ç½®ç®¡ç†å™¨æ¨¡å—
ç”¨äºç®¡ç†0.1Bæ¨¡å‹çš„å„ç§é…ç½®ï¼ŒåŒ…æ‹¬é¢„è®­ç»ƒã€å¾®è°ƒã€æ¨ç†ç­‰é…ç½®
æä¾›é…ç½®éªŒè¯ã€é»˜è®¤å€¼è®¾ç½®ã€è®­ç»ƒæ­¥æ•°è®¡ç®—ç­‰åŠŸèƒ½
"""

import os
import yaml
import logging
from typing import Dict, Tuple, Optional, Any
from pathlib import Path

logger = logging.getLogger(__name__)


class BaseConfig:
    """åŸºç¡€é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str):
        self.config_path = config_path
        self.config = self.load_config()
        
    def load_config(self) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def save_config(self, save_path: Optional[str] = None):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        save_path = save_path or self.config_path
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            logger.info(f"âœ… é…ç½®æ–‡ä»¶å·²ä¿å­˜: {save_path}")
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
            raise
    
    def get(self, key: str, default: Any = None) -> Any:
        """è·å–é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„åµŒå¥—é”®"""
        keys = key.split('.')
        value = self.config
        
        for k in keys:
            if isinstance(value, dict) and k in value:
                value = value[k]
            else:
                return default
        
        return value
    
    def set(self, key: str, value: Any):
        """è®¾ç½®é…ç½®å€¼ï¼Œæ”¯æŒç‚¹å·åˆ†éš”çš„åµŒå¥—é”®"""
        keys = key.split('.')
        config = self.config
        
        for k in keys[:-1]:
            if k not in config:
                config[k] = {}
            config = config[k]
        
        config[keys[-1]] = value


class PretrainingConfig(BaseConfig):
    """é¢„è®­ç»ƒé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str):
        super().__init__(config_path)
        self.validate_config()
        
    def validate_config(self):
        """éªŒè¯é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§"""
        required_sections = ['model', 'training', 'data', 'checkpoint', 'hardware']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„èŠ‚: {section}")
        
        # éªŒè¯å¹¶è®¾ç½®è®­ç»ƒé…ç½®é»˜è®¤å€¼
        self._validate_training_config()
        
        # éªŒè¯å¹¶è®¾ç½®æ¨¡å‹é…ç½®é»˜è®¤å€¼
        self._validate_model_config()
        
        # éªŒè¯å¹¶è®¾ç½®æ•°æ®é…ç½®é»˜è®¤å€¼
        self._validate_data_config()
        
        # éªŒè¯å¹¶è®¾ç½®ç¡¬ä»¶é…ç½®é»˜è®¤å€¼
        self._validate_hardware_config()
        
        # æ‰“å°é…ç½®æ‘˜è¦
        self._print_config_summary()
        
    def _validate_training_config(self):
        """éªŒè¯å¹¶è®¾ç½®è®­ç»ƒé…ç½®é»˜è®¤å€¼"""
        training_config = self.config['training']
        
        # å¿…éœ€çš„è®­ç»ƒé…ç½®é”®
        required_keys = ['batch_size', 'learning_rate']
        for key in required_keys:
            if key not in training_config:
                raise ValueError(f"è®­ç»ƒé…ç½®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
        
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            'num_epochs': 3,
            'shards_per_epoch': 5,
            'weight_decay': 0.01,
            'warmup_steps': 1000,
            'max_steps': 50000,
            'optimizer': 'adamw',
            'beta1': 0.9,
            'beta2': 0.95,
            'eps': 1e-8,
            'lr_scheduler': 'cosine',
            'min_lr_ratio': 0.1,
            'gradient_clip_val': 1.0,
            'accumulate_grad_batches': 1,
            'use_amp': True,
            'log_every_n_steps': 100
        }
        
        for key, default_value in defaults.items():
            if key not in training_config:
                training_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®è®­ç»ƒé…ç½®é»˜è®¤å€¼: {key} = {default_value}")
    
    def _validate_model_config(self):
        """éªŒè¯å¹¶è®¾ç½®æ¨¡å‹é…ç½®é»˜è®¤å€¼"""
        model_config = self.config['model']
        
        # å¿…éœ€çš„æ¨¡å‹é…ç½®é”®
        required_keys = ['d_model', 'n_layers', 'n_heads', 'context_window']
        for key in required_keys:
            if key not in model_config:
                raise ValueError(f"æ¨¡å‹é…ç½®ç¼ºå°‘å¿…éœ€çš„é”®: {key}")
        
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            'ffn_dim': model_config['d_model'] * 4,
            'dropout': 0.1,
            'tie_weights': True,
            'max_position_embeddings': model_config['context_window']
        }
        
        for key, default_value in defaults.items():
            if key not in model_config:
                model_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®æ¨¡å‹é…ç½®é»˜è®¤å€¼: {key} = {default_value}")
    
    def _validate_data_config(self):
        """éªŒè¯å¹¶è®¾ç½®æ•°æ®é…ç½®é»˜è®¤å€¼"""
        data_config = self.config['data']
        
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            'max_seq_length': self.config['model']['context_window'],
            'pad_token_id': 0,
            'eos_token_id': 2,
            'use_bpe_tokenizer': True,
            'vocab_cache_path': 'chinese_tokenizer_vocab_25k.json'
        }
        
        for key, default_value in defaults.items():
            if key not in data_config:
                data_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®æ•°æ®é…ç½®é»˜è®¤å€¼: {key} = {default_value}")
    
    def _validate_hardware_config(self):
        """éªŒè¯å¹¶è®¾ç½®ç¡¬ä»¶é…ç½®é»˜è®¤å€¼"""
        hardware_config = self.config['hardware']
        
        # è®¾ç½®é»˜è®¤å€¼
        defaults = {
            'device': 'auto',
            'multi_gpu': False,
            'gradient_checkpointing': True,
            'dataloader_num_workers': 4,
            'pin_memory': True
        }
        
        for key, default_value in defaults.items():
            if key not in hardware_config:
                hardware_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®ç¡¬ä»¶é…ç½®é»˜è®¤å€¼: {key} = {default_value}")
    
    def _print_config_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        training_config = self.config['training']
        model_config = self.config['model']
        
        logger.info(f"ğŸ“Š é…ç½®æ‘˜è¦:")
        logger.info(f"  æ¨¡å‹é…ç½®:")
        logger.info(f"    - d_model: {model_config['d_model']}")
        logger.info(f"    - n_layers: {model_config['n_layers']}")
        logger.info(f"    - n_heads: {model_config['n_heads']}")
        logger.info(f"    - context_window: {model_config['context_window']}")
        logger.info(f"    - ffn_dim: {model_config['ffn_dim']}")
        
        logger.info(f"  è®­ç»ƒé…ç½®:")
        logger.info(f"    - æ€»è½®æ•°: {training_config['num_epochs']}")
        logger.info(f"    - æ¯è½®åˆ†ç‰‡æ•°: {training_config['shards_per_epoch']}")
        logger.info(f"    - æ‰¹æ¬¡å¤§å°: {training_config['batch_size']}")
        logger.info(f"    - å­¦ä¹ ç‡: {training_config['learning_rate']}")
        logger.info(f"    - ä¼˜åŒ–å™¨: {training_config['optimizer']}")
        
    def calculate_training_steps(self, dataset_size: int) -> Tuple[int, int]:
        """æ ¹æ®æ•°æ®é›†å¤§å°è®¡ç®—è®­ç»ƒæ­¥æ•°"""
        training_config = self.config['training']
        batch_size = training_config['batch_size']
        shards_per_epoch = training_config['shards_per_epoch']
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç‰‡çš„æ ·æœ¬æ•°ï¼ˆæ•°æ®é›†æ€»å¤§å°é™¤ä»¥åˆ†ç‰‡æ•°ï¼‰
        samples_per_shard = dataset_size // shards_per_epoch
        
        # è®¡ç®—æ¯ä¸ªåˆ†ç‰‡çš„æ­¥æ•°ï¼ˆåˆ†ç‰‡æ ·æœ¬æ•°é™¤ä»¥æ‰¹æ¬¡å¤§å°ï¼‰
        steps_per_shard = max(1, samples_per_shard // batch_size)
        
        # è®¡ç®—æ¯è½®çš„æ­¥æ•°ï¼ˆæ¯åˆ†ç‰‡æ­¥æ•°ä¹˜ä»¥åˆ†ç‰‡æ•°ï¼‰
        steps_per_epoch = steps_per_shard * shards_per_epoch
        
        # æ›´æ–°é…ç½®
        training_config['steps_per_shard'] = steps_per_shard
        training_config['steps_per_epoch'] = steps_per_epoch
        training_config['samples_per_shard'] = samples_per_shard
        
        # æ›´æ–°max_stepsä¸ºæ€»è®­ç»ƒæ­¥æ•°
        total_steps = steps_per_epoch * training_config['num_epochs']
        training_config['max_steps'] = total_steps
        
        logger.info(f"ğŸ“Š è®­ç»ƒæ­¥æ•°è®¡ç®—ç»“æœ:")
        logger.info(f"  æ•°æ®é›†å¤§å°: {dataset_size:,}")
        logger.info(f"  æ¯åˆ†ç‰‡æ ·æœ¬æ•°: {samples_per_shard:,}")
        logger.info(f"  æ¯åˆ†ç‰‡æ­¥æ•°: {steps_per_shard:,}")
        logger.info(f"  æ¯è½®æ­¥æ•°: {steps_per_epoch:,}")
        logger.info(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_steps:,}")
        
        return steps_per_shard, steps_per_epoch
    
    def calculate_model_parameters(self) -> Dict[str, int]:
        """è®¡ç®—æ¨¡å‹å‚æ•°é‡"""
        model_config = self.config['model']
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œvocab_sizeå¯èƒ½åŠ¨æ€ç¡®å®š
        vocab_size = model_config.get('vocab_size', 25000)  # é»˜è®¤25k
        d_model = model_config['d_model']
        n_layers = model_config['n_layers']
        n_heads = model_config['n_heads']
        ffn_dim = model_config['ffn_dim']
        tie_weights = model_config['tie_weights']
        
        # è®¡ç®—å„éƒ¨åˆ†å‚æ•°é‡
        embedding_params = vocab_size * d_model
        
        # æ¯å±‚Transformerçš„å‚æ•°é‡
        # æ³¨æ„åŠ›å±‚: 3ä¸ªçº¿æ€§å±‚(Q,K,V) + è¾“å‡ºæŠ•å½±å±‚
        attention_params_per_layer = 4 * d_model * d_model
        # FFNå±‚: 2ä¸ªçº¿æ€§å±‚
        ffn_params_per_layer = 2 * d_model * ffn_dim
        # LayerNormå‚æ•°ï¼ˆRMSNormåªæœ‰scaleå‚æ•°ï¼‰
        norm_params_per_layer = 2 * d_model
        
        transformer_params_per_layer = attention_params_per_layer + ffn_params_per_layer + norm_params_per_layer
        total_transformer_params = n_layers * transformer_params_per_layer
        
        # è¾“å‡ºå±‚å‚æ•°
        output_params = 0 if tie_weights else vocab_size * d_model
        
        # æœ€ç»ˆLayerNorm
        final_norm_params = d_model
        
        # æ€»å‚æ•°é‡
        total_params = embedding_params + total_transformer_params + output_params + final_norm_params
        
        param_info = {
            'embedding_params': embedding_params,
            'transformer_params': total_transformer_params,
            'output_params': output_params,
            'norm_params': final_norm_params,
            'total_params': total_params,
            'vocab_size': vocab_size,
            'tie_weights': tie_weights
        }
        
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡ä¼°ç®—:")
        logger.info(f"  - Embeddingå±‚: {embedding_params:,} å‚æ•°")
        logger.info(f"  - Transformerå±‚: {total_transformer_params:,} å‚æ•°")
        logger.info(f"  - è¾“å‡ºå±‚: {output_params:,} å‚æ•° (tie_weights={tie_weights})")
        logger.info(f"  - å½’ä¸€åŒ–å±‚: {final_norm_params:,} å‚æ•°")
        logger.info(f"  - æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        logger.info(f"  - æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB (FP32)")
        
        return param_info


class FineTuningConfig(BaseConfig):
    """å¾®è°ƒé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str):
        super().__init__(config_path)
        self.validate_config()
        
    def validate_config(self):
        """éªŒè¯å¾®è°ƒé…ç½®"""
        required_sections = ['model', 'training', 'data', 'checkpoint']
        for section in required_sections:
            if section not in self.config:
                raise ValueError(f"å¾®è°ƒé…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„èŠ‚: {section}")
        
        # è®¾ç½®å¾®è°ƒç‰¹æœ‰çš„é»˜è®¤å€¼
        training_config = self.config['training']
        
        # å¾®è°ƒé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡å’Œè¾ƒå°‘çš„è½®æ•°
        defaults = {
            'num_epochs': 1,
            'learning_rate': 1e-5,  # å¾®è°ƒä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
            'weight_decay': 0.01,
            'warmup_steps': 500,
            'gradient_clip_val': 1.0,
            'use_amp': True
        }
        
        for key, default_value in defaults.items():
            if key not in training_config:
                training_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®å¾®è°ƒé…ç½®é»˜è®¤å€¼: {key} = {default_value}")


class InferenceConfig(BaseConfig):
    """æ¨ç†é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str):
        super().__init__(config_path)
        self.validate_config()
        
    def validate_config(self):
        """éªŒè¯æ¨ç†é…ç½®"""
        # æ¨ç†é…ç½®ç›¸å¯¹ç®€å•ï¼Œä¸»è¦éªŒè¯å¿…éœ€çš„æ¨¡å‹å’Œæ¨ç†å‚æ•°
        if 'model' not in self.config:
            raise ValueError("æ¨ç†é…ç½®æ–‡ä»¶ç¼ºå°‘æ¨¡å‹é…ç½®èŠ‚")
        
        # è®¾ç½®æ¨ç†é»˜è®¤å‚æ•°
        if 'inference' not in self.config:
            self.config['inference'] = {}
        
        inference_config = self.config['inference']
        defaults = {
            'max_new_tokens': 256,
            'temperature': 0.8,
            'top_k': 40,
            'top_p': 0.9,
            'do_sample': True,
            'use_kv_cache': True
        }
        
        for key, default_value in defaults.items():
            if key not in inference_config:
                inference_config[key] = default_value
                logger.info(f"ğŸ“ è®¾ç½®æ¨ç†é…ç½®é»˜è®¤å€¼: {key} = {default_value}")


def create_config_manager(config_path: str, config_type: str = "pretraining") -> BaseConfig:
    """é…ç½®ç®¡ç†å™¨å·¥å‚å‡½æ•°"""
    config_type = config_type.lower()
    
    if config_type == "pretraining":
        return PretrainingConfig(config_path)
    elif config_type == "finetuning":
        return FineTuningConfig(config_path)
    elif config_type == "inference":
        return InferenceConfig(config_path)
    else:
        # é»˜è®¤è¿”å›åŸºç¡€é…ç½®ç®¡ç†å™¨
        return BaseConfig(config_path)


def merge_configs(base_config_path: str, override_config_path: str, 
                 output_path: Optional[str] = None) -> Dict:
    """åˆå¹¶ä¸¤ä¸ªé…ç½®æ–‡ä»¶"""
    base_config = BaseConfig(base_config_path)
    override_config = BaseConfig(override_config_path)
    
    # æ·±åº¦åˆå¹¶é…ç½®
    merged_config = _deep_merge_dict(base_config.config.copy(), override_config.config)
    
    if output_path:
        with open(output_path, 'w', encoding='utf-8') as f:
            yaml.dump(merged_config, f, default_flow_style=False, allow_unicode=True)
        logger.info(f"âœ… åˆå¹¶åçš„é…ç½®å·²ä¿å­˜: {output_path}")
    
    return merged_config


def _deep_merge_dict(base_dict: Dict, override_dict: Dict) -> Dict:
    """æ·±åº¦åˆå¹¶å­—å…¸"""
    result = base_dict.copy()
    
    for key, value in override_dict.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge_dict(result[key], value)
        else:
            result[key] = value
    
    return result


if __name__ == "__main__":
    # æµ‹è¯•é…ç½®ç®¡ç†å™¨
    import argparse
    
    parser = argparse.ArgumentParser(description="é…ç½®ç®¡ç†å™¨æµ‹è¯•")
    parser.add_argument("--config", type=str, default="pretrain/config/config.yaml", 
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--type", type=str, default="pretraining", 
                       choices=["pretraining", "finetuning", "inference"],
                       help="é…ç½®ç±»å‹")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        config_manager = create_config_manager(args.config, args.type)
        print("âœ… é…ç½®ç®¡ç†å™¨æµ‹è¯•æˆåŠŸ")
        
        if isinstance(config_manager, PretrainingConfig):
            # æµ‹è¯•å‚æ•°é‡è®¡ç®—
            param_info = config_manager.calculate_model_parameters()
            print(f"ğŸ“Š æ€»å‚æ•°é‡: {param_info['total_params']:,}")
            
            # æµ‹è¯•è®­ç»ƒæ­¥æ•°è®¡ç®—ï¼ˆå‡è®¾æ•°æ®é›†å¤§å°ï¼‰
            steps_per_shard, steps_per_epoch = config_manager.calculate_training_steps(100000)
            print(f"ğŸ“Š æ¯åˆ†ç‰‡æ­¥æ•°: {steps_per_shard}, æ¯è½®æ­¥æ•°: {steps_per_epoch}")
            
    except Exception as e:
        print(f"âŒ é…ç½®ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        raise
