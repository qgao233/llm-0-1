#!/usr/bin/env python3
"""
0.1Bå‚æ•°LLMæ¨¡å‹æ¨ç†è„šæœ¬
æ”¯æŒé«˜æ•ˆæ¨ç†ã€KVç¼“å­˜ã€æ‰¹é‡ç”Ÿæˆç­‰åŠŸèƒ½
ä¸“ä¸ºæ¶ˆè´¹çº§ç¡¬ä»¶ä¼˜åŒ–
"""

import torch
import torch.nn.functional as F
import os
import sys
import time
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import numpy as np

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from llmcore import create_model
from data_collectors import DataProcessor
from config_manager import InferenceConfig
from checkpoint_manager import InferenceCheckpointManager


def get_device(force_cpu: bool = False, device_config: Dict = None) -> Tuple[torch.device, bool, List[int]]:
    """è·å–æ¨ç†è®¾å¤‡"""
    if force_cpu:
        return torch.device('cpu'), False, []
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        if device_count > 1:
            print(f"ğŸ¯ æ£€æµ‹åˆ° {device_count} ä¸ªGPU")
            return torch.device('cuda:0'), True, list(range(device_count))
        else:
            return torch.device('cuda:0'), False, [0]
    else:
        print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        return torch.device('cpu'), False, []


def sample_next_token(logits: torch.Tensor, temperature: float = 1.0, 
                     top_k: Optional[int] = None, top_p: Optional[float] = None) -> torch.Tensor:
    """é‡‡æ ·ä¸‹ä¸€ä¸ªtoken - 0.1Bæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬"""
    if temperature <= 0:
        return torch.argmax(logits, dim=-1, keepdim=True)
    
    logits = logits / temperature
    
    # Top-ké‡‡æ ·
    if top_k is not None and top_k > 0:
        top_k = min(top_k, logits.size(-1))
        values, indices = torch.topk(logits, top_k)
        logits = torch.full_like(logits, float('-inf'))
        logits.scatter_(-1, indices, values)
    
    # Top-pé‡‡æ ·
    if top_p is not None and 0 < top_p < 1:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # åˆ›å»ºæ©ç 
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
    
    # é‡‡æ ·
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)


class LLMInference:
    """0.1Bæ¨¡å‹æ¨ç†å™¨"""
    
    def __init__(self, model_dir: Optional[str] = None, 
                 config_path: str = "pretrain/config/config.yaml"):
        self.config_path = config_path
        self.model = None
        self.config = None
        self.data_processor = None
        self.device = None
        self.itos = None
        self.stoi = None
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # è®¾ç½®æ¨¡å‹ç›®å½•ï¼ˆä»é…ç½®æ–‡ä»¶æˆ–å‚æ•°è·å–ï¼‰
        if model_dir:
            self.model_dir = Path(model_dir)
        else:
            # ä¼˜å…ˆä»æ¨ç†é…ç½®è·å–æ¨¡å‹ç›®å½•
            inference_config = self.config.get('inference', {})
            if 'model_dir' in inference_config:
                self.model_dir = Path(inference_config['model_dir'])
            else:
                # å›é€€åˆ°è®­ç»ƒæ£€æŸ¥ç‚¹ç›®å½•
                checkpoint_dir = self.config.get('checkpoint', {}).get('save_dir', 'checkpoints/0_1b')
                self.model_dir = Path(checkpoint_dir)
        
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {self.model_dir}")
        
        # è®¾ç½®è®¾å¤‡
        self._setup_device()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # å‡†å¤‡åˆ†è¯å™¨
        self._setup_tokenizer()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨åŠ è½½å’ŒéªŒè¯é…ç½®
            config_manager = InferenceConfig(self.config_path)
            self.config = config_manager.config
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_device(self):
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        device_config = self.config.get('hardware', {})
        self.device, is_multi_gpu, available_gpus = get_device(
            force_cpu=device_config.get('device') == 'cpu'
        )
        print(f"ğŸ¯ æ¨ç†è®¾å¤‡: {self.device}")
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½0.1Bæ¨¡å‹...")
        
        # ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        checkpoint_manager = InferenceCheckpointManager(str(self.model_dir))
        best_model_path = checkpoint_manager.find_best_model()
        
        if not best_model_path:
            raise FileNotFoundError(f"åœ¨ {self.model_dir} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        try:
            checkpoint = torch.load(best_model_path, map_location=self.device, weights_only=False)
            
            # è·å–æ¨¡å‹é…ç½®
            if 'config' in checkpoint:
                model_config = checkpoint['config']['model']
            else:
                model_config = self.config['model']
            
            # åˆ›å»ºæ¨¡å‹
            self.model = create_model(model_config)
            
            # ä½¿ç”¨æ£€æŸ¥ç‚¹ç®¡ç†å™¨åŠ è½½æƒé‡
            checkpoint_manager.load_checkpoint(best_model_path, self.model, strict=False)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model = self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            if 'epoch' in checkpoint:
                print(f"ğŸ“Š è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']}")
            if 'step' in checkpoint:
                print(f"ğŸ“Š è®­ç»ƒæ­¥æ•°: {checkpoint['step']}")
            if 'loss' in checkpoint:
                print(f"ğŸ“‰ è®­ç»ƒæŸå¤±: {checkpoint['loss']:.4f}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_tokenizer(self):
        """è®¾ç½®åˆ†è¯å™¨"""
        print("ğŸ”¤ å‡†å¤‡åˆ†è¯å™¨...")
        
        data_config = self.config['data']
        tokenizer_type = "chinese_bpe" if data_config.get('use_bpe_tokenizer', True) else "char_level"
        vocab_path = data_config.get('vocab_cache_path', 'chinese_tokenizer_vocab_25k.json')
        
        self.data_processor = DataProcessor(
            tokenizer_type=tokenizer_type,
            vocab_path=vocab_path,
            config_path=self.config_path
        )
        
        # å‡†å¤‡è¯æ±‡è¡¨
        try:
            # å°è¯•ä»ç°æœ‰çš„è¯æ±‡è¡¨æ–‡ä»¶åŠ è½½
            if os.path.exists(vocab_path):
                print(f"âœ… åˆ†è¯å™¨å‡†å¤‡å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {self.data_processor.vocab_size or 'Unknown'}")
            else:
                print("âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹")
            
            # è·å–è¯æ±‡è¡¨æ˜ å°„
            if hasattr(self.data_processor, 'tokenizer') and self.data_processor.tokenizer:
                if hasattr(self.data_processor.tokenizer, 'itos'):
                    self.itos = self.data_processor.tokenizer.itos
                if hasattr(self.data_processor.tokenizer, 'stoi'):
                    self.stoi = self.data_processor.tokenizer.stoi
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def encode_text(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        if self.data_processor:
            return self.data_processor.encode_text(text)
        else:
            return [self.stoi.get(ch, 0) for ch in text if ch in self.stoi]
    
    def decode_tokens(self, tokens: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        if self.data_processor:
            return self.data_processor.decode_text(tokens)
        else:
            return ''.join([self.itos.get(idx, '') for idx in tokens])
    
    def generate(self, prompt: str, max_new_tokens: int = 256, 
                temperature: float = 0.8, top_k: int = 40, top_p: float = 0.9,
                use_kv_cache: bool = True, show_progress: bool = False) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        input_tokens = self.encode_text(prompt)
        if not input_tokens:
            return "æŠ±æ­‰ï¼Œæ— æ³•ç†è§£è¾“å…¥ã€‚"
        
        # è½¬æ¢ä¸ºå¼ é‡
        input_ids = torch.tensor([input_tokens], dtype=torch.long, device=self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            if use_kv_cache and hasattr(self.model, 'generate'):
                # ä½¿ç”¨æ¨¡å‹å†…ç½®çš„é«˜æ•ˆç”Ÿæˆæ–¹æ³•
                output_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    use_kv_cache=True
                )
            else:
                # ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•
                output_ids = self._generate_traditional(
                    input_ids, max_new_tokens, temperature, top_k, top_p, show_progress
                )
        
        # è§£ç è¾“å‡ºï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        generated_tokens = output_ids[0, len(input_tokens):].tolist()
        return self.decode_tokens(generated_tokens)
    
    def _generate_traditional(self, input_ids: torch.Tensor, max_new_tokens: int,
                            temperature: float, top_k: int, top_p: float, 
                            show_progress: bool = False) -> torch.Tensor:
        """ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•ï¼ˆä¸ä½¿ç”¨KVç¼“å­˜ï¼‰"""
        generated_ids = input_ids.clone()
        context_window = self.config['model']['context_window']
        
        for step in range(max_new_tokens):
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            current_context = generated_ids[:, -context_window:]
            
            # å‰å‘ä¼ æ’­
            logits = self.model(current_context)
            next_token_logits = logits[:, -1, :]
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = sample_next_token(
                next_token_logits,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p
            )
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_ids = torch.cat([generated_ids, next_token], dim=-1)
            
            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and step % 10 == 0:
                print(f"ç”Ÿæˆè¿›åº¦: {step}/{max_new_tokens}")
        
        return generated_ids
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆ"""
        results = []
        for prompt in prompts:
            result = self.generate(prompt, **kwargs)
            results.append(result)
        return results
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("\n" + "="*60)
        print("ğŸ¤– é€šç”¨å¯¹è¯å°åŠ©æ‰‹å·²å¯åŠ¨ï¼(0.1Bæ¨¡å‹)")
        print("ğŸ’¡ æ‚¨å¯ä»¥å’Œæˆ‘èŠå¤©ï¼Œæˆ‘ä¼šå›å¤æ‚¨")
        print("ğŸ“ è¾“å…¥ 'quit'ã€'exit' æˆ– 'bye' é€€å‡º")
        print("ğŸ›ï¸ è¾“å…¥ 'settings' æŸ¥çœ‹ç”Ÿæˆå‚æ•°")
        print("="*60)
        
        # é»˜è®¤ç”Ÿæˆå‚æ•° - 0.1Bæ¨¡å‹ä¼˜åŒ–
        settings = {
            'max_new_tokens': 256,  # å‡å°‘ç”Ÿæˆé•¿åº¦ä»¥é€‚é…å°æ¨¡å‹
            'temperature': 0.8,
            'top_k': 40,           # å‡å°‘å€™é€‰è¯æ•°é‡
            'top_p': 0.9,
            'use_kv_cache': True
        }
        
        conversation_history = []
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                    break
                
                if user_input.lower() == 'settings':
                    print("\nâš™ï¸ å½“å‰ç”Ÿæˆå‚æ•°:")
                    for key, value in settings.items():
                        print(f"  {key}: {value}")
                    continue
                
                # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¿æŒè¾ƒçŸ­çš„å†å²ä»¥é€‚é…å°æ¨¡å‹ï¼‰
                context_parts = []
                for item in conversation_history[-2:]:  # ä¿ç•™æœ€è¿‘2è½®å¯¹è¯
                    context_parts.append(f"ç”¨æˆ·ï¼š{item['user']}")
                    context_parts.append(f"åŠ©æ‰‹ï¼š{item['bot']}")
                
                context_parts.append(f"ç”¨æˆ·ï¼š{user_input}")
                context_parts.append("åŠ©æ‰‹ï¼š")
                context_prompt = ''.join(context_parts)
                
                # ç”Ÿæˆå›å¤
                print("ğŸ¤” æ€è€ƒä¸­...", end="", flush=True)
                start_time = time.time()
                
                response = self.generate(
                    context_prompt,
                    max_new_tokens=settings['max_new_tokens'],
                    temperature=settings['temperature'],
                    top_k=settings['top_k'],
                    top_p=settings['top_p'],
                    use_kv_cache=settings['use_kv_cache']
                )
                
                generation_time = time.time() - start_time
                
                # æ¸…é™¤"æ€è€ƒä¸­..."å¹¶æ˜¾ç¤ºå›å¤
                print(f"\r{'':20}\r", end="")
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
                
                # æ·»åŠ åˆ°å†å²
                conversation_history.append({
                    'user': user_input,
                    'bot': response
                })
                
                # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡5è½®ï¼ˆå°æ¨¡å‹å†…å­˜é™åˆ¶ï¼‰
                if len(conversation_history) > 5:
                    conversation_history.pop(0)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
                print("ğŸ”„ è¯·é‡è¯•...")


def warmup_model(model, device, warmup_steps=2):
    """é¢„çƒ­æ¨¡å‹ä»¥è·å¾—ç¨³å®šçš„æ¨ç†æ€§èƒ½"""
    print(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹ ({warmup_steps} æ­¥)...")
    
    model.eval()
    dummy_input = torch.randint(0, 1000, (1, 32), device=device)  # å‡å°‘è¾“å…¥å¤§å°
    
    with torch.no_grad():
        for i in range(warmup_steps):
            _ = model(dummy_input)
            print(f"  é¢„çƒ­æ­¥éª¤ {i+1}/{warmup_steps}")
    
    print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")


def load_model_and_config(model_dir=None, 
                         config_path="pretrain/config/config.yaml", 
                         enable_warmup=True):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®çš„ä¾¿æ·å‡½æ•°"""
    inference = LLMInference(model_dir, config_path)
    
    if enable_warmup:
        warmup_model(inference.model, inference.device)
    
    return inference.model, inference.config


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='0.1B LLMæ¨¡å‹æ¨ç†')
    parser.add_argument('--model-dir', default=None, help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆä¸æŒ‡å®šæ—¶ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')
    parser.add_argument('--config', default='pretrain/config/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--prompt', default='ä½ å¥½ï¼Œæˆ‘æƒ³äº†è§£', help='æµ‹è¯•æç¤ºæ–‡æœ¬')
    parser.add_argument('--max-tokens', type=int, default=50, help='æœ€å¤§ç”Ÿæˆtokenæ•°')
    parser.add_argument('--no-chat', action='store_true', help='ä¸å¯åŠ¨äº¤äº’å¼èŠå¤©')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨0.1Bæ¨¡å‹æ¨ç†...")
    if args.model_dir:
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.model_dir}")
    else:
        print("ğŸ“ æ¨¡å‹ç›®å½•: ä»é…ç½®æ–‡ä»¶è¯»å–")
    print(f"âš™ï¸ é…ç½®æ–‡ä»¶: {args.config}")
    
    try:
        inference = LLMInference(model_dir=args.model_dir, config_path=args.config)
        
        # æµ‹è¯•ç”Ÿæˆ
        print(f"\nğŸ§ª æµ‹è¯•ç”Ÿæˆ:")
        print(f"è¾“å…¥: {args.prompt}")
        
        result = inference.generate(
            args.prompt,
            max_new_tokens=args.max_tokens,
            temperature=0.8,
            show_progress=True
        )
        print(f"è¾“å‡º: {result}")
        
        # å¯åŠ¨äº¤äº’å¼èŠå¤©ï¼ˆé™¤éæŒ‡å®šä¸å¯åŠ¨ï¼‰
        if not args.no_chat:
            inference.interactive_chat()
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²ç»è®­ç»ƒäº†0.1Bæ¨¡å‹å¹¶ä¿å­˜åœ¨ç›¸åº”ç›®å½•ä¸‹")
        print("ğŸ’¡ æˆ–è€…æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„checkpoint.save_dirè®¾ç½®")
        raise


if __name__ == "__main__":
    main()
