#!/usr/bin/env python3
"""
0.1Bå‚æ•°LLMæ¨¡å‹äº¤äº’å¼èŠå¤©è„šæœ¬
æ”¯æŒæµå¼è¾“å‡ºã€å¯¹è¯å†å²ã€å‚æ•°è°ƒæ•´ç­‰åŠŸèƒ½
ä¸“ä¸ºæ¶ˆè´¹çº§ç¡¬ä»¶ä¼˜åŒ–
"""

import torch
import os
import sys
import time
import json
from typing import List, Dict
from pathlib import Path

from infer import LLMInference, sample_next_token


class ChatBot:
    """0.1Bæ¨¡å‹èŠå¤©æœºå™¨äºº"""
    
    def __init__(self, model_dir=None, 
                 config_path="pretrain/config/config.yaml"):
        """åˆå§‹åŒ–0.1BèŠå¤©æœºå™¨äºº"""
        self.model_dir = model_dir
        self.config_path = config_path
        self.conversation_history = []
        
        # åŠ è½½æ¨¡å‹å’Œé…ç½®
        print("ğŸš€ æ­£åœ¨åŠ è½½0.1Bæ¨¡å‹...")
        self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œç›¸å…³é…ç½®"""
        try:
            # è¯¢é—®æ˜¯å¦éœ€è¦é¢„çƒ­
            warmup_choice = input("ğŸ”¥ æ˜¯å¦é¢„çƒ­æ¨¡å‹ä»¥åŠ é€Ÿæ¨ç†ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower()
            enable_warmup = warmup_choice != 'n'
            
            # åˆ›å»ºæ¨ç†å™¨
            self.inference = LLMInference(self.model_dir, self.config_path)
            
            # é¢„çƒ­æ¨¡å‹
            if enable_warmup:
                self._warmup_model()
            
            print("âœ… 0.1Bæ¨¡å‹åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²ç»è®­ç»ƒäº†0.1Bæ¨¡å‹å¹¶ä¿å­˜åœ¨ç›¸åº”ç›®å½•ä¸‹")
            sys.exit(1)
    
    def _warmup_model(self):
        """é¢„çƒ­æ¨¡å‹"""
        print("ğŸ”¥ é¢„çƒ­0.1Bæ¨¡å‹ä¸­...")
        dummy_prompt = "æµ‹è¯•"
        
        start_time = time.time()
        _ = self.inference.generate(
            dummy_prompt,
            max_new_tokens=10,
            temperature=0.8,
            show_progress=False
        )
        warmup_time = time.time() - start_time
        
        print(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆï¼Œç”¨æ—¶: {warmup_time:.2f}ç§’")
    
    def generate_response(self, prompt: str, max_tokens: int = 256, 
                         temperature: float = 0.8, top_k: int = 40, top_p: float = 0.9,
                         use_kv_cache: bool = True, show_progress: bool = False, 
                         stream_delay: float = 0.03) -> str:
        """ç”Ÿæˆå›å¤"""
        try:
            # ç”Ÿæˆå®Œæ•´å›å¤
            response = self.inference.generate(
                prompt,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                use_kv_cache=use_kv_cache,
                show_progress=False  # å†…éƒ¨ä¸æ˜¾ç¤ºè¿›åº¦
            )
            
            # å¦‚æœéœ€è¦æµå¼æ˜¾ç¤º
            if show_progress and response:
                self._fake_stream_output(response, delay=stream_delay)
            
            return response.strip()
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
    
    def _fake_stream_output(self, text: str, delay: float = 0.03):
        """å‡çš„æµå¼è¾“å‡ºæ•ˆæœ"""
        print("ğŸ¤– åŠ©æ‰‹: ", end="", flush=True)
        
        for char in text:
            print(char, end="", flush=True)
            time.sleep(delay)
        
        print()  # æ¢è¡Œ
    
    def add_to_history(self, user_input: str, bot_response: str):
        """æ·»åŠ å¯¹è¯åˆ°å†å²è®°å½•"""
        self.conversation_history.append({
            'user': user_input,
            'bot': bot_response,
            'timestamp': time.time()
        })
        
        # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡8è½®å¯¹è¯ï¼ˆ0.1Bæ¨¡å‹å†…å­˜é™åˆ¶ï¼‰
        if len(self.conversation_history) > 8:
            self.conversation_history.pop(0)
    
    def get_context_prompt(self, current_input: str) -> str:
        """æ„å»ºåŒ…å«å†å²å¯¹è¯çš„æç¤º"""
        context_parts = []
        
        # æ·»åŠ æœ€è¿‘å‡ è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼ˆ0.1Bæ¨¡å‹å¤„ç†è¾ƒçŸ­ä¸Šä¸‹æ–‡ï¼‰
        for item in self.conversation_history[-3:]:  # æœ€å¤š3è½®å†å²
            context_parts.append(f"ç”¨æˆ·ï¼š{item['user']}")
            context_parts.append(f"åŠ©æ‰‹ï¼š{item['bot']}")
        
        # æ·»åŠ å½“å‰è¾“å…¥
        context_parts.append(f"ç”¨æˆ·ï¼š{current_input}")
        context_parts.append("åŠ©æ‰‹ï¼š")
        
        return ''.join(context_parts)
    
    def chat(self):
        """å¼€å§‹èŠå¤©å¾ªç¯"""
        print("\n" + "="*70)
        print("ğŸ¤– é€šç”¨å¯¹è¯å°åŠ©æ‰‹å·²å¯åŠ¨ï¼")
        print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ª0.1äº¿å‚æ•°çš„å¤§æ¨¡å‹ï¼Œé€‚åˆè½»é‡çº§å¯¹è¯")
        print("ğŸ“š æ‚¨å¯ä»¥å’Œæˆ‘èŠå¤©ï¼Œæˆ‘ä¼šå›å¤æ‚¨")
        print("ğŸ“ è¾“å…¥ 'quit'ã€'exit' æˆ– 'bye' é€€å‡º")
        print("ğŸ›ï¸ è¾“å…¥ 'settings' æŸ¥çœ‹å’Œä¿®æ”¹ç”Ÿæˆå‚æ•°")
        print("ğŸ“œ è¾“å…¥ 'history' æŸ¥çœ‹å¯¹è¯å†å²")
        print("ğŸ”„ è¾“å…¥ 'clear' æ¸…ç©ºå¯¹è¯å†å²")
        print("âš¡ è¾“å…¥ 'stream' åˆ‡æ¢æµå¼è¾“å‡ºæ˜¾ç¤ºæ¨¡å¼")
        print("ğŸ’¾ è¾“å…¥ 'save' ä¿å­˜å¯¹è¯å†å²")
        print("ğŸ“Š è¾“å…¥ 'stats' æŸ¥çœ‹æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯")
        print("="*70)
        
        # é»˜è®¤ç”Ÿæˆå‚æ•° - 0.1Bæ¨¡å‹ä¼˜åŒ–
        settings = {
            'max_tokens': 256,      # é€‚ä¸­çš„ç”Ÿæˆé•¿åº¦
            'temperature': 0.8,     # é€‚ä¸­çš„éšæœºæ€§
            'top_k': 40,           # è¾ƒå°‘çš„å€™é€‰è¯æ•°é‡
            'top_p': 0.9,
            'use_kv_cache': True,   # å¯ç”¨KVç¼“å­˜ä»¥æé«˜æ•ˆç‡
            'stream_delay': 0.03    # ç¨æ…¢çš„æµå¼è¾“å‡ºå»¶è¿Ÿ
        }
        
        # æµå¼è¾“å‡ºæ¨¡å¼
        stream_mode = True
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_tokens_generated = 0
        total_generation_time = 0
        conversation_count = 0
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    self._show_session_stats(conversation_count, total_tokens_generated, total_generation_time)
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                    break
                
                elif user_input.lower() == 'settings':
                    self._show_settings(settings)
                    continue
                
                elif user_input.lower() == 'history':
                    self._show_history()
                    continue
                
                elif user_input.lower() == 'clear':
                    self.conversation_history.clear()
                    print("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…ç©º")
                    continue
                
                elif user_input.lower() == 'stream':
                    stream_mode = not stream_mode
                    status = "å¼€å¯" if stream_mode else "å…³é—­"
                    print(f"âš¡ æµå¼è¾“å‡ºæ˜¾ç¤ºå·²{status}")
                    continue
                
                elif user_input.lower() == 'save':
                    self._save_conversation()
                    continue
                
                elif user_input.lower() == 'stats':
                    self._show_model_stats()
                    continue
                
                # æ„å»ºä¸Šä¸‹æ–‡æç¤º
                context_prompt = self.get_context_prompt(user_input)
                
                # æ˜¾ç¤ºæ€è€ƒä¸­æç¤º
                thinking_text = "ğŸ¤” 0.1Bæ¨¡å‹æ€è€ƒä¸­..."
                print(thinking_text, end="", flush=True)
                
                # ç”Ÿæˆå›å¤
                start_time = time.time()
                response = self.generate_response(
                    context_prompt,
                    max_tokens=settings['max_tokens'],
                    temperature=settings['temperature'],
                    top_k=settings['top_k'],
                    top_p=settings['top_p'],
                    use_kv_cache=settings['use_kv_cache'],
                    show_progress=stream_mode,
                    stream_delay=settings['stream_delay']
                )
                generation_time = time.time() - start_time
                
                # æ¸…é™¤æ€è€ƒä¸­æç¤º
                clear_spaces = " " * (len(thinking_text) + 5)
                print(f"\r{clear_spaces}\r", end="", flush=True)
                
                # å¦‚æœä¸æ˜¯æµå¼æ¨¡å¼ï¼Œç›´æ¥æ˜¾ç¤ºå®Œæ•´å›å¤
                if not stream_mode:
                    print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                
                # æ˜¾ç¤ºç”Ÿæˆç»Ÿè®¡
                if response:
                    # ä¼°ç®—ç”Ÿæˆçš„tokenæ•°é‡
                    estimated_tokens = len(response) // 2  # ç²—ç•¥ä¼°ç®—
                    tokens_per_second = estimated_tokens / generation_time if generation_time > 0 else 0
                    
                    print(f"ğŸ“Š ç”Ÿæˆç»Ÿè®¡: {estimated_tokens} tokens, {generation_time:.2f}s, {tokens_per_second:.1f} tokens/s")
                    
                    # æ›´æ–°ç»Ÿè®¡
                    total_tokens_generated += estimated_tokens
                    total_generation_time += generation_time
                    conversation_count += 1
                    
                    # æ·»åŠ åˆ°å†å²
                    self.add_to_history(user_input, response)
                else:
                    print("ğŸ¤– åŠ©æ‰‹: æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚")
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
                print("ğŸ”„ è¯·é‡è¯•...")
    
    def _show_settings(self, settings: Dict):
        """æ˜¾ç¤ºå’Œä¿®æ”¹è®¾ç½®"""
        print("\nâš™ï¸ å½“å‰ç”Ÿæˆå‚æ•°:")
        for key, value in settings.items():
            print(f"  {key}: {value}")
        
        print("\nğŸ’¡ å‚æ•°è¯´æ˜:")
        print("  max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ (50-512)")
        print("  temperature: éšæœºæ€§ (0.1-2.0, è¶Šå°è¶Šä¿å®ˆ)")
        print("  top_k: å€™é€‰è¯æ•°é‡ (1-100, 0è¡¨ç¤ºä¸é™åˆ¶)")
        print("  top_p: ç´¯ç§¯æ¦‚ç‡é˜ˆå€¼ (0.1-1.0)")
        print("  use_kv_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜ (true/false)")
        print("  stream_delay: æµå¼è¾“å‡ºå»¶è¿Ÿç§’æ•° (0.01-0.1)")
        
        modify = input("\nğŸ”§ æ˜¯å¦è¦ä¿®æ”¹å‚æ•°ï¼Ÿ(y/n): ").strip().lower()
        if modify == 'y':
            try:
                for key in settings.keys():
                    if key == 'use_kv_cache':
                        new_value = input(f"  {key} (å½“å‰: {settings[key]}): ").strip()
                        if new_value.lower() in ['true', 'false']:
                            settings[key] = new_value.lower() == 'true'
                    else:
                        new_value = input(f"  {key} (å½“å‰: {settings[key]}): ").strip()
                        if new_value:
                            if key == 'max_tokens':
                                settings[key] = max(50, min(512, int(new_value)))
                            elif key == 'temperature':
                                settings[key] = max(0.1, min(2.0, float(new_value)))
                            elif key == 'top_k':
                                settings[key] = max(0, min(100, int(new_value)))
                            elif key == 'top_p':
                                settings[key] = max(0.1, min(1.0, float(new_value)))
                            elif key == 'stream_delay':
                                settings[key] = max(0.01, min(0.1, float(new_value)))
                print("âœ… å‚æ•°å·²æ›´æ–°")
            except ValueError:
                print("âŒ å‚æ•°æ ¼å¼é”™è¯¯ï¼Œä¿æŒåŸè®¾ç½®")
    
    def _show_history(self):
        """æ˜¾ç¤ºå¯¹è¯å†å²"""
        if not self.conversation_history:
            print("ğŸ“ æš‚æ— å¯¹è¯å†å²")
            return
        
        print(f"\nğŸ“œ å¯¹è¯å†å² (æœ€è¿‘{len(self.conversation_history)}è½®):")
        print("-" * 60)
        for i, item in enumerate(self.conversation_history, 1):
            timestamp = time.strftime("%H:%M:%S", time.localtime(item['timestamp']))
            print(f"{i}. [{timestamp}] ğŸ‘¤ ç”¨æˆ·: {item['user']}")
            print(f"   ğŸ¤– åŠ©æ‰‹: {item['bot']}")
            print()
    
    def _save_conversation(self):
        """ä¿å­˜å¯¹è¯å†å²"""
        if not self.conversation_history:
            print("ğŸ“ æš‚æ— å¯¹è¯å†å²å¯ä¿å­˜")
            return
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"conversation_0_1b_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
    
    def _show_model_stats(self):
        """æ˜¾ç¤ºæ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        print("\nğŸ“Š 0.1Bæ¨¡å‹ç»Ÿè®¡ä¿¡æ¯:")
        
        # æ¨¡å‹å‚æ•°ä¿¡æ¯
        total_params = sum(p.numel() for p in self.inference.model.parameters())
        trainable_params = sum(p.numel() for p in self.inference.model.parameters() if p.requires_grad)
        
        print(f"  ğŸ—ï¸  æ¨¡å‹æ¶æ„:")
        print(f"    - æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        print(f"    - å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e6:.1f}M)")
        print(f"    - éšè—ç»´åº¦: {self.inference.config['model']['d_model']}")
        print(f"    - æ³¨æ„åŠ›å¤´æ•°: {self.inference.config['model']['n_heads']}")
        print(f"    - å±‚æ•°: {self.inference.config['model']['n_layers']}")
        print(f"    - ä¸Šä¸‹æ–‡çª—å£: {self.inference.config['model']['context_window']}")
        print(f"    - è¯æ±‡è¡¨å¤§å°: {self.inference.config['model'].get('vocab_size', 'Dynamic')}")
        
        # è®¾å¤‡ä¿¡æ¯
        print(f"  ğŸ¯ è¿è¡Œç¯å¢ƒ:")
        print(f"    - è®¾å¤‡: {self.inference.device}")
        if self.inference.device.type == 'cuda':
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_name = torch.cuda.get_device_name(0)
                print(f"    - GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        
        # å¯¹è¯ç»Ÿè®¡
        print(f"  ğŸ’¬ å¯¹è¯ç»Ÿè®¡:")
        print(f"    - å†å²å¯¹è¯è½®æ•°: {len(self.conversation_history)}")
        if self.conversation_history:
            session_time = time.time() - self.conversation_history[0]['timestamp']
            print(f"    - ä¼šè¯æ—¶é•¿: {session_time/60:.1f} åˆ†é’Ÿ")
    
    def _show_session_stats(self, conversation_count: int, total_tokens: int, total_time: float):
        """æ˜¾ç¤ºä¼šè¯ç»Ÿè®¡"""
        if conversation_count > 0:
            print(f"\nğŸ“Š æœ¬æ¬¡ä¼šè¯ç»Ÿè®¡:")
            print(f"  ğŸ’¬ å¯¹è¯è½®æ•°: {conversation_count}")
            print(f"  ğŸ”¤ ç”Ÿæˆtokenæ•°: {total_tokens}")
            print(f"  â±ï¸  æ€»ç”Ÿæˆæ—¶é—´: {total_time:.2f}ç§’")
            print(f"  ğŸš€ å¹³å‡ç”Ÿæˆé€Ÿåº¦: {total_tokens/total_time:.1f} tokens/s")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='0.1B LLMèŠå¤©æœºå™¨äºº')
    parser.add_argument('--model-dir', default=None, help='æ¨¡å‹ä¿å­˜ç›®å½•ï¼ˆä¸æŒ‡å®šæ—¶ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')
    parser.add_argument('--config', default='pretrain/config/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨é€šç”¨å¯¹è¯å°èŠå¤©åŠ©æ‰‹...")
    if args.model_dir:
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {args.model_dir}")
    else:
        print("ğŸ“ æ¨¡å‹ç›®å½•: ä»é…ç½®æ–‡ä»¶è¯»å–")
    print(f"âš™ï¸ é…ç½®æ–‡ä»¶: {args.config}")
    
    try:
        chatbot = ChatBot(args.model_dir, args.config)
        chatbot.chat()
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²ç»è®­ç»ƒäº†0.1Bæ¨¡å‹å¹¶ä¿å­˜åœ¨ç›¸åº”ç›®å½•ä¸‹")
        print("ğŸ’¡ æˆ–è€…ä½¿ç”¨ä¸åŒçš„å‚æ•°:")
        print("   python chat.py --model-dir /path/to/model --config pretrain/config/config.yaml")
        print("   python chat.py --config pretrain/config/config.yaml  # ä»é…ç½®æ–‡ä»¶è¯»å–æ¨¡å‹ç›®å½•")


if __name__ == "__main__":
    main()
