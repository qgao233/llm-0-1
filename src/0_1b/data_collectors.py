#!/usr/bin/env python3
"""
æ•°æ®æ”¶é›†å™¨æ¨¡å— - æ”¯æŒå¤šç§æ•°æ®ç±»å‹çš„æ”¶é›†å’Œå¤„ç†
ä¸º3Bæ¨¡å‹æä¾›çµæ´»çš„æ•°æ®æ”¶é›†ç­–ç•¥
æ•´åˆäº†æ•°æ®å¤„ç†å™¨çš„åŠŸèƒ½
"""

import torch
import json
import random
import numpy as np
import os
import urllib.request
import gc
import yaml
from abc import ABC, abstractmethod
from typing import Dict, List, Tuple, Optional, Union, Iterator
from pathlib import Path
import logging

# å¯¼å…¥ä¸­æ–‡åˆ†è¯å™¨
try:
    from chinese_tokenizer import ChineseTokenizer, create_chinese_tokenizer
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥ä¸­æ–‡åˆ†è¯å™¨ï¼Œå°†ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯")
    ChineseTokenizer = None
    create_chinese_tokenizer = None

logger = logging.getLogger(__name__)


class DataProcessor:
    """
    æ•°æ®å¤„ç†å™¨ - æ”¯æŒå­—ç¬¦çº§ã€BPEåˆ†è¯å’ŒæŒ‡ä»¤å¾®è°ƒæ•°æ®
    æ•´åˆåˆ°æ•°æ®æ”¶é›†å™¨ç³»ç»Ÿä¸­
    """
    
    def __init__(self, tokenizer_type: str = "chinese_bpe", vocab_path: Optional[str] = None, 
                 config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            tokenizer_type: åˆ†è¯å™¨ç±»å‹ ("char_level" æˆ– "chinese_bpe")
            vocab_path: è¯æ±‡è¡¨è·¯å¾„
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.tokenizer_type = tokenizer_type
        self.vocab_path = vocab_path
        self.config_path = config_path
        self.config = None
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        if config_path and os.path.exists(config_path):
            self.load_config(config_path)
        
        # åˆå§‹åŒ–åˆ†è¯å™¨
        if tokenizer_type == "chinese_bpe" and create_chinese_tokenizer:
            logger.info("ğŸš€ ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–BPEåˆ†è¯å™¨")
            self.tokenizer = create_chinese_tokenizer()
            if vocab_path:
                # å°è¯•åŠ è½½è¯æ±‡è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨ä¹Ÿä¸ä¼šæŠ¥é”™
                self.tokenizer.load_vocab(vocab_path)
        else:
            logger.info("ğŸ“ ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯å™¨")
            self.tokenizer = None
        
        # æ•°æ®ç¼“å­˜
        self.dataset = None
        self.vocab = None
        self.itos = None
        self.stoi = None
        self.vocab_size = None
        
    def load_config(self, config_path: str):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.info(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            
            # ä»é…ç½®æ–‡ä»¶æ›´æ–°ç›¸å…³å‚æ•°
            if self.config and 'data' in self.config:
                data_config = self.config['data']
                
                # æ³¨æ„ï¼švocab_size ä¸å†ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œç”±åˆ†è¯å™¨è‡ªåŠ¨ç¡®å®š
                
                # æ›´æ–°è¯æ±‡è¡¨ç¼“å­˜è·¯å¾„
                if 'vocab_cache_path' in data_config:
                    self.vocab_path = data_config['vocab_cache_path']
                
                # æ›´æ–°åˆ†è¯å™¨ç±»å‹
                if 'use_bpe_tokenizer' in data_config:
                    self.tokenizer_type = "chinese_bpe" if data_config['use_bpe_tokenizer'] else "char_level"
                    
        except Exception as e:
            logger.warning(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            self.config = None
    
    def encode_text(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—"""
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            return self.tokenizer.encode(text)
        else:
            # å­—ç¬¦çº§ç¼–ç 
            if not self.stoi:
                # å¦‚æœæ²¡æœ‰å­—ç¬¦æ˜ å°„ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„
                chars = sorted(list(set(text)))
                self.stoi = {ch: i for i, ch in enumerate(chars)}
                self.itos = {i: ch for i, ch in enumerate(chars)}
            return [self.stoi.get(ch, 0) for ch in text]
    
    def decode_text(self, indices: List[int]) -> str:
        """è§£ç æ•°å­—åºåˆ—ä¸ºæ–‡æœ¬"""
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            # ç›´æ¥ä½¿ç”¨BPEåˆ†è¯å™¨è§£ç 
            try:
                return self.tokenizer.decode(indices)
            except Exception as e:
                logger.warning(f"âš ï¸ BPEè§£ç å¤±è´¥: {e}")
                # é™çº§åˆ°é€ä¸ªè§£ç 
                result = []
                for idx in indices:
                    if self.itos and idx in self.itos:
                        result.append(self.itos[idx])
                    else:
                        result.append(f'<UNK_{idx}>')
                return ''.join(result)
        else:
            # å­—ç¬¦çº§è§£ç 
            if not self.itos:
                return ''.join([f'<{idx}>' for idx in indices])
            return ''.join([self.itos.get(idx, '') for idx in indices])
    
    def prepare_vocab_from_data(self, data_sources: List[str]) -> None:
        """ä»æ•°æ®æºå‡†å¤‡è¯æ±‡è¡¨"""
        logger.info("ğŸ“š ä»æ•°æ®æºå‡†å¤‡è¯æ±‡è¡¨...")
        
        # æ”¶é›†æ‰€æœ‰æ–‡æœ¬ç”¨äºè®­ç»ƒåˆ†è¯å™¨
        all_texts = []
        for source in data_sources:
            if not Path(source).exists():
                continue
                
            try:
                if source.endswith('.jsonl'):
                    with open(source, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                item = json.loads(line)
                                text = self._format_instruction_item(item)
                                all_texts.append(text)
                elif source.endswith('.json'):
                    with open(source, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            for item in data:
                                text = self._format_instruction_item(item)
                                all_texts.append(text)
                else:
                    with open(source, 'r', encoding='utf-8') as f:
                        text = f.read().strip()
                        if text:
                            all_texts.append(text)
            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†æ•°æ®æºå¤±è´¥ {source}: {e}")
        
        if not all_texts:
            logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬æ•°æ®")
            return
        
        # è®­ç»ƒåˆ†è¯å™¨æˆ–åˆ›å»ºå­—ç¬¦çº§è¯æ±‡è¡¨
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            # è®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆå®é™…ä¸Šæ˜¯ç»Ÿè®¡æ›´æ–°ï¼Œå› ä¸ºåŸºäºtiktokenï¼‰
            combined_text = '\n'.join(all_texts[:10000])  # é™åˆ¶è®­ç»ƒæ•°æ®é‡
            self.tokenizer.train_on_text(combined_text)
            # è·å–åˆ†è¯å™¨ç¡®å®šçš„è¯æ±‡è¡¨å¤§å°
            self.vocab_size = self.tokenizer.vocab_size
            self.vocab = list(range(self.vocab_size))
            
            # ä¿å­˜åˆ†è¯å™¨
            if self.vocab_path:
                self.tokenizer.save_vocab(self.vocab_path)
                logger.info(f"ğŸ’¾ åˆ†è¯å™¨å·²ä¿å­˜åˆ°: {self.vocab_path}")
            else:
                logger.warning("âš ï¸ vocab_path ä¸ºç©ºï¼Œè·³è¿‡ä¿å­˜åˆ†è¯å™¨")
        else:
            # åˆ›å»ºå­—ç¬¦çº§è¯æ±‡è¡¨
            all_chars = set()
            for text in all_texts:
                all_chars.update(text)
            
            self.vocab = sorted(list(all_chars))
            self.itos = {i: ch for i, ch in enumerate(self.vocab)}
            self.stoi = {ch: i for i, ch in enumerate(self.vocab)}
            self.vocab_size = len(self.vocab)
        
        logger.info(f"âœ… è¯æ±‡è¡¨å‡†å¤‡å®Œæˆï¼Œå¤§å°: {self.vocab_size:,}")
    
    def _format_instruction_item(self, item: Dict) -> str:
        """æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®é¡¹"""
        instruction = item.get('instruction', '').strip()
        input_text = item.get('input', '').strip()
        output_text = item.get('output', '').strip()
        
        if input_text:
            return f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nå›ç­”ï¼š{output_text}"
        else:
            return f"æŒ‡ä»¤ï¼š{instruction}\nå›ç­”ï¼š{output_text}"


class BaseDataCollector(ABC):
    """
    åŸºç¡€æ•°æ®æ”¶é›†å™¨æŠ½è±¡ç±»
    å®šä¹‰æ‰€æœ‰æ•°æ®æ”¶é›†å™¨çš„ç»Ÿä¸€æ¥å£
    """
    
    def __init__(self, name: str, data_processor, config: Dict = None):
        """
        åˆå§‹åŒ–æ•°æ®æ”¶é›†å™¨
        
        Args:
            name: æ”¶é›†å™¨åç§°
            data_processor: æ•°æ®å¤„ç†å™¨å®ä¾‹
            config: é…ç½®å‚æ•°
        """
        self.name = name
        self.data_processor = data_processor
        self.config = config or {}
        self.data_cache = []
        self.current_index = 0
        
    @abstractmethod
    def load_data(self, data_source: Union[str, List[str]]) -> None:
        """
        åŠ è½½æ•°æ®æº
        
        Args:
            data_source: æ•°æ®æºè·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
        """
        pass
    
    @abstractmethod
    def get_sample(self, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
        
        Args:
            seq_length: åºåˆ—é•¿åº¦
            
        Returns:
            (input_tensor, target_tensor): è¾“å…¥å’Œç›®æ ‡å¼ é‡
        """
        pass
    
    @abstractmethod
    def get_batch(self, batch_size: int, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒæ ·æœ¬
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°
            seq_length: åºåˆ—é•¿åº¦
            
        Returns:
            (input_batch, target_batch): è¾“å…¥å’Œç›®æ ‡æ‰¹æ¬¡å¼ é‡
        """
        pass
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return len(self.data_cache)
    
    def reset(self) -> None:
        """é‡ç½®æ•°æ®ç´¢å¼•"""
        self.current_index = 0
        if self.config.get('shuffle', True):
            random.shuffle(self.data_cache)


class ConversationDataCollector(BaseDataCollector):
    """
    å¯¹è¯æ•°æ®æ”¶é›†å™¨
    å¤„ç†instruction-outputæ ¼å¼çš„å¯¹è¯æ•°æ®
    """
    
    def __init__(self, data_processor, config: Dict = None):
        super().__init__("ConversationCollector", data_processor, config)
        self.conversation_templates = [
            "æŒ‡ä»¤ï¼š{instruction}\nå›ç­”ï¼š{output}",
            "é—®é¢˜ï¼š{instruction}\nç­”æ¡ˆï¼š{output}",
            "ç”¨æˆ·ï¼š{instruction}\nåŠ©æ‰‹ï¼š{output}",
            "Human: {instruction}\nAssistant: {output}",
        ]
        self.current_template = 0
        
    def load_data(self, data_source: Union[str, List[str]]) -> None:
        """åŠ è½½å¯¹è¯æ•°æ®"""
        logger.info(f"ğŸ—£ï¸ {self.name} åŠ è½½å¯¹è¯æ•°æ®...")
        
        if isinstance(data_source, str):
            data_sources = [data_source]
        else:
            data_sources = data_source
        
        self.data_cache = []
        
        for source in data_sources:
            if not Path(source).exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {source}")
                continue
                
            try:
                # æ”¯æŒJSONå’ŒJSONLæ ¼å¼
                if source.endswith('.jsonl'):
                    with open(source, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                item = json.loads(line)
                                self.data_cache.append(item)
                else:
                    with open(source, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.data_cache.extend(data)
                        else:
                            self.data_cache.append(data)
                            
                logger.info(f"  âœ… ä» {source} åŠ è½½äº† {len(self.data_cache)} ä¸ªå¯¹è¯æ ·æœ¬")
                
            except Exception as e:
                logger.error(f"  âŒ åŠ è½½æ•°æ®å¤±è´¥ {source}: {e}")
        
        logger.info(f"ğŸ“Š {self.name} æ€»å…±åŠ è½½äº† {len(self.data_cache)} ä¸ªå¯¹è¯æ ·æœ¬")
        
        # æ‰“ä¹±æ•°æ®
        if self.config.get('shuffle', True):
            random.shuffle(self.data_cache)
    
    def _format_conversation(self, item: Dict) -> str:
        """æ ¼å¼åŒ–å¯¹è¯æ•°æ®"""
        instruction = item.get('instruction', '').strip()
        input_text = item.get('input', '').strip()
        output_text = item.get('output', '').strip()
        
        # é€‰æ‹©æ¨¡æ¿
        template = self.conversation_templates[self.current_template % len(self.conversation_templates)]
        
        # å¤„ç†æœ‰è¾“å…¥çš„æƒ…å†µ
        if input_text:
            full_instruction = f"{instruction}\nè¾“å…¥ï¼š{input_text}"
        else:
            full_instruction = instruction
        
        # æ ¼å¼åŒ–å¯¹è¯
        formatted = template.format(instruction=full_instruction, output=output_text)
        
        # è½®æ¢æ¨¡æ¿ä»¥å¢åŠ å¤šæ ·æ€§
        self.current_template += 1
        
        return formatted
    
    def get_sample(self, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªå¯¹è¯æ ·æœ¬"""
        if not self.data_cache:
            raise ValueError("æ•°æ®ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        # å¾ªç¯è·å–æ ·æœ¬
        if self.current_index >= len(self.data_cache):
            self.reset()
        
        # è·å–å¯¹è¯æ•°æ®
        conversation_item = self.data_cache[self.current_index]
        self.current_index += 1
        
        # æ ¼å¼åŒ–å¯¹è¯
        formatted_text = self._format_conversation(conversation_item)
        
        # ç¼–ç æ–‡æœ¬
        tokens = self.data_processor.encode_text(formatted_text)
        
        # å¦‚æœtokenså¤ªçŸ­ï¼Œå°è¯•åˆå¹¶å¤šä¸ªå¯¹è¯
        while len(tokens) < seq_length and self.current_index < len(self.data_cache):
            next_item = self.data_cache[self.current_index]
            next_text = self._format_conversation(next_item)
            next_tokens = self.data_processor.encode_text(next_text)
            
            # æ·»åŠ åˆ†éš”ç¬¦
            separator_tokens = self.data_processor.encode_text("\n\n")
            tokens.extend(separator_tokens + next_tokens)
            self.current_index += 1
        
        # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
        if len(tokens) > seq_length + 1:
            tokens = tokens[:seq_length + 1]
        elif len(tokens) < seq_length + 1:
            # ç”¨pad tokenå¡«å……ï¼ˆé€šå¸¸æ˜¯0ï¼‰
            pad_token = 0
            tokens.extend([pad_token] * (seq_length + 1 - len(tokens)))
        
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡å¼ é‡
        x = torch.tensor(tokens[:-1], dtype=torch.long)
        y = torch.tensor(tokens[1:], dtype=torch.long)
        
        return x, y
    
    def get_batch(self, batch_size: int, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„å¯¹è¯æ ·æœ¬"""
        batch_x = []
        batch_y = []
        
        for _ in range(batch_size):
            x, y = self.get_sample(seq_length)
            batch_x.append(x)
            batch_y.append(y)
        
        return torch.stack(batch_x), torch.stack(batch_y)


class LongTextDataCollector(BaseDataCollector):
    """
    é•¿æ–‡æœ¬åºåˆ—æ•°æ®æ”¶é›†å™¨
    å¤„ç†è¿ç»­çš„é•¿æ–‡æœ¬æ•°æ®ï¼Œé€‚åˆé¢„è®­ç»ƒ
    """
    
    def __init__(self, data_processor, config: Dict = None):
        super().__init__("LongTextCollector", data_processor, config)
        self.text_data = ""
        self.tokens = []
        
    def load_data(self, data_source: Union[str, List[str]]) -> None:
        """åŠ è½½é•¿æ–‡æœ¬æ•°æ®"""
        logger.info(f"ğŸ“– {self.name} åŠ è½½é•¿æ–‡æœ¬æ•°æ®...")
        
        if isinstance(data_source, str):
            data_sources = [data_source]
        else:
            data_sources = data_source
        
        all_texts = []
        
        for source in data_sources:
            if not Path(source).exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {source}")
                continue
                
            try:
                with open(source, 'r', encoding='utf-8') as f:
                    text = f.read().strip()
                    if text:
                        all_texts.append(text)
                        logger.info(f"  âœ… ä» {source} åŠ è½½äº† {len(text):,} ä¸ªå­—ç¬¦")
                        
            except Exception as e:
                logger.error(f"  âŒ åŠ è½½æ•°æ®å¤±è´¥ {source}: {e}")
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬
        self.text_data = '\n\n'.join(all_texts)
        
        # ç¼–ç æ•´ä¸ªæ–‡æœ¬
        self.tokens = self.data_processor.encode_text(self.text_data)
        
        logger.info(f"ğŸ“Š {self.name} æ€»å…±åŠ è½½äº† {len(self.text_data):,} ä¸ªå­—ç¬¦ï¼Œ{len(self.tokens):,} ä¸ªtokens")
        
        # åˆ›å»ºæ•°æ®ç¼“å­˜ï¼ˆæ»‘åŠ¨çª—å£ç´¢å¼•ï¼‰
        self.data_cache = list(range(len(self.tokens)))
        
        if self.config.get('shuffle', True):
            random.shuffle(self.data_cache)
    
    def get_sample(self, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªé•¿æ–‡æœ¬æ ·æœ¬"""
        if not self.tokens:
            raise ValueError("tokensä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        if len(self.tokens) <= seq_length:
            raise ValueError(f"æ–‡æœ¬é•¿åº¦ {len(self.tokens)} å¿…é¡»å¤§äºåºåˆ—é•¿åº¦ {seq_length}")
        
        # éšæœºé€‰æ‹©èµ·å§‹ä½ç½®
        max_start = len(self.tokens) - seq_length - 1
        start_idx = random.randint(0, max_start)
        
        # è·å–åºåˆ—
        x = torch.tensor(self.tokens[start_idx:start_idx + seq_length], dtype=torch.long)
        y = torch.tensor(self.tokens[start_idx + 1:start_idx + seq_length + 1], dtype=torch.long)
        
        return x, y
    
    def get_batch(self, batch_size: int, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„é•¿æ–‡æœ¬æ ·æœ¬"""
        batch_x = []
        batch_y = []
        
        for _ in range(batch_size):
            x, y = self.get_sample(seq_length)
            batch_x.append(x)
            batch_y.append(y)
        
        return torch.stack(batch_x), torch.stack(batch_y)


class CodeDataCollector(BaseDataCollector):
    """
    ä»£ç æ•°æ®æ”¶é›†å™¨
    ä¸“é—¨å¤„ç†ä»£ç è®­ç»ƒæ•°æ®
    """
    
    def __init__(self, data_processor, config: Dict = None):
        super().__init__("CodeCollector", data_processor, config)
        self.code_templates = [
            "# ä»£ç ç¤ºä¾‹\n{code}",
            "```python\n{code}\n```",
            "ä»¥ä¸‹æ˜¯Pythonä»£ç ï¼š\n{code}",
        ]
        
    def load_data(self, data_source: Union[str, List[str]]) -> None:
        """åŠ è½½ä»£ç æ•°æ®"""
        logger.info(f"ğŸ’» {self.name} åŠ è½½ä»£ç æ•°æ®...")
        
        if isinstance(data_source, str):
            data_sources = [data_source]
        else:
            data_sources = data_source
        
        self.data_cache = []
        
        for source in data_sources:
            if not Path(source).exists():
                logger.warning(f"âš ï¸ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {source}")
                continue
                
            try:
                # æ”¯æŒ.pyæ–‡ä»¶å’ŒJSONæ ¼å¼
                if source.endswith('.py'):
                    with open(source, 'r', encoding='utf-8') as f:
                        code_content = f.read().strip()
                        if code_content:
                            self.data_cache.append({'code': code_content})
                else:
                    with open(source, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.data_cache.extend(data)
                        else:
                            self.data_cache.append(data)
                            
                logger.info(f"  âœ… ä» {source} åŠ è½½äº†ä»£ç æ•°æ®")
                
            except Exception as e:
                logger.error(f"  âŒ åŠ è½½æ•°æ®å¤±è´¥ {source}: {e}")
        
        logger.info(f"ğŸ“Š {self.name} æ€»å…±åŠ è½½äº† {len(self.data_cache)} ä¸ªä»£ç æ ·æœ¬")
        
        if self.config.get('shuffle', True):
            random.shuffle(self.data_cache)
    
    def get_sample(self, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªä»£ç æ ·æœ¬"""
        if not self.data_cache:
            raise ValueError("æ•°æ®ç¼“å­˜ä¸ºç©ºï¼Œè¯·å…ˆè°ƒç”¨load_data()")
        
        if self.current_index >= len(self.data_cache):
            self.reset()
        
        # è·å–ä»£ç æ•°æ®
        code_item = self.data_cache[self.current_index]
        self.current_index += 1
        
        # æ ¼å¼åŒ–ä»£ç 
        code_content = code_item.get('code', '')
        template = random.choice(self.code_templates)
        formatted_text = template.format(code=code_content)
        
        # ç¼–ç æ–‡æœ¬
        tokens = self.data_processor.encode_text(formatted_text)
        
        # å¤„ç†é•¿åº¦
        if len(tokens) > seq_length + 1:
            tokens = tokens[:seq_length + 1]
        elif len(tokens) < seq_length + 1:
            pad_token = 0
            tokens.extend([pad_token] * (seq_length + 1 - len(tokens)))
        
        x = torch.tensor(tokens[:-1], dtype=torch.long)
        y = torch.tensor(tokens[1:], dtype=torch.long)
        
        return x, y
    
    def get_batch(self, batch_size: int, seq_length: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªæ‰¹æ¬¡çš„ä»£ç æ ·æœ¬"""
        batch_x = []
        batch_y = []
        
        for _ in range(batch_size):
            x, y = self.get_sample(seq_length)
            batch_x.append(x)
            batch_y.append(y)
        
        return torch.stack(batch_x), torch.stack(batch_y)


class MixedDataset(torch.utils.data.Dataset):
    """
    æ··åˆæ•°æ®é›†ç±»
    éšæœºè°ƒç”¨ä¸åŒçš„æ•°æ®æ”¶é›†å™¨è·å–è®­ç»ƒæ ·æœ¬
    """
    
    def __init__(self, collectors: List[BaseDataCollector], seq_length: int, 
                 weights: Optional[List[float]] = None, config: Dict = None):
        """
        åˆå§‹åŒ–æ··åˆæ•°æ®é›†
        
        Args:
            collectors: æ•°æ®æ”¶é›†å™¨åˆ—è¡¨
            seq_length: åºåˆ—é•¿åº¦
            weights: å„æ”¶é›†å™¨çš„é‡‡æ ·æƒé‡
            config: é…ç½®å‚æ•°
        """
        self.collectors = collectors
        self.seq_length = seq_length
        self.config = config or {}
        
        # è®¾ç½®é‡‡æ ·æƒé‡
        if weights is None:
            self.weights = [1.0] * len(collectors)
        else:
            if len(weights) != len(collectors):
                raise ValueError("æƒé‡æ•°é‡å¿…é¡»ä¸æ”¶é›†å™¨æ•°é‡ç›¸åŒ")
            self.weights = weights
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(self.weights)
        self.weights = [w / total_weight for w in self.weights]
        
        # è®¡ç®—æ€»æ•°æ®é›†å¤§å°ï¼ˆå–æœ€å¤§çš„æ”¶é›†å™¨å¤§å°ï¼‰
        self.total_size = max(len(collector) for collector in collectors) if collectors else 0
        
        logger.info(f"ğŸ”€ æ··åˆæ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        for i, collector in enumerate(collectors):
            logger.info(f"  ğŸ“Š {collector.name}: {len(collector):,} æ ·æœ¬, æƒé‡: {self.weights[i]:.3f}")
        logger.info(f"  ğŸ“ˆ æ€»æ•°æ®é›†å¤§å°: {self.total_size:,}")
    
    def __len__(self) -> int:
        """è¿”å›æ•°æ®é›†å¤§å°"""
        return self.total_size
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬"""
        # æ ¹æ®æƒé‡éšæœºé€‰æ‹©æ”¶é›†å™¨
        collector = np.random.choice(self.collectors, p=self.weights)
        
        # ä»é€‰ä¸­çš„æ”¶é›†å™¨è·å–æ ·æœ¬
        try:
            x, y = collector.get_sample(self.seq_length)
            return x, y
        except Exception as e:
            logger.warning(f"âš ï¸ ä» {collector.name} è·å–æ ·æœ¬å¤±è´¥: {e}")
            # é™çº§åˆ°ç¬¬ä¸€ä¸ªæ”¶é›†å™¨
            return self.collectors[0].get_sample(self.seq_length)
    
    def get_batch_from_collector(self, collector_name: str, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """ä»æŒ‡å®šæ”¶é›†å™¨è·å–æ‰¹æ¬¡"""
        for collector in self.collectors:
            if collector.name == collector_name:
                return collector.get_batch(batch_size, self.seq_length)
        
        raise ValueError(f"æœªæ‰¾åˆ°æ”¶é›†å™¨: {collector_name}")
    
    def reset_all_collectors(self) -> None:
        """é‡ç½®æ‰€æœ‰æ”¶é›†å™¨"""
        for collector in self.collectors:
            collector.reset()
    
    def get_collector_stats(self) -> Dict:
        """è·å–æ”¶é›†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}
        for i, collector in enumerate(self.collectors):
            stats[collector.name] = {
                'size': len(collector),
                'weight': self.weights[i],
                'current_index': collector.current_index
            }
        return stats


def create_data_collectors(config: Dict) -> Tuple[List[BaseDataCollector], DataProcessor]:
    """
    æ ¹æ®é…ç½®åˆ›å»ºæ•°æ®æ”¶é›†å™¨å’Œæ•°æ®å¤„ç†å™¨
    
    Args:
        config: é…ç½®å­—å…¸
        
    Returns:
        (æ•°æ®æ”¶é›†å™¨åˆ—è¡¨, æ•°æ®å¤„ç†å™¨)
    """
    # åˆ›å»ºæ•°æ®å¤„ç†å™¨
    data_config = config.get('data', {})
    tokenizer_type = "chinese_bpe" if data_config.get('use_bpe_tokenizer', True) else "char_level"
    vocab_path = data_config.get('vocab_cache_path', 'chinese_tokenizer_vocab_25k.json')
    logger.info(f"ğŸ” è°ƒè¯•: ä»é…ç½®è·å–çš„ vocab_path = '{vocab_path}'")
    
    data_processor = DataProcessor(
        tokenizer_type=tokenizer_type,
        vocab_path=vocab_path,
        config_path=None  # ä¸éœ€è¦é‡å¤åŠ è½½é…ç½®ï¼Œç›´æ¥ä½¿ç”¨ä¼ å…¥çš„config
    )
    
    # æ”¶é›†æ‰€æœ‰æ•°æ®æºç”¨äºå‡†å¤‡è¯æ±‡è¡¨
    all_data_sources = []
    collector_configs = config.get('data_collectors', {})
    
    # æ”¶é›†å¯¹è¯æ•°æ®æº
    if collector_configs.get('conversation', {}).get('enabled', True):
        conv_sources = collector_configs.get('conversation', {}).get('data_sources', [])
        all_data_sources.extend(conv_sources)
    
    # æ”¶é›†é•¿æ–‡æœ¬æ•°æ®æº
    if collector_configs.get('long_text', {}).get('enabled', False):
        text_sources = collector_configs.get('long_text', {}).get('data_sources', [])
        all_data_sources.extend(text_sources)
    
    # æ”¶é›†ä»£ç æ•°æ®æº
    if collector_configs.get('code', {}).get('enabled', False):
        code_sources = collector_configs.get('code', {}).get('data_sources', [])
        all_data_sources.extend(code_sources)
    
    # å¦‚æœæ²¡æœ‰é…ç½®æ•°æ®æºï¼Œä½¿ç”¨é»˜è®¤é…ç½®
    if not all_data_sources:
        raise ValueError("æœªé…ç½®ä»»ä½•æ•°æ®æºï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šæ•°æ®æº")
    
    # å‡†å¤‡è¯æ±‡è¡¨
    if all_data_sources:
        data_processor.prepare_vocab_from_data(all_data_sources)
    
    collectors = []
    
    # åˆ›å»ºå¯¹è¯æ•°æ®æ”¶é›†å™¨
    if collector_configs.get('conversation', {}).get('enabled', True):
        conv_config = collector_configs.get('conversation', {})
        conv_collector = ConversationDataCollector(data_processor, conv_config)
        
        # åŠ è½½å¯¹è¯æ•°æ®
        conv_data_sources = conv_config.get('data_sources', [])
        if conv_data_sources:
            conv_collector.load_data(conv_data_sources)
            collectors.append(conv_collector)
            logger.info(f"âœ… åˆ›å»ºå¯¹è¯æ•°æ®æ”¶é›†å™¨: {len(conv_collector)} æ ·æœ¬")
    
    # åˆ›å»ºé•¿æ–‡æœ¬æ•°æ®æ”¶é›†å™¨
    if collector_configs.get('long_text', {}).get('enabled', False):
        text_config = collector_configs.get('long_text', {})
        text_collector = LongTextDataCollector(data_processor, text_config)
        
        # åŠ è½½é•¿æ–‡æœ¬æ•°æ®
        text_data_sources = text_config.get('data_sources', [])
        if text_data_sources:
            text_collector.load_data(text_data_sources)
            collectors.append(text_collector)
            logger.info(f"âœ… åˆ›å»ºé•¿æ–‡æœ¬æ•°æ®æ”¶é›†å™¨: {len(text_collector)} tokens")
    
    # åˆ›å»ºä»£ç æ•°æ®æ”¶é›†å™¨
    if collector_configs.get('code', {}).get('enabled', False):
        code_config = collector_configs.get('code', {})
        code_collector = CodeDataCollector(data_processor, code_config)
        
        # åŠ è½½ä»£ç æ•°æ®
        code_data_sources = code_config.get('data_sources', [])
        if code_data_sources:
            code_collector.load_data(code_data_sources)
            collectors.append(code_collector)
            logger.info(f"âœ… åˆ›å»ºä»£ç æ•°æ®æ”¶é›†å™¨: {len(code_collector)} æ ·æœ¬")
    
    if not collectors:
        logger.warning("âš ï¸ æœªåˆ›å»ºä»»ä½•æ•°æ®æ”¶é›†å™¨ï¼Œå°†ä½¿ç”¨é»˜è®¤å¯¹è¯æ”¶é›†å™¨")
        # åˆ›å»ºé»˜è®¤å¯¹è¯æ”¶é›†å™¨
        default_collector = ConversationDataCollector(data_processor)
        # å°è¯•åŠ è½½é»˜è®¤æ•°æ®
        if all_data_sources:
            default_collector.load_data(all_data_sources)
        collectors.append(default_collector)
    
    return collectors, data_processor


# å…¼å®¹æ€§å‡½æ•°
def create_data_processor_from_config(config_path: str) -> DataProcessor:
    """
    ä»é…ç½®æ–‡ä»¶åˆ›å»ºæ•°æ®å¤„ç†å™¨
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        DataProcessorå®ä¾‹
    """
    return DataProcessor(config_path=config_path)


if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®æ”¶é›†å™¨
    print("ğŸ§ª æµ‹è¯•æ•°æ®æ”¶é›†å™¨ç³»ç»Ÿ...")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = {
        'data': {
            'use_bpe_tokenizer': True,
            'vocab_cache_path': 'test_vocab.json',
        },
        'model': {
            'vocab_size': 10000
        },
        'data_collectors': {
            'conversation': {
                'enabled': True,
                'data_sources': ["data/train_2M_CN_split/train_2M_CN_part_01.jsonl"],
                'shuffle': True
            }
        }
    }
    
    # æµ‹è¯•åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    print("\n1. æµ‹è¯•åˆ›å»ºæ•°æ®æ”¶é›†å™¨:")
    try:
        collectors, processor = create_data_collectors(test_config)
        print(f"âœ… åˆ›å»ºäº† {len(collectors)} ä¸ªæ•°æ®æ”¶é›†å™¨")
        print(f"âœ… æ•°æ®å¤„ç†å™¨ç±»å‹: {processor.tokenizer_type}")
        
        if collectors:
            # æµ‹è¯•è·å–æ ·æœ¬
            conv_collector = collectors[0]
            if len(conv_collector) > 0:
                x, y = conv_collector.get_sample(128)
                print(f"âœ… å¯¹è¯æ ·æœ¬å½¢çŠ¶: x={x.shape}, y={y.shape}")
                
                batch_x, batch_y = conv_collector.get_batch(2, 128)
                print(f"âœ… å¯¹è¯æ‰¹æ¬¡å½¢çŠ¶: x={batch_x.shape}, y={batch_y.shape}")
            else:
                print("âš ï¸ æ”¶é›†å™¨ä¸­æ²¡æœ‰æ•°æ®")
        
    except Exception as e:
        print(f"âš ï¸ æ•°æ®æ”¶é›†å™¨æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # æµ‹è¯•æ··åˆæ•°æ®é›†
    print("\n2. æµ‹è¯•æ··åˆæ•°æ®é›†:")
    try:
        if 'collectors' in locals() and collectors:
            mixed_dataset = MixedDataset(collectors, seq_length=128)
            
            print(f"âœ… æ··åˆæ•°æ®é›†å¤§å°: {len(mixed_dataset)}")
            
            # æµ‹è¯•è·å–æ ·æœ¬
            if len(mixed_dataset) > 0:
                x, y = mixed_dataset[0]
                print(f"âœ… æ··åˆæ ·æœ¬å½¢çŠ¶: x={x.shape}, y={y.shape}")
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                stats = mixed_dataset.get_collector_stats()
                print(f"ğŸ“Š æ”¶é›†å™¨ç»Ÿè®¡: {stats}")
            else:
                print("âš ï¸ æ··åˆæ•°æ®é›†ä¸ºç©º")
        else:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„æ”¶é›†å™¨")
        
    except Exception as e:
        print(f"âš ï¸ æ··åˆæ•°æ®é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ æ•°æ®æ”¶é›†å™¨ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
