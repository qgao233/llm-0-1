#!/usr/bin/env python3
"""
Checkpointç®¡ç†å™¨æ¨¡å—
ç”¨äºç®¡ç†æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ£€æŸ¥ç‚¹ä¿å­˜ã€åŠ è½½ã€æ¸…ç†ç­‰åŠŸèƒ½
æ”¯æŒé¢„è®­ç»ƒã€å¾®è°ƒç­‰ä¸åŒè®­ç»ƒé˜¶æ®µçš„æ£€æŸ¥ç‚¹ç®¡ç†
"""

import os
import torch
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Any, Union, List
import yaml
import json

logger = logging.getLogger(__name__)


class BaseCheckpointManager:
    """åŸºç¡€æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir: str, keep_last_n: int = 3, create_timestamp_dir: bool = True):
        """
        åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        
        Args:
            save_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
            keep_last_n: ä¿ç•™æœ€è¿‘Nä¸ªæ£€æŸ¥ç‚¹
            create_timestamp_dir: æ˜¯å¦åˆ›å»ºæ—¶é—´æˆ³å­ç›®å½•
        """
        self.save_dir = Path(save_dir)
        self.keep_last_n = keep_last_n
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        if create_timestamp_dir:
            # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.checkpoint_dir = self.save_dir / f"checkpoint_{timestamp}"
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.checkpoint_dir = self.save_dir
        
        logger.info(f"ğŸ“ Checkpointç›®å½•: {self.checkpoint_dir}")
    
    def save_checkpoint(self, model, optimizer, step: int, loss: float, 
                       config: Dict, extra_info: Optional[Dict] = None, 
                       scaler: Optional[Any] = None, 
                       checkpoint_name: Optional[str] = None) -> Optional[Path]:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹
            step: è®­ç»ƒæ­¥æ•°
            loss: å½“å‰æŸå¤±
            config: é…ç½®ä¿¡æ¯
            extra_info: é¢å¤–ä¿¡æ¯
            scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„scaler
            checkpoint_name: è‡ªå®šä¹‰æ£€æŸ¥ç‚¹åç§°
            
        Returns:
            ä¿å­˜çš„æ£€æŸ¥ç‚¹è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        if checkpoint_name is None:
            checkpoint_name = f"checkpoint_step_{step:06d}.pt"
        
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        
        # å‡†å¤‡checkpointæ•°æ®
        checkpoint_data = {
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': config,
            'timestamp': datetime.now().isoformat(),
        }
        
        # ä¿å­˜æ··åˆç²¾åº¦è®­ç»ƒçš„scalerçŠ¶æ€
        if scaler:
            checkpoint_data['scaler_state_dict'] = scaler.state_dict()
        
        if extra_info:
            checkpoint_data.update(extra_info)
        
        try:
            torch.save(checkpoint_data, checkpoint_path)
            logger.info(f"ğŸ’¾ å·²ä¿å­˜checkpoint: {checkpoint_name} (loss: {loss:.4f})")
            
            # æ¸…ç†æ—§çš„checkpoint
            self._cleanup_old_checkpoints()
            
            return checkpoint_path
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜checkpointå¤±è´¥: {e}")
            return None
    
    def load_checkpoint(self, checkpoint_path: Union[str, Path], model, 
                       optimizer: Optional[Any] = None, 
                       scaler: Optional[Any] = None,
                       strict: bool = True) -> Optional[Dict]:
        """
        åŠ è½½æ£€æŸ¥ç‚¹
        
        Args:
            checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            model: æ¨¡å‹å®ä¾‹
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„scalerï¼ˆå¯é€‰ï¼‰
            strict: æ˜¯å¦ä¸¥æ ¼åŒ¹é…æ¨¡å‹æƒé‡
            
        Returns:
            æ£€æŸ¥ç‚¹æ•°æ®å­—å…¸ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            
            # åŠ è½½æ¨¡å‹æƒé‡
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                
                # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                if any(key.startswith('module.') for key in state_dict.keys()):
                    state_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}
                
                model.load_state_dict(state_dict, strict=strict)
            
            # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
            if optimizer and 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # åŠ è½½æ··åˆç²¾åº¦è®­ç»ƒçš„scalerçŠ¶æ€
            if scaler and 'scaler_state_dict' in checkpoint:
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
                logger.info("ğŸ”¥ å·²æ¢å¤æ··åˆç²¾åº¦è®­ç»ƒçŠ¶æ€")
            
            logger.info(f"âœ… å·²åŠ è½½checkpoint: {checkpoint_path}")
            if 'step' in checkpoint:
                logger.info(f"  æ­¥æ•°: {checkpoint['step']}")
            if 'loss' in checkpoint:
                logger.info(f"  æŸå¤±: {checkpoint['loss']:.4f}")
            
            return checkpoint
        except Exception as e:
            logger.error(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            return None
    
    def save_best_model(self, model, optimizer, step: int, loss: float, 
                       config: Dict, extra_info: Optional[Dict] = None,
                       scaler: Optional[Any] = None) -> Optional[Path]:
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        best_model_path = self.checkpoint_dir / "best_model.pt"
        
        checkpoint_data = {
            'step': step,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'config': config,
            'timestamp': datetime.now().isoformat(),
            'is_best_model': True
        }
        
        if scaler:
            checkpoint_data['scaler_state_dict'] = scaler.state_dict()
        
        if extra_info:
            checkpoint_data.update(extra_info)
        
        try:
            torch.save(checkpoint_data, best_model_path)
            logger.info(f"ğŸ’« ä¿å­˜æœ€ä½³æ¨¡å‹ (loss: {loss:.4f})")
            return best_model_path
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def find_latest_checkpoint(self, pattern: str = "checkpoint_step_*.pt") -> Optional[Path]:
        """æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoint_files = list(self.checkpoint_dir.glob(pattern))
        if not checkpoint_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_checkpoint = max(checkpoint_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"ğŸ“ æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")
        return latest_checkpoint
    
    def list_checkpoints(self, pattern: str = "checkpoint_step_*.pt") -> List[Path]:
        """åˆ—å‡ºæ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶"""
        checkpoint_files = list(self.checkpoint_dir.glob(pattern))
        checkpoint_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return checkpoint_files
    
    def _cleanup_old_checkpoints(self, pattern: str = "checkpoint_step_*.pt"):
        """æ¸…ç†æ—§çš„checkpointæ–‡ä»¶"""
        checkpoint_files = list(self.checkpoint_dir.glob(pattern))
        if len(checkpoint_files) > self.keep_last_n:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            checkpoint_files.sort(key=lambda x: x.stat().st_mtime)
            # åˆ é™¤æœ€æ—§çš„æ–‡ä»¶
            for old_file in checkpoint_files[:-self.keep_last_n]:
                try:
                    old_file.unlink()
                    logger.info(f"ğŸ—‘ï¸ å·²æ¸…ç†æ—§checkpoint: {old_file.name}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†æ–‡ä»¶å¤±è´¥ {old_file.name}: {e}")
    
    def save_config(self, config: Dict, filename: str = "config.yaml"):
        """ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬"""
        config_path = self.checkpoint_dir / filename
        try:
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
            logger.info(f"ğŸ“ é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_path}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
    
    def get_checkpoint_info(self, checkpoint_path: Union[str, Path]) -> Optional[Dict]:
        """è·å–æ£€æŸ¥ç‚¹ä¿¡æ¯ï¼ˆä¸åŠ è½½æƒé‡ï¼‰"""
        try:
            checkpoint = torch.load(checkpoint_path, map_location='cpu')
            info = {
                'step': checkpoint.get('step', 'Unknown'),
                'loss': checkpoint.get('loss', 'Unknown'),
                'timestamp': checkpoint.get('timestamp', 'Unknown'),
                'file_size': Path(checkpoint_path).stat().st_size / (1024 * 1024)  # MB
            }
            
            # æ·»åŠ é¢å¤–ä¿¡æ¯
            for key in ['epoch', 'shard', 'is_best_model']:
                if key in checkpoint:
                    info[key] = checkpoint[key]
            
            return info
        except Exception as e:
            logger.error(f"âŒ è·å–æ£€æŸ¥ç‚¹ä¿¡æ¯å¤±è´¥: {e}")
            return None


class PretrainingCheckpointManager(BaseCheckpointManager):
    """é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir: str, keep_last_n: int = 3):
        super().__init__(save_dir, keep_last_n, create_timestamp_dir=True)
        # ä¸ºé¢„è®­ç»ƒåˆ›å»ºç‰¹æ®Šçš„æ—¶é—´æˆ³ç›®å½•å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.checkpoint_dir = self.save_dir / f"pretrain_{timestamp}"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ é¢„è®­ç»ƒCheckpointç›®å½•: {self.checkpoint_dir}")
    
    def save_checkpoint(self, model, optimizer, epoch: int, shard: int, step: int, 
                       loss: float, config: Dict, extra_info: Optional[Dict] = None, 
                       scaler: Optional[Any] = None) -> Optional[Path]:
        """
        ä¿å­˜é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹
            epoch: å½“å‰è½®æ•°
            shard: å½“å‰åˆ†ç‰‡
            step: è®­ç»ƒæ­¥æ•°
            loss: å½“å‰æŸå¤±
            config: é…ç½®ä¿¡æ¯
            extra_info: é¢å¤–ä¿¡æ¯
            scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„scaler
        """
        checkpoint_name = f"epoch_{epoch:02d}_shard_{shard:02d}_step_{step:06d}.pt"
        
        # æ·»åŠ é¢„è®­ç»ƒç‰¹æœ‰çš„ä¿¡æ¯
        pretrain_info = {
            'epoch': epoch,
            'shard': shard,
            'training_type': 'pretraining'
        }
        
        if extra_info:
            pretrain_info.update(extra_info)
        
        return super().save_checkpoint(
            model, optimizer, step, loss, config, 
            extra_info=pretrain_info, scaler=scaler, 
            checkpoint_name=checkpoint_name
        )
    
    def save_best_model(self, model, optimizer, epoch: int, shard: int, step: int, 
                       loss: float, config: Dict, extra_info: Optional[Dict] = None,
                       scaler: Optional[Any] = None) -> Optional[Path]:
        """ä¿å­˜æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹"""
        best_info = {
            'epoch': epoch,
            'shard': shard,
            'training_type': 'pretraining'
        }
        
        if extra_info:
            best_info.update(extra_info)
        
        return super().save_best_model(
            model, optimizer, step, loss, config, 
            extra_info=best_info, scaler=scaler
        )


class FineTuningCheckpointManager(BaseCheckpointManager):
    """å¾®è°ƒæ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, save_dir: str, keep_last_n: int = 2):
        super().__init__(save_dir, keep_last_n, create_timestamp_dir=True)
        # ä¸ºå¾®è°ƒåˆ›å»ºç‰¹æ®Šçš„æ—¶é—´æˆ³ç›®å½•å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.checkpoint_dir = self.save_dir / f"finetune_{timestamp}"
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ å¾®è°ƒCheckpointç›®å½•: {self.checkpoint_dir}")
    
    def save_checkpoint(self, model, optimizer, epoch: int, step: int, 
                       loss: float, config: Dict, extra_info: Optional[Dict] = None, 
                       scaler: Optional[Any] = None) -> Optional[Path]:
        """
        ä¿å­˜å¾®è°ƒæ£€æŸ¥ç‚¹
        
        Args:
            model: æ¨¡å‹å®ä¾‹
            optimizer: ä¼˜åŒ–å™¨å®ä¾‹
            epoch: å½“å‰è½®æ•°
            step: è®­ç»ƒæ­¥æ•°
            loss: å½“å‰æŸå¤±
            config: é…ç½®ä¿¡æ¯
            extra_info: é¢å¤–ä¿¡æ¯
            scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„scaler
        """
        checkpoint_name = f"finetune_epoch_{epoch:02d}_step_{step:06d}.pt"
        
        # æ·»åŠ å¾®è°ƒç‰¹æœ‰çš„ä¿¡æ¯
        finetune_info = {
            'epoch': epoch,
            'training_type': 'finetuning'
        }
        
        if extra_info:
            finetune_info.update(extra_info)
        
        return super().save_checkpoint(
            model, optimizer, step, loss, config, 
            extra_info=finetune_info, scaler=scaler, 
            checkpoint_name=checkpoint_name
        )


class InferenceCheckpointManager(BaseCheckpointManager):
    """æ¨ç†æ£€æŸ¥ç‚¹ç®¡ç†å™¨ï¼ˆä¸»è¦ç”¨äºåŠ è½½ï¼‰"""
    
    def __init__(self, model_dir: str):
        super().__init__(model_dir, keep_last_n=1, create_timestamp_dir=False)
    
    def find_best_model(self) -> Optional[Path]:
        """æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶"""
        best_model_path = self.checkpoint_dir / "best_model.pt"
        if best_model_path.exists():
            logger.info(f"ğŸ’ æ‰¾åˆ°æœ€ä½³æ¨¡å‹: {best_model_path}")
            return best_model_path
        
        # å¦‚æœæ²¡æœ‰æœ€ä½³æ¨¡å‹ï¼ŒæŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
        latest_checkpoint = self.find_latest_checkpoint("*.pt")
        if latest_checkpoint:
            logger.info(f"ğŸ“ ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹: {latest_checkpoint}")
            return latest_checkpoint
        
        logger.warning(f"âš ï¸ åœ¨ {self.checkpoint_dir} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return None


def create_checkpoint_manager(save_dir: str, manager_type: str = "base", 
                            **kwargs) -> BaseCheckpointManager:
    """
    æ£€æŸ¥ç‚¹ç®¡ç†å™¨å·¥å‚å‡½æ•°
    
    Args:
        save_dir: ä¿å­˜ç›®å½•
        manager_type: ç®¡ç†å™¨ç±»å‹ ("base", "pretraining", "finetuning", "inference")
        **kwargs: å…¶ä»–å‚æ•°
    
    Returns:
        å¯¹åº”ç±»å‹çš„æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    """
    manager_type = manager_type.lower()
    
    if manager_type == "pretraining":
        return PretrainingCheckpointManager(save_dir, **kwargs)
    elif manager_type == "finetuning":
        return FineTuningCheckpointManager(save_dir, **kwargs)
    elif manager_type == "inference":
        return InferenceCheckpointManager(save_dir)
    else:
        return BaseCheckpointManager(save_dir, **kwargs)


def resume_training_from_checkpoint(checkpoint_path: Union[str, Path], 
                                  model, optimizer, scaler=None) -> Dict:
    """
    ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒçš„ä¾¿æ·å‡½æ•°
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
        model: æ¨¡å‹å®ä¾‹
        optimizer: ä¼˜åŒ–å™¨å®ä¾‹
        scaler: æ··åˆç²¾åº¦è®­ç»ƒçš„scaler
    
    Returns:
        åŒ…å«æ¢å¤ä¿¡æ¯çš„å­—å…¸
    """
    manager = BaseCheckpointManager("./", keep_last_n=1, create_timestamp_dir=False)
    checkpoint_data = manager.load_checkpoint(checkpoint_path, model, optimizer, scaler)
    
    if checkpoint_data:
        resume_info = {
            'epoch': checkpoint_data.get('epoch', 0),
            'shard': checkpoint_data.get('shard', 0),
            'step': checkpoint_data.get('step', 0),
            'loss': checkpoint_data.get('loss', float('inf')),
            'training_type': checkpoint_data.get('training_type', 'unknown')
        }
        
        logger.info(f"ğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ:")
        for key, value in resume_info.items():
            logger.info(f"  {key}: {value}")
        
        return resume_info
    
    return {}


if __name__ == "__main__":
    # æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨
    import argparse
    
    parser = argparse.ArgumentParser(description="æ£€æŸ¥ç‚¹ç®¡ç†å™¨æµ‹è¯•")
    parser.add_argument("--save-dir", type=str, default="./test_checkpoints", 
                       help="æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•")
    parser.add_argument("--type", type=str, default="base", 
                       choices=["base", "pretraining", "finetuning", "inference"],
                       help="ç®¡ç†å™¨ç±»å‹")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO, 
                       format='%(asctime)s - %(levelname)s - %(message)s')
    
    try:
        # åˆ›å»ºæµ‹è¯•ç®¡ç†å™¨
        manager = create_checkpoint_manager(args.save_dir, args.type)
        
        # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        print(f"âœ… æ£€æŸ¥ç‚¹ç®¡ç†å™¨æµ‹è¯•æˆåŠŸ")
        print(f"ğŸ“ æ£€æŸ¥ç‚¹ç›®å½•: {manager.checkpoint_dir}")
        
        # åˆ—å‡ºç°æœ‰æ£€æŸ¥ç‚¹
        checkpoints = manager.list_checkpoints()
        print(f"ğŸ“‹ ç°æœ‰æ£€æŸ¥ç‚¹æ•°é‡: {len(checkpoints)}")
        
        if checkpoints:
            for cp in checkpoints[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                info = manager.get_checkpoint_info(cp)
                if info:
                    print(f"  - {cp.name}: step={info['step']}, loss={info['loss']:.4f}")
        
    except Exception as e:
        print(f"âŒ æ£€æŸ¥ç‚¹ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        raise
