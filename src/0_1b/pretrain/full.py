#!/usr/bin/env python3
"""
0.1Bæ¨¡å‹å…¨å‚é¢„è®­ç»ƒè„šæœ¬
å®ç°ä¸‰å±‚åµŒå¥—å¾ªç¯çš„é¢„è®­ç»ƒæ¶æ„ï¼š
1. ç¬¬ä¸€å±‚ï¼šè½®æ•°å¾ªç¯ (epochs)
2. ç¬¬äºŒå±‚ï¼šåˆ†ç‰‡å¾ªç¯ (shards) - ç”¨äºå®šæœŸcheckpointä¿å­˜
3. ç¬¬ä¸‰å±‚ï¼šæ­¥æ•°å¾ªç¯ (steps) - çœŸæ­£çš„æ ·æœ¬å¤„ç†å¾ªç¯
"""

import os
import sys
import time
import json
import yaml
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import numpy as np
import logging
from datetime import datetime
from pathlib import Path
import warnings
import gc
from tqdm.auto import tqdm
from torch.cuda.amp import autocast, GradScaler

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥æœ¬é¡¹ç›®æ¨¡å—
from llmcore import create_model
from data_collectors import create_data_collectors, MixedDataset
from utils import get_device
from config_manager import PretrainingConfig
from checkpoint_manager import PretrainingCheckpointManager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)


class ShardDataset(Dataset):
    """
    åˆ†ç‰‡æ•°æ®é›†ç±»ï¼Œç”¨äºé…åˆDataLoaderä½¿ç”¨
    """
    
    def __init__(self, mixed_dataset, shard_size: int, seq_length: int):
        """
        åˆå§‹åŒ–åˆ†ç‰‡æ•°æ®é›†
        
        Args:
            mixed_dataset: æ··åˆæ•°æ®é›†å®ä¾‹
            shard_size: åˆ†ç‰‡å¤§å°ï¼ˆæ ·æœ¬æ•°é‡ï¼‰
            seq_length: åºåˆ—é•¿åº¦
        """
        self.mixed_dataset = mixed_dataset
        self.shard_size = shard_size
        self.seq_length = seq_length
        
    def __len__(self):
        return self.shard_size
    
    def __getitem__(self, idx):
        """è·å–ä¸€ä¸ªè®­ç»ƒæ ·æœ¬"""
        # ä»æ··åˆæ•°æ®é›†éšæœºè·å–æ ·æœ¬
        collector = np.random.choice(self.mixed_dataset.collectors, p=self.mixed_dataset.weights)
        x, y = collector.get_sample(self.seq_length)
        return x, y

class TrainingMonitor:
    """è®­ç»ƒç›‘æ§å™¨"""
    
    def __init__(self, log_dir: str):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¶é—´æˆ³æ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.current_log_dir = self.log_dir / f"pretrain_{timestamp}"
        self.current_log_dir.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_history = []
        
        logger.info(f"ğŸ“Š æ—¥å¿—ç›®å½•: {self.current_log_dir}")
    
    def log_step(self, epoch, shard, step, loss, lr, elapsed_time, samples_per_sec=None):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        log_entry = {
            'epoch': epoch,
            'shard': shard,
            'step': step,
            'loss': loss,
            'learning_rate': lr,
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().isoformat()
        }
        
        if samples_per_sec:
            log_entry['samples_per_sec'] = samples_per_sec
        
        self.training_history.append(log_entry)
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        if samples_per_sec:
            logger.info(f"ğŸ“ˆ Epoch {epoch:02d}/{shard:02d} Step {step:06d} | "
                       f"Loss: {loss:.4f} | LR: {lr:.2e} | "
                       f"Speed: {samples_per_sec:.1f} samples/s")
        else:
            logger.info(f"ğŸ“ˆ Epoch {epoch:02d}/{shard:02d} Step {step:06d} | "
                       f"Loss: {loss:.4f} | LR: {lr:.2e}")
    
    def save_training_log(self):
        """ä¿å­˜è®­ç»ƒæ—¥å¿—"""
        log_file = self.current_log_dir / "training_log.json"
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(self.training_history, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: {log_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è®­ç»ƒæ—¥å¿—å¤±è´¥: {e}")

def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    training_config = config['training']
    
    # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½æ˜¯æ­£ç¡®çš„æ•°å€¼ç±»å‹
    lr = float(training_config['learning_rate'])
    weight_decay = float(training_config.get('weight_decay', 0.01))
    beta1 = float(training_config.get('beta1', 0.9))
    beta2 = float(training_config.get('beta2', 0.95))
    eps = float(training_config.get('eps', 1e-8))
    
    if training_config.get('optimizer', 'adamw').lower() == 'adamw':
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(beta1, beta2),
            eps=eps
        )
    else:
        # é»˜è®¤ä½¿ç”¨Adam
        optimizer = torch.optim.Adam(
            model.parameters(),
            lr=lr,
            weight_decay=weight_decay
        )
    
    logger.info(f"ğŸ”§ åˆ›å»ºä¼˜åŒ–å™¨: {type(optimizer).__name__}")
    logger.info(f"  å­¦ä¹ ç‡: {lr}")
    logger.info(f"  æƒé‡è¡°å‡: {weight_decay}")
    return optimizer

def create_lr_scheduler(optimizer, config):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    training_config = config['training']
    scheduler_type = training_config.get('lr_scheduler', 'cosine')
    
    # ç¡®ä¿æ•°å€¼å‚æ•°ç±»å‹æ­£ç¡®
    max_steps = int(training_config['max_steps'])
    learning_rate = float(training_config['learning_rate'])
    min_lr_ratio = float(training_config.get('min_lr_ratio', 0.1))
    
    if scheduler_type == 'cosine':
        # ä½™å¼¦é€€ç«è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=max_steps,
            eta_min=learning_rate * min_lr_ratio
        )
    elif scheduler_type == 'linear':
        # çº¿æ€§è¡°å‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=1.0,
            end_factor=min_lr_ratio,
            total_iters=max_steps
        )
    else:
        # æ— è°ƒåº¦å™¨
        scheduler = None
    
    if scheduler:
        logger.info(f"ğŸ“š åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_type}")
        logger.info(f"  æœ€å¤§æ­¥æ•°: {max_steps}")
        logger.info(f"  æœ€å°å­¦ä¹ ç‡æ¯”ä¾‹: {min_lr_ratio}")
    else:
        logger.info("ğŸ“š ä¸ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    
    return scheduler

@torch.no_grad()
def evaluate_model(model, dataset, config, device, num_eval_batches=10):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    
    total_loss = 0.0
    batch_size = config['training']['batch_size']
    seq_length = config['model']['context_window']
    
    for _ in range(num_eval_batches):
        try:
            # è·å–è¯„ä¼°batch
            batch_x, batch_y = dataset.collectors[0].get_batch(
                batch_size, 
                config['model']['context_window']
            )
            
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            # å‰å‘ä¼ æ’­
            logits = model(batch_x)
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), batch_y.view(-1))
            
            total_loss += loss.item()
            
        except Exception as e:
            logger.warning(f"âš ï¸ è¯„ä¼°æ‰¹æ¬¡å¤±è´¥: {e}")
            continue
    
    model.train()
    avg_loss = total_loss / num_eval_batches if num_eval_batches > 0 else float('inf')
    return avg_loss

def train_model(config_path: str, resume_checkpoint: str = None):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹0.1Bæ¨¡å‹é¢„è®­ç»ƒ")
    
    # 1. åŠ è½½é…ç½®
    config_manager = PretrainingConfig(config_path)
    config = config_manager.config
    
    # 2. è®¾å¤‡æ£€æµ‹
    device, use_multi_gpu, available_gpus = get_device(
        force_cpu=False,
        device_config=config.get('hardware', {})
    )
    
    logger.info(f"ğŸ¯ è®­ç»ƒè®¾å¤‡: {device}")
    if use_multi_gpu:
        logger.info(f"ğŸš€ å¯ç”¨å¤šGPUè®­ç»ƒï¼Œä½¿ç”¨ {len(available_gpus)} ä¸ªGPU")
    else:
        logger.info(f"ğŸ“± ä½¿ç”¨å•GPUè®­ç»ƒ")
    
    # 3. åˆ›å»ºæ•°æ®æ”¶é›†å™¨å’Œæ•°æ®é›†
    logger.info("ğŸ“š å‡†å¤‡æ•°æ®é›†...")
    try:
        collectors, data_processor = create_data_collectors(config)
        
        # åˆ›å»ºæ··åˆæ•°æ®é›†
        dataset = MixedDataset(
            collectors=collectors,
            seq_length=config['model']['context_window'],
            weights=None,  # ä½¿ç”¨é»˜è®¤æƒé‡
            config=config.get('data', {})
        )
        
        logger.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆï¼Œæ€»å¤§å°: {len(dataset):,}")
        
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        raise
    
    # 4. åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    try:
        # ä»åˆ†è¯å™¨è·å–å®é™…è¯æ±‡è¡¨å¤§å°
        actual_vocab_size = data_processor.vocab_size
        if actual_vocab_size:
            config['model']['vocab_size'] = actual_vocab_size
            logger.info(f"ğŸ“š ä½¿ç”¨åˆ†è¯å™¨è®¡ç®—çš„è¯æ±‡è¡¨å¤§å°: {actual_vocab_size:,}")
        else:
            # å¦‚æœåˆ†è¯å™¨æ²¡æœ‰vocab_sizeï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise ValueError("åˆ†è¯å™¨å¿…é¡»æä¾›è¯æ±‡è¡¨å¤§å°(vocab_size)")
            
        model = create_model(config['model'])
        model = model.to(device)
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
        if config.get('hardware', {}).get('gradient_checkpointing', True):
            if hasattr(model, 'gradient_checkpointing_enable'):
                model.gradient_checkpointing_enable()
            logger.info("ğŸ’¾ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
        # å¤šGPUæ”¯æŒ
        if use_multi_gpu:
            model = nn.DataParallel(model)
            logger.info("ğŸš€ å¯ç”¨DataParallelå¤šGPUè®­ç»ƒ")
        
        # ç»Ÿè®¡æ¨¡å‹å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        # è®¡ç®—å„éƒ¨åˆ†å‚æ•°é‡
        vocab_size = config['model']['vocab_size']
        d_model = config['model']['d_model']
        n_layers = config['model']['n_layers']
        ffn_dim = config['model']['ffn_dim']
        
        embedding_params = vocab_size * d_model
        transformer_params = n_layers * (3 * d_model * d_model + 2 * d_model * ffn_dim)
        tie_weights = config['model'].get('tie_weights', True)
        output_params = 0 if tie_weights else vocab_size * d_model
        
        logger.info(f"âœ… æ¨¡å‹åˆ›å»ºå®Œæˆ")
        logger.info(f"ğŸ“Š è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
        logger.info(f"ğŸ“Š æ¨¡å‹å‚æ•°åˆ†å¸ƒ:")
        logger.info(f"  - Embeddingå±‚: {embedding_params:,} å‚æ•°")
        logger.info(f"  - Transformerå±‚: {transformer_params:,} å‚æ•°")
        logger.info(f"  - è¾“å‡ºå±‚: {output_params:,} å‚æ•° (tie_weights={tie_weights})")
        logger.info(f"ğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e6:.1f}M)")
        logger.info(f"ğŸ“Š å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {total_params * 4 / 1024**2:.1f} MB (FP32)")
        
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        raise
    
    # 5. åˆ›å»ºä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œæ··åˆç²¾åº¦æ”¯æŒ
    optimizer = create_optimizer(model, config)
    scheduler = create_lr_scheduler(optimizer, config)
    
    # è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒ
    scaler = None
    if config.get('training', {}).get('use_amp', True) and device.type == 'cuda':
        scaler = GradScaler()
        logger.info("ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    
    # 6. åˆ›å»ºcheckpointç®¡ç†å™¨å’Œè®­ç»ƒç›‘æ§å™¨
    checkpoint_manager = PretrainingCheckpointManager(
        save_dir=config['checkpoint']['save_dir'],
        keep_last_n=config['checkpoint'].get('keep_last_n_checkpoints', 3)
    )
    
    training_monitor = TrainingMonitor(
        log_dir=config['logging']['log_dir']
    )
    
    # 7. æ¢å¤è®­ç»ƒï¼ˆå¦‚æœæŒ‡å®šäº†checkpointï¼‰
    start_epoch = 0
    start_shard = 0
    start_step = 0
    
    if resume_checkpoint:
        checkpoint_data = checkpoint_manager.load_checkpoint(resume_checkpoint, model, optimizer, scaler)
        if checkpoint_data:
            start_epoch = checkpoint_data['epoch']
            start_shard = checkpoint_data['shard']
            start_step = checkpoint_data['step']
            logger.info(f"ğŸ”„ ä»checkpointæ¢å¤è®­ç»ƒ: epoch {start_epoch}, shard {start_shard}, step {start_step}")
    
    # 8. è®¡ç®—è®­ç»ƒæ­¥æ•°
    logger.info("ğŸ§® è®¡ç®—è®­ç»ƒæ­¥æ•°...")
    dataset_size = len(dataset)
    steps_per_shard, steps_per_epoch = config_manager.calculate_training_steps(dataset_size)
    
    # 9. å¼€å§‹ä¸‰å±‚åµŒå¥—å¾ªç¯è®­ç»ƒ
    logger.info("ğŸƒâ€â™‚ï¸ å¼€å§‹é¢„è®­ç»ƒå¾ªç¯...")
    
    training_config = config['training']
    num_epochs = training_config['num_epochs']
    shards_per_epoch = training_config['shards_per_epoch']
    
    global_step = start_step
    best_loss = float('inf')
    
    try:
        # ç¬¬ä¸€å±‚å¾ªç¯ï¼šè½®æ•° (Epochs)
        for epoch in range(start_epoch, num_epochs):
            logger.info(f"ğŸ”¥ å¼€å§‹ç¬¬ {epoch + 1}/{num_epochs} è½®è®­ç»ƒ")
            epoch_start_time = time.time()
            
            # é‡ç½®æ•°æ®æ”¶é›†å™¨
            dataset.reset_all_collectors()
            
            # ç¬¬äºŒå±‚å¾ªç¯ï¼šåˆ†ç‰‡ (Shards) - ç”¨äºå®šæœŸä¿å­˜checkpoint
            for shard in range(start_shard if epoch == start_epoch else 0, shards_per_epoch):
                logger.info(f"ğŸ§© å¼€å§‹ç¬¬ {shard + 1}/{shards_per_epoch} åˆ†ç‰‡")
                shard_start_time = time.time()
                shard_total_loss = 0.0
                
                # åˆ›å»ºåˆ†ç‰‡æ•°æ®é›†
                shard_samples = training_config['samples_per_shard']
                shard_dataset = ShardDataset(
                    mixed_dataset=dataset,
                    shard_size=shard_samples,
                    seq_length=config['model']['context_window']
                )
                
                # åˆ›å»ºDataLoader
                num_workers = 0 if use_multi_gpu else config.get('hardware', {}).get('dataloader_num_workers', 0)
                shard_loader = DataLoader(
                    shard_dataset,
                    batch_size=training_config['batch_size'],
                    shuffle=True,
                    num_workers=num_workers,
                    pin_memory=torch.cuda.is_available() and not use_multi_gpu,  # å¤šGPUæ—¶ä¸ä½¿ç”¨pin_memory
                    drop_last=True  # ç¡®ä¿batchå¤§å°ä¸€è‡´ï¼Œå¯¹å¤šGPUè®­ç»ƒå¾ˆé‡è¦
                )
                
                # ä½¿ç”¨å•ä¸ªè¿›åº¦æ¡æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
                desc = f"Epoch {epoch+1}/{num_epochs} | Shard {shard+1}/{shards_per_epoch} | Loss: N/A | LR: N/A | Step: {global_step}"
                pbar = tqdm(shard_loader, desc=desc)
                
                for batch_idx, (x, y) in enumerate(pbar):
                    step_start_time = time.time()
                    
                    try:
                        # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                        x = x.to(device)
                        y = y.to(device)
                        
                        # å‰å‘ä¼ æ’­
                        optimizer.zero_grad()
                        
                        if scaler:
                            # æ··åˆç²¾åº¦è®­ç»ƒ
                            with autocast():
                                logits = model(x)
                                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
                            
                            # åå‘ä¼ æ’­
                            scaler.scale(loss).backward()
                            
                            # æ¢¯åº¦è£å‰ª
                            if training_config.get('gradient_clip_val', 0) > 0:
                                scaler.unscale_(optimizer)
                                torch.nn.utils.clip_grad_norm_(
                                    model.parameters(), 
                                    training_config['gradient_clip_val']
                                )
                            
                            # ä¼˜åŒ–å™¨æ­¥è¿›
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            # å¸¸è§„è®­ç»ƒ
                            logits = model(x)
                            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
                            
                            # åå‘ä¼ æ’­
                            loss.backward()
                            
                            # æ¢¯åº¦è£å‰ª
                            if training_config.get('gradient_clip_val', 0) > 0:
                                torch.nn.utils.clip_grad_norm_(
                                    model.parameters(), 
                                    training_config['gradient_clip_val']
                                )
                            
                            # ä¼˜åŒ–å™¨æ­¥è¿›
                            optimizer.step()
                        
                        # ç«‹å³åˆ é™¤logitsä»¥é‡Šæ”¾å†…å­˜
                        del logits
                        
                        # å­¦ä¹ ç‡è°ƒåº¦
                        if scheduler:
                            scheduler.step()
                        
                        # æ›´æ–°ç»Ÿè®¡
                        current_loss = loss.item()
                        shard_total_loss += current_loss
                        
                        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                        step_elapsed = time.time() - step_start_time
                        current_lr = optimizer.param_groups[0]['lr']
                        
                        # æ›´æ–°è¿›åº¦æ¡æè¿°
                        samples_per_sec = training_config['batch_size'] / step_elapsed if step_elapsed > 0 else 0
                        desc = (f"Epoch {epoch+1}/{num_epochs} | Shard {shard+1}/{shards_per_epoch} | "
                               f"Loss: {current_loss:.4f} | LR: {current_lr:.2e} | "
                               f"Step: {global_step} | Speed: {samples_per_sec:.1f} samples/s")
                        pbar.set_description(desc)
                        
                        # è®°å½•è®­ç»ƒæ­¥éª¤
                        if global_step % training_config.get('log_every_n_steps', 100) == 0:
                            training_monitor.log_step(
                                epoch=epoch,
                                shard=shard,
                                step=global_step,
                                loss=current_loss,
                                lr=current_lr,
                                elapsed_time=step_elapsed,
                                samples_per_sec=samples_per_sec
                            )
                        
                        global_step += 1
                        
                    except RuntimeError as e:
                        if "out of memory" in str(e):
                            logger.error(f"âŒ GPUå†…å­˜ä¸è¶³: {e}")
                            # ç´§æ€¥æ¸…ç†
                            if 'x' in locals():
                                del x
                            if 'y' in locals():
                                del y
                            if 'logits' in locals():
                                del logits
                            if 'loss' in locals():
                                del loss
                            gc.collect()
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            raise
                        else:
                            logger.error(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥ (epoch {epoch}, shard {shard}, step {global_step}): {e}")
                            raise
                    except Exception as e:
                        logger.error(f"âŒ è®­ç»ƒæ­¥éª¤å¤±è´¥ (epoch {epoch}, shard {shard}, step {global_step}): {e}")
                        continue
                
                # å…³é—­è¿›åº¦æ¡
                pbar.close()
                
                # åˆ†ç‰‡å®Œæˆåçš„å¤„ç†
                shard_elapsed = time.time() - shard_start_time
                avg_shard_loss = shard_total_loss / len(shard_loader) if len(shard_loader) > 0 else 0
                
                logger.info(f"âœ… åˆ†ç‰‡ {shard + 1} å®Œæˆ")
                logger.info(f"  å¹³å‡æŸå¤±: {avg_shard_loss:.4f}")
                logger.info(f"  è€—æ—¶: {shard_elapsed:.2f}s")
                logger.info(f"  å¤„ç†æ‰¹æ¬¡æ•°: {len(shard_loader)}")
                
                # ä¿å­˜åˆ†ç‰‡çº§checkpoint
                checkpoint_manager.save_checkpoint(
                    model=model,
                    optimizer=optimizer,
                    epoch=epoch,
                    shard=shard,
                    step=global_step,
                    loss=avg_shard_loss,
                    config=config,
                    scaler=scaler,
                    extra_info={
                        'shard_elapsed_time': shard_elapsed,
                        'steps_completed': global_step,
                        'batches_processed': len(shard_loader)
                    }
                )
                
                # æ›´æ–°æœ€ä½³æŸå¤±
                if avg_shard_loss < best_loss:
                    best_loss = avg_shard_loss
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    checkpoint_manager.save_best_model(
                        model=model,
                        optimizer=optimizer,
                        epoch=epoch,
                        shard=shard,
                        step=global_step,
                        loss=best_loss,
                        config=config,
                        scaler=scaler
                    )
                
                # å†…å­˜æ¸…ç†
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            # é‡ç½®åˆ†ç‰‡èµ·å§‹ä½ç½®
            start_shard = 0
            
            # è½®æ¬¡å®Œæˆåçš„å¤„ç†
            epoch_elapsed = time.time() - epoch_start_time
            
            logger.info(f"ğŸ¯ ç¬¬ {epoch + 1} è½®è®­ç»ƒå®Œæˆ")
            logger.info(f"  æ€»è€—æ—¶: {epoch_elapsed / 3600:.2f} å°æ—¶")
            logger.info(f"  æœ€ä½³æŸå¤±: {best_loss:.4f}")
            
            # æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä¼°
            if len(dataset) > 0:
                eval_loss = evaluate_model(model, dataset, config, device)
                logger.info(f"ğŸ“Š è¯„ä¼°æŸå¤±: {eval_loss:.4f}")
                
                # è®°å½•è¯„ä¼°ç»“æœ
                training_monitor.log_step(
                    epoch=epoch,
                    shard=-1,  # è¡¨ç¤ºè¯„ä¼°æ­¥éª¤
                    step=global_step,
                    loss=eval_loss,
                    lr=optimizer.param_groups[0]['lr'],
                    elapsed_time=0
                )
    
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ—¥å¿—
        logger.info("ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ—¥å¿—...")
        
        # ä¿å­˜æœ€ç»ˆcheckpoint
        final_checkpoint_path = checkpoint_manager.save_checkpoint(
            model=model,
            optimizer=optimizer,
            epoch=epoch if 'epoch' in locals() else 0,
            shard=shard if 'shard' in locals() else 0,
            step=global_step,
            loss=best_loss,
            config=config,
            scaler=scaler,
            extra_info={
                'training_completed': True,
                'best_loss': best_loss
            }
        )
        
        # ä¿å­˜è®­ç»ƒæ—¥å¿—
        training_monitor.save_training_log()
        
        # ä¿å­˜é…ç½®æ–‡ä»¶å‰¯æœ¬
        config_backup_path = checkpoint_manager.checkpoint_dir / "config.yaml"
        with open(config_backup_path, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
        
        logger.info(f"ğŸ‰ é¢„è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {checkpoint_manager.checkpoint_dir}")
        logger.info(f"ğŸ’« æœ€ä½³æŸå¤±: {best_loss:.4f}")
        logger.info(f"ğŸ”¢ æ€»è®­ç»ƒæ­¥æ•°: {global_step}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="0.1Bæ¨¡å‹é¢„è®­ç»ƒè„šæœ¬")
    parser.add_argument(
        "--config", 
        type=str, 
        default="config/config.yaml",
        help="é…ç½®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--resume", 
        type=str, 
        default=None,
        help="æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„"
    )
    
    args = parser.parse_args()
    
    # è·å–é…ç½®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    config_path = os.path.join(os.path.dirname(__file__), args.config)
    
    if not os.path.exists(config_path):
        logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        return
    
    # å¼€å§‹è®­ç»ƒ
    train_model(config_path, args.resume)

if __name__ == "__main__":
    main()
