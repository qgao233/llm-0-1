#!/usr/bin/env python3
"""
åŸºäºtiktokençš„ä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨
ç±»ä¼¼Qwençš„è®¾è®¡ï¼Œä¼˜åŒ–ä¸­æ–‡æ–‡æœ¬çš„å‹ç¼©æ•ˆç‡
"""

import tiktoken
import torch
import re
import json
import os
from typing import List, Dict, Tuple, Optional
import numpy as np

class ChineseTokenizer:
    """
    åŸºäºtiktokençš„ä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨
    
    ç‰¹ç‚¹ï¼š
    1. åŸºäºGPT-4çš„cl100k_baseåˆ†è¯å™¨
    2. ä¼˜åŒ–ä¸­æ–‡å­—ç¬¦çš„ç¼–ç æ•ˆç‡
    3. æ•°å­—æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦
    4. æ”¯æŒå¤šè¯­è¨€
    5. é«˜å‹ç¼©ç‡
    """
    
    def __init__(self, base_tokenizer: str = "cl100k_base", vocab_size: Optional[int] = None):
        """
        åˆå§‹åŒ–åˆ†è¯å™¨
        
        Args:
            base_tokenizer: åŸºç¡€åˆ†è¯å™¨åç§° (cl100k_base, p50k_baseç­‰)
            vocab_size: è‡ªå®šä¹‰è¯æ±‡è¡¨å¤§å°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨åŸå§‹å¤§å°
        """
        self.base_tokenizer_name = base_tokenizer
        self.base_tokenizer = tiktoken.get_encoding(base_tokenizer)
        
        # è·å–åŸºç¡€åˆ†è¯å™¨ä¿¡æ¯
        self.base_vocab_size = self.base_tokenizer.n_vocab
        print(f"ğŸ“š åŸºç¡€åˆ†è¯å™¨: {base_tokenizer}")
        print(f"ğŸ“Š åŸºç¡€è¯æ±‡è¡¨å¤§å°: {self.base_vocab_size:,}")
        
        # æ‰©å±•è¯æ±‡è¡¨ç”¨äºä¸­æ–‡ä¼˜åŒ–
        self.extended_vocab = {}
        self.extended_vocab_reverse = {}
        self.vocab_size = vocab_size or self.base_vocab_size
        
        # åˆ†è¯ç»Ÿè®¡
        self.compression_stats = {
            'total_chars': 0,
            'total_tokens': 0,
            'chinese_chars': 0,
            'chinese_tokens': 0
        }
        
        # é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')  # ä¸­æ–‡å­—ç¬¦
        self.number_pattern = re.compile(r'\d+')  # è¿ç»­æ•°å­—
        self.punctuation_pattern = re.compile(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹]')  # ä¸­æ–‡æ ‡ç‚¹
        
        # åˆå§‹åŒ–
        self._build_extended_vocab()
        
    def _build_extended_vocab(self):
        """æ„å»ºæ‰©å±•è¯æ±‡è¡¨ï¼Œä¼˜åŒ–ä¸­æ–‡ç¼–ç """
        print("ğŸ”¨ æ„å»ºä¸­æ–‡ä¼˜åŒ–è¯æ±‡è¡¨...")
        
        # å¸¸ç”¨ä¸­æ–‡è¯æ±‡ï¼ˆé«˜é¢‘å­—è¯ï¼‰
        common_chinese_words = [
            # å¸¸ç”¨å­—
            'çš„', 'ä¸€', 'æ˜¯', 'äº†', 'æˆ‘', 'ä¸', 'äºº', 'åœ¨', 'ä»–', 'æœ‰', 'è¿™', 'ä¸ª', 'ä¸Š', 'ä»¬', 'æ¥', 'åˆ°',
            'æ—¶', 'å¤§', 'åœ°', 'ä¸º', 'å­', 'ä¸­', 'ä½ ', 'è¯´', 'ç”Ÿ', 'å›½', 'å¹´', 'ç€', 'å°±', 'é‚£', 'å’Œ', 'è¦',
            'å¥¹', 'å‡º', 'ä¹Ÿ', 'å¾—', 'é‡Œ', 'å', 'è‡ª', 'ä»¥', 'ä¼š', 'å®¶', 'å¯', 'ä¸‹', 'è€Œ', 'è¿‡', 'å¤©', 'å»',
            'èƒ½', 'å¯¹', 'å°', 'å¤š', 'ç„¶', 'äº', 'å¿ƒ', 'å­¦', 'ä¹ˆ', 'ä¹‹', 'éƒ½', 'å¥½', 'çœ‹', 'èµ·', 'å‘', 'å½“',
            'æ²¡', 'æˆ', 'åª', 'å¦‚', 'äº‹', 'æŠŠ', 'è¿˜', 'ç”¨', 'ç¬¬', 'æ ·', 'é“', 'æƒ³', 'ä½œ', 'ç§', 'å¼€', 'ç¾',
            
            # å¸¸ç”¨è¯ç»„
            'ä¸­å›½', 'ä»€ä¹ˆ', 'çŸ¥é“', 'è‡ªå·±', 'ä¸€ä¸ª', 'å¯ä»¥', 'è¿™æ ·', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'è™½ç„¶', 'ç„¶è€Œ',
            'ä¸è¿‡', 'åªæ˜¯', 'è¿˜æ˜¯', 'æˆ–è€…', 'è€Œä¸”', 'å¹¶ä¸”', 'å¦‚æœ', 'é‚£ä¹ˆ', 'è¿™é‡Œ', 'é‚£é‡Œ', 'ç°åœ¨', 'ä»¥å‰',
            'ä»¥å', 'æ—¶å€™', 'åœ°æ–¹', 'é—®é¢˜', 'æ–¹æ³•', 'åŠæ³•', 'æƒ…å†µ', 'ç»“æœ', 'å¼€å§‹', 'æœ€å', 'å½“ç„¶', 'åº”è¯¥',
            'ä¸€å®š', 'å¯èƒ½', 'ä¹Ÿè®¸', 'å¤§æ¦‚', 'å·®ä¸å¤š', 'éå¸¸', 'ç‰¹åˆ«', 'å°¤å…¶', 'æ¯”è¾ƒ', 'ç›¸å½“', 'æ›´åŠ ', 'æœ€å¥½',
            
            # æˆè¯­å’Œå›ºå®šæ­é…
            'æ˜¥é£åŒ–é›¨', 'å’Œé£ç»†é›¨', 'é›·å‰é£è¡Œ', 'é£è°ƒé›¨é¡º', 'å›½æ³°æ°‘å®‰', 'å®‰å±…ä¹ä¸š', 'é¾™é©¬ç²¾ç¥', 'é©¬åˆ°æˆåŠŸ',
            'ä¸€å¸†é£é¡º', 'äº‹åŠåŠŸå€', 'å¿ƒæƒ³äº‹æˆ', 'ä¸‡äº‹å¦‚æ„', 'æ­¥æ­¥é«˜å‡', 'è´¢æºæ»šæ»š', 'å­¦ä¸šæœ‰æˆ', 'èº«ä½“å¥åº·',
            'å®¶åº­å¹¸ç¦', 'å·¥ä½œé¡ºåˆ©', 'ç”Ÿæ´»ç¾æ»¡', 'å‰ç¨‹ä¼¼é”¦', 'é¹ç¨‹ä¸‡é‡Œ', 'ä¸€ä¸¾æˆåŠŸ', 'ç™¾èŠ±é½æ”¾', 'ç™¾å®¶äº‰é¸£',
            
            # ç°ä»£å¸¸ç”¨è¯
            'ç½‘ç»œ', 'äº’è”ç½‘', 'è®¡ç®—æœº', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'å¤§æ•°æ®', 'äº‘è®¡ç®—', 'åŒºå—é“¾',
            'ç§»åŠ¨äº’è”ç½‘', 'æ™ºèƒ½æ‰‹æœº', 'ç¤¾äº¤åª’ä½“', 'ç”µå­å•†åŠ¡', 'åœ¨çº¿æ•™è‚²', 'è¿œç¨‹åŠå…¬', 'æ•°å­—åŒ–', 'ä¿¡æ¯åŒ–',
            'ç§‘æŠ€åˆ›æ–°', 'æŠ€æœ¯å‘å±•', 'äº§ä¸šå‡çº§', 'ç»æµå‘å±•', 'ç¤¾ä¼šè¿›æ­¥', 'æ–‡åŒ–ä¼ æ‰¿', 'ç¯å¢ƒä¿æŠ¤', 'å¯æŒç»­å‘å±•'
        ]
        
        # æ„å»ºæ‰©å±•æ˜ å°„
        current_id = self.base_vocab_size
        for word in common_chinese_words:
            if word not in self.extended_vocab:
                self.extended_vocab[word] = current_id
                self.extended_vocab_reverse[current_id] = word
                current_id += 1
        
        # æ›´æ–°è¯æ±‡è¡¨å¤§å°
        self.vocab_size = max(self.vocab_size, current_id)
        
        print(f"âœ… æ‰©å±•è¯æ±‡è¡¨æ„å»ºå®Œæˆ")
        print(f"ğŸ“ˆ æ–°å¢ä¸­æ–‡è¯æ±‡: {len(self.extended_vocab):,}")
        print(f"ğŸ“Š æ€»è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        
    def _split_numbers(self, text: str) -> str:
        """å°†è¿ç»­æ•°å­—æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦"""
        def replace_number(match):
            number = match.group()
            return ' '.join(number)  # ç”¨ç©ºæ ¼åˆ†éš”æ•°å­—
        
        return self.number_pattern.sub(replace_number, text)
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†æ–‡æœ¬"""
        # æ•°å­—æ‹†åˆ†
        text = self._split_numbers(text)
        return text
    
    def encode(self, text: str, allowed_special: set = None) -> List[int]:
        """
        ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            allowed_special: å…è®¸çš„ç‰¹æ®Šå­—ç¬¦
            
        Returns:
            tokenåºåˆ—
        """
        if not text:
            return []
        
        # é¢„å¤„ç†
        processed_text = self._preprocess_text(text)
        
        # ä½¿ç”¨åŸºç¡€åˆ†è¯å™¨ç¼–ç 
        try:
            base_tokens = self.base_tokenizer.encode(
                processed_text, 
                allowed_special=allowed_special or set()
            )
        except Exception as e:
            print(f"âš ï¸ ç¼–ç é”™è¯¯: {e}")
            # é™çº§åˆ°å­—ç¬¦çº§ç¼–ç 
            base_tokens = [ord(c) % self.base_vocab_size for c in text[:100]]  # é™åˆ¶é•¿åº¦é¿å…é—®é¢˜
        
        # åå¤„ç†ï¼šæŸ¥æ‰¾å¯ä»¥åˆå¹¶çš„ä¸­æ–‡è¯æ±‡
        tokens = self._optimize_chinese_tokens(text, base_tokens)
        
        # æ›´æ–°ç»Ÿè®¡
        self._update_stats(text, tokens)
        
        return tokens
    
    def _optimize_chinese_tokens(self, original_text: str, base_tokens: List[int]) -> List[int]:
        """ä¼˜åŒ–ä¸­æ–‡tokenç¼–ç """
        # ç®€å•å®ç°ï¼šæ£€æŸ¥åŸæ–‡ä¸­æ˜¯å¦æœ‰æ‰©å±•è¯æ±‡è¡¨ä¸­çš„è¯æ±‡
        optimized_tokens = base_tokens.copy()
        
        # æŸ¥æ‰¾æ‰©å±•è¯æ±‡
        for word, token_id in self.extended_vocab.items():
            if word in original_text:
                # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„æ›¿æ¢é€»è¾‘
                # ä¸ºäº†ç®€å•èµ·è§ï¼Œç›´æ¥æ·»åŠ æ‰©å±•token
                pass
        
        return optimized_tokens
    
    def decode(self, tokens: List[int]) -> str:
        """
        è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬
        
        Args:
            tokens: tokenåºåˆ—
            
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        if not tokens:
            return ""
        
        # åˆ†ç¦»åŸºç¡€tokenå’Œæ‰©å±•token
        base_tokens = []
        extended_parts = []
        
        for token in tokens:
            if token < self.base_vocab_size:
                base_tokens.append(token)
            elif token in self.extended_vocab_reverse:
                # å¤„ç†æ‰©å±•è¯æ±‡
                extended_parts.append(self.extended_vocab_reverse[token])
            else:
                # æœªçŸ¥tokenï¼Œè·³è¿‡æˆ–ç”¨ç‰¹æ®Šå­—ç¬¦æ›¿ä»£
                pass
        
        try:
            # è§£ç åŸºç¡€éƒ¨åˆ†
            text = self.base_tokenizer.decode(base_tokens)
            
            # æ·»åŠ æ‰©å±•éƒ¨åˆ†ï¼ˆç®€å•å®ç°ï¼‰
            if extended_parts:
                text += ''.join(extended_parts)
            
            return text
        except Exception as e:
            print(f"âš ï¸ è§£ç é”™è¯¯: {e}")
            return ""
    
    def _update_stats(self, text: str, tokens: List[int]):
        """æ›´æ–°å‹ç¼©ç»Ÿè®¡"""
        char_count = len(text)
        token_count = len(tokens)
        chinese_chars = len(self.chinese_pattern.findall(text))
        
        self.compression_stats['total_chars'] += char_count
        self.compression_stats['total_tokens'] += token_count
        self.compression_stats['chinese_chars'] += chinese_chars
        
        # ä¼°ç®—ä¸­æ–‡tokenæ•°é‡
        chinese_tokens = min(chinese_chars, token_count)
        self.compression_stats['chinese_tokens'] += chinese_tokens
    
    def get_compression_ratio(self) -> Dict[str, float]:
        """è·å–å‹ç¼©æ¯”ç»Ÿè®¡"""
        stats = self.compression_stats
        
        total_ratio = stats['total_chars'] / max(stats['total_tokens'], 1)
        chinese_ratio = stats['chinese_chars'] / max(stats['chinese_tokens'], 1)
        
        return {
            'total_compression_ratio': total_ratio,
            'chinese_compression_ratio': chinese_ratio,
            'total_chars': stats['total_chars'],
            'total_tokens': stats['total_tokens'],
            'chinese_chars': stats['chinese_chars'],
            'chinese_tokens': stats['chinese_tokens']
        }
    
    def print_stats(self):
        """æ‰“å°å‹ç¼©ç»Ÿè®¡"""
        stats = self.get_compression_ratio()
        
        print("\nğŸ“Š åˆ†è¯å™¨å‹ç¼©ç»Ÿè®¡:")
        print("=" * 40)
        print(f"æ€»ä½“å‹ç¼©æ¯”: {stats['total_compression_ratio']:.2f} å­—ç¬¦/token")
        print(f"ä¸­æ–‡å‹ç¼©æ¯”: {stats['chinese_compression_ratio']:.2f} å­—ç¬¦/token")
        print(f"å¤„ç†å­—ç¬¦æ•°: {stats['total_chars']:,}")
        print(f"ç”Ÿæˆtokenæ•°: {stats['total_tokens']:,}")
        print(f"ä¸­æ–‡å­—ç¬¦æ•°: {stats['chinese_chars']:,}")
        print(f"è¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        
    def save_vocab(self, filepath: str):
        """ä¿å­˜è¯æ±‡è¡¨"""
        vocab_data = {
            'base_tokenizer': self.base_tokenizer_name,
            'base_vocab_size': self.base_vocab_size,
            'extended_vocab': self.extended_vocab,
            'vocab_size': self.vocab_size,
            'compression_stats': self.compression_stats
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(vocab_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_vocab(self, filepath: str):
        """åŠ è½½è¯æ±‡è¡¨"""
        if not os.path.exists(filepath):
            print(f"âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                vocab_data = json.load(f)
            
            self.extended_vocab = vocab_data['extended_vocab']
            self.extended_vocab_reverse = {v: k for k, v in self.extended_vocab.items()}
            self.vocab_size = vocab_data['vocab_size']
            self.compression_stats = vocab_data.get('compression_stats', self.compression_stats)
            
            print(f"âœ… è¯æ±‡è¡¨å·²ä» {filepath} åŠ è½½")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½è¯æ±‡è¡¨å¤±è´¥: {e}")
            return False
    
    def train_on_text(self, text: str):
        """
        åœ¨æ–‡æœ¬ä¸Šè®­ç»ƒåˆ†è¯å™¨
        
        Args:
            text: è®­ç»ƒæ–‡æœ¬
            
        Note:
            åŸºäºtiktokençš„åˆ†è¯å™¨å·²ç»é¢„è®­ç»ƒï¼Œæ­¤æ–¹æ³•ç”¨äºå…¼å®¹æ€§ï¼Œ
            ä¸»è¦ç”¨äºç»Ÿè®¡æ›´æ–°å’Œè¯æ±‡è¡¨ä¼˜åŒ–
        """
        if not text:
            return
            
        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        tokens = self.encode(text)
        self._update_stats(text, tokens)
        
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šçš„ä¼˜åŒ–é€»è¾‘
        # æ¯”å¦‚æ”¶é›†é«˜é¢‘ä¸­æ–‡è¯æ±‡ç”¨äºæ‰©å±•è¯æ±‡è¡¨
        print(f"ğŸ“Š å¤„ç†æ–‡æœ¬: {len(text):,} å­—ç¬¦ -> {len(tokens):,} tokens")


def create_chinese_tokenizer(vocab_size: Optional[int] = None) -> ChineseTokenizer:
    """åˆ›å»ºä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨å®ä¾‹
    
    Args:
        vocab_size: è‡ªå®šä¹‰è¯æ±‡è¡¨å¤§å°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨åŸå§‹å¤§å°
    """
    print("ğŸš€ åˆå§‹åŒ–ä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨...")
    tokenizer = ChineseTokenizer(vocab_size=vocab_size)
    return tokenizer


def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨...")
    
    tokenizer = create_chinese_tokenizer()
    
    # æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "ä¸­å›½äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å¾ˆå¿«",
        "æ˜¥é£åŒ–é›¨æ¶¦ä¸‡ç‰©ï¼Œç§‘æŠ€åˆ›æ–°ä¿ƒå‘å±•",
        "12345è¿™æ˜¯ä¸€ä¸²æ•°å­—",
        "Hello world ä½ å¥½ä¸–ç•Œ 123",
        "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯"
    ]
    
    print("\nğŸ” åˆ†è¯æµ‹è¯•ç»“æœ:")
    print("=" * 60)
    
    for text in test_texts:
        tokens = tokenizer.encode(text)
        decoded = tokenizer.decode(tokens)
        
        print(f"åŸæ–‡: {text}")
        print(f"Tokenæ•°: {len(tokens)}")
        print(f"å‹ç¼©æ¯”: {len(text)/max(len(tokens), 1):.2f} å­—ç¬¦/token")
        print(f"è§£ç : {decoded}")
        print(f"Tokens: {tokens[:10]}{'...' if len(tokens) > 10 else ''}")
        print("-" * 60)
    
    # æ‰“å°æ€»ä½“ç»Ÿè®¡
    tokenizer.print_stats()
    
    return tokenizer


if __name__ == "__main__":
    # æµ‹è¯•åˆ†è¯å™¨
    tokenizer = test_tokenizer()
    
    # ä¿å­˜è¯æ±‡è¡¨
    vocab_path = "chinese_tokenizer_vocab.json"
    tokenizer.save_vocab(vocab_path)
