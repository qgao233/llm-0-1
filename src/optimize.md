# 将LlaMa的算子加入到上面的简易模型架构`seq2seq_base.py`中

主要包括：

- RMS_Norm
- RoPE
- SwiGLU

## RMSNorm快速了解

norm，做标准化，训练过程中的张量标准化操作，通过计算均值和方差，将样本进行归一化。 

在大学课程《概率与统计》我们学过，样本的均值代表样本的特征，而方差代表离散程度。

因此，通过计算，让数据变为均值为0，方差为1的数据。 这样可以使数据服从标准的正态分布。

记得大学时候，老师讲这一段的时候，着重强调：“高斯分布，正态分布”，也可以叫自然分布，自然界的很多统计情况，几乎都满足高斯分布。 两边向中心靠拢，超过中心的，随着逐渐增大，会越来越少，没超过中心的，距离中心越远，数量也越来越少。而分布的众数永远都是在中间。

啊~ 数学之美，但是也美不过高数老师（嘿嘿）。

使用均值和方差计算数据的标准差，这样既保留了数据的异常值，同时维持数据的异常结构，这样可以稳定梯度，让梯度变化更稳定，减少梯度消失或者爆炸的问题，因为维持了异常结构，也能减少过拟合问题，增强泛化能力。



RMSNorm出来之前，广泛使用的batch_normlize，针对批次数据做标准化。标准化的数值是一个batch作为一个样本总体，计算其均值与方差。



而后，又出现了layer_norm，其是针对每个token的特征向量做归一化处理（不知道特征向量，请看本人之前的rope文章。应该可以理解token和特征向量的关系。）依旧需要计算均值和方差。



RMSNorm和layer_norm的主要区别在于RMSNorm不需要同时计算均值和方差两个统计量，而只需要计算均方根这一个统计量。在模型表现效果几乎与layer_norm持平的前提下，节省7%-64%的计算量。

RMS_Norm计算公式：