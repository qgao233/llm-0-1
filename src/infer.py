from llmcore import (
    create_model, 
    get_device
)
from data_processor import DataProcessor, download_and_prepare_data, decode_text
from config_manager import get_config_manager
import torch
import torch.nn.functional as F
import json
import os

def top_k_sampling(logits, k=50):
    """Top-Ké‡‡æ ·ï¼šåªä¿ç•™æ¦‚ç‡æœ€é«˜çš„kä¸ªtoken"""
    if k <= 0:
        return logits
    
    # è·å–top-kçš„å€¼å’Œç´¢å¼•
    top_k_values, top_k_indices = torch.topk(logits, k, dim=-1)
    
    # åˆ›å»ºä¸€ä¸ªä¸logitsåŒæ ·å¤§å°çš„å¼ é‡ï¼Œå¡«å……ä¸ºè´Ÿæ— ç©·
    filtered_logits = torch.full_like(logits, float('-inf'))
    
    # åªä¿ç•™top-kçš„å€¼
    filtered_logits.scatter_(-1, top_k_indices, top_k_values)
    
    return filtered_logits

def top_p_sampling(logits, p=0.9):
    """Top-P (nucleus)é‡‡æ ·ï¼šä¿ç•™ç´¯ç§¯æ¦‚ç‡è¾¾åˆ°pçš„æœ€å°tokené›†åˆ"""
    if p >= 1.0:
        return logits
    
    # æŒ‰æ¦‚ç‡é™åºæ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
    
    # è®¡ç®—softmaxæ¦‚ç‡
    sorted_probs = F.softmax(sorted_logits, dim=-1)
    
    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„ä½ç½®
    sorted_indices_to_remove = cumulative_probs > p
    
    # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡pçš„tokenï¼ˆç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªtokenï¼‰
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = False
    
    # åˆ›å»ºè¿‡æ»¤åçš„logits
    filtered_logits = sorted_logits.clone()
    filtered_logits[sorted_indices_to_remove] = float('-inf')
    
    # å°†ç»“æœæ˜ å°„å›åŸå§‹é¡ºåº
    original_logits = torch.full_like(logits, float('-inf'))
    original_logits.scatter_(-1, sorted_indices, filtered_logits)
    
    return original_logits

def sample_next_token(logits, temperature=1.0, top_k=0, top_p=1.0):
    """
    ä½¿ç”¨temperatureã€top-kå’Œtop-pé‡‡æ ·ä¸‹ä¸€ä¸ªtoken
    
    Args:
        logits: æ¨¡å‹è¾“å‡ºçš„logits
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ (0.1-2.0)
        top_k: top-ké‡‡æ ·å‚æ•°ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨
        top_p: top-pé‡‡æ ·å‚æ•°ï¼Œ1.0è¡¨ç¤ºä¸ä½¿ç”¨
    """
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    if temperature != 1.0:
        logits = logits / temperature
    
    # åº”ç”¨top-ké‡‡æ ·
    if top_k > 0:
        logits = top_k_sampling(logits, top_k)
    
    # åº”ç”¨top-pé‡‡æ ·
    if top_p < 1.0:
        logits = top_p_sampling(logits, top_p)
    
    # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
    probs = F.softmax(logits, dim=-1)
    
    # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
    next_token = torch.multinomial(probs, num_samples=1)
    
    return next_token

def generate(model, itos, config, max_new_tokens=100, temperature=1.0, top_k=50, top_p=0.9, seed_text=None, data_processor=None):
    """
    ç”Ÿæˆæ–‡æœ¬ - ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        itos: ç´¢å¼•åˆ°å­—ç¬¦çš„æ˜ å°„
        config: é…ç½®å­—å…¸
        max_new_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°
        temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ (0.1-2.0)
        top_k: top-ké‡‡æ ·å‚æ•°ï¼Œ0è¡¨ç¤ºä¸ä½¿ç”¨
        top_p: top-pé‡‡æ ·å‚æ•°ï¼Œ1.0è¡¨ç¤ºä¸ä½¿ç”¨
        seed_text: ç§å­æ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™éšæœºç”Ÿæˆ
    """
    device = config.get('device', torch.device('cpu'))
    
    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šä¸”å¤„äºè¯„ä¼°æ¨¡å¼
    model = model.to(device)
    model.eval()
    
    if seed_text is not None:
        # ä½¿ç”¨æä¾›çš„ç§å­æ–‡æœ¬
        print(f"ä½¿ç”¨ç§å­æ–‡æœ¬: '{seed_text}'")
        # è¿™é‡Œéœ€è¦stoiæ¥ç¼–ç ç§å­æ–‡æœ¬ï¼Œæš‚æ—¶ä½¿ç”¨éšæœºåˆå§‹åŒ–
        idx = torch.randint(1, min(4000, config.get('vocab_size', 4000)), (1, len(seed_text))).long().to(device)
    else:
        # ç”Ÿæˆéšæœºæ•°ï¼Œä½œä¸ºè¾“å…¥æ•°æ®
        idx = torch.randint(1, min(4000, config.get('vocab_size', 4000)), (5, 1)).long().to(device)
        print("éšæœºç”Ÿæˆçš„å­—ç¬¦:", [itos[i.item()] for i in idx])
    
    print(f"é‡‡æ ·å‚æ•°: temperature={temperature}, top_k={top_k}, top_p={top_p}")
    print("åˆå§‹åºåˆ—å½¢çŠ¶:", idx.shape)
    
    # å¯ç”¨æ¨ç†ä¼˜åŒ–
    if device.type == 'cuda':
        # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦åŠ é€Ÿæ¨ç†
        torch.backends.cudnn.benchmark = True
    
    # ç”Ÿæˆå¾ªç¯
    generated_count = 0
    for step in range(max_new_tokens):
        # è·å–å½“å‰ä¸Šä¸‹æ–‡çª—å£çš„è¾“å…¥
        context = idx[:, -config['context_window']:]
        
        # å‰å‘ä¼ æ’­è·å–logits
        with torch.no_grad():
            logits = model(context)
        
        # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„logits
        last_time_step_logits = logits[:, -1, :]
        
        # ä½¿ç”¨æ”¹è¿›çš„é‡‡æ ·æ–¹æ³•
        idx_next = sample_next_token(
            last_time_step_logits, 
            temperature=temperature, 
            top_k=top_k, 
            top_p=top_p
        )
        
        # æ‹¼æ¥æ–°ç”Ÿæˆçš„token
        idx = torch.cat([idx, idx_next], dim=-1)
        generated_count += 1
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†è‡ªç„¶çš„åœæ­¢ç¬¦
        new_char = data_processor.decode_text([idx_next[0].item()])
        if new_char in ['ã€‚', 'ï¼', 'ï¼Ÿ', '\n'] and generated_count >= 20:
            # ç”Ÿæˆäº†å¥å·ç­‰ç»“æŸç¬¦ï¼Œä¸”å·²ç”Ÿæˆè¶³å¤Ÿé•¿åº¦ï¼Œå¯ä»¥åœæ­¢
            break
        
        # å¯é€‰ï¼šæ‰“å°ç”Ÿæˆè¿›åº¦
        if step % 20 == 0 and step > 0:
            print(f"ç”Ÿæˆè¿›åº¦: {step}/{max_new_tokens}")
    
    # è§£ç ç”Ÿæˆçš„åºåˆ—
    print("æœ€ç»ˆåºåˆ—å½¢çŠ¶:", idx.shape)
    print("å‰10ä¸ªtoken IDs:", idx[0][:10].tolist() if idx.size(0) > 0 else "ç©ºåºåˆ—")
    
    # æ£€æŸ¥itosè¯è¡¨æ˜¯å¦æ­£å¸¸
    print(f"è¯è¡¨æ ·ä¾‹: {list(itos.items())[:5]}")
    
    generated_texts = []
    for i, token_ids in enumerate(idx.tolist()):
        try:
            # ä½¿ç”¨data_processorè¿›è¡Œè§£ç ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if data_processor:
                text = data_processor.decode_text(token_ids)
            else:
                # é™çº§åˆ°ä½¿ç”¨itos
                valid_token_ids = [tid for tid in token_ids if tid in itos]
                if len(valid_token_ids) != len(token_ids):
                    print(f"âš ï¸ åºåˆ—{i+1}ä¸­æœ‰{len(token_ids) - len(valid_token_ids)}ä¸ªæ— æ•ˆtoken ID")
                text = decode_text(valid_token_ids, itos=itos)
            
            generated_texts.append(text)
            print(f"åºåˆ—{i+1}è§£ç : {text[:50]}{'...' if len(text) > 50 else ''}")
        except Exception as e:
            print(f"âŒ åºåˆ—{i+1}è§£ç å¤±è´¥: {e}")
            generated_texts.append(f"<è§£ç å¤±è´¥: {e}>")
    
    return generated_texts

def warmup_model(model, config, warmup_steps=3):
    """æ¨¡å‹é¢„çƒ­ï¼Œå‡å°‘é¦–æ¬¡æ¨ç†å»¶è¿Ÿ"""
    print("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    device = config['device']
    
    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥è¿›è¡Œé¢„çƒ­
    dummy_input = torch.randint(
        1, min(100, config.get('vocab_size', 4000)), 
        (1, config['context_window']), 
        device=device
    )
    
    model.eval()
    with torch.no_grad():
        for i in range(warmup_steps):
            # å‰å‘ä¼ æ’­é¢„çƒ­
            _ = model(dummy_input)
            
            # å¦‚æœæ˜¯GPUï¼ŒåŒæ­¥ä¸€ä¸‹ç¡®ä¿æ“ä½œå®Œæˆ
            if device.type == 'cuda':
                torch.cuda.synchronize()
    
    print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")

def load_model_and_config(model_dir="./model_save", enable_warmup=True):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    # åŠ è½½é…ç½®
    config_path = os.path.join(model_dir, "config.json")
    with open(config_path, 'r') as f:
        config = json.load(f)

    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
    if 'device' not in config and 'device_type' not in config:
        config['device'] = get_device()
    else:
        # ä»device_typeé‡å»ºdeviceå¯¹è±¡
        if 'device_type' in config:
            device_str = config['device_type']
            if 'cuda' in device_str and torch.cuda.is_available():
                config['device'] = torch.device('cuda')
            else:
                config['device'] = torch.device('cpu')
        elif 'device' in config:
            # å…¼å®¹æ—§ç‰ˆæœ¬é…ç½®
            if config['device'] == 'cuda' and not torch.cuda.is_available():
                print("è­¦å‘Š: é…ç½®æ–‡ä»¶æŒ‡å®šä½¿ç”¨CUDAï¼Œä½†CUDAä¸å¯ç”¨ï¼Œæ”¹ç”¨CPU")
                config['device'] = torch.device('cpu')
            elif isinstance(config['device'], str):
                config['device'] = torch.device(config['device'])

    print("åŠ è½½æ¨¡å‹é…ç½®:", config)
    print(f"æ¨ç†è®¾å¤‡: {config['device']}")

    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)

    # åŠ è½½æ¨¡å‹æƒé‡
    model_save_path = os.path.join(model_dir, "pytorch_model.bin")
    print("åŠ è½½æ¨¡å‹æƒé‡...")
    # æ ¹æ®å½“å‰è®¾å¤‡åŠ è½½æ¨¡å‹æƒé‡
    device = config['device']
    if device.type == 'cuda':
        state_dict = torch.load(model_save_path)
    else:
        # å¦‚æœå½“å‰ä½¿ç”¨CPUä½†æ¨¡å‹æ˜¯åœ¨GPUä¸Šè®­ç»ƒçš„ï¼Œéœ€è¦æ˜ å°„åˆ°CPU
        state_dict = torch.load(model_save_path, map_location='cpu')

    # è¿‡æ»¤æ‰ç¼“å­˜ç›¸å…³çš„é”®ï¼Œè¿™äº›ä¸æ˜¯æ¨¡å‹å‚æ•°
    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.endswith('.R_cache')}

    # åŠ è½½è¿‡æ»¤åçš„çŠ¶æ€å­—å…¸
    missing_keys, unexpected_keys = model.load_state_dict(filtered_state_dict, strict=False)

    if missing_keys:
        print(f"è­¦å‘Š: ç¼ºå°‘çš„é”®: {missing_keys}")
    if unexpected_keys:
        print(f"è­¦å‘Š: æ„å¤–çš„é”®: {unexpected_keys}")

    print("æ¨¡å‹æƒé‡åŠ è½½å®Œæˆ")

    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    print(f"æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {device}")

    # æ¨¡å‹é¢„çƒ­
    if enable_warmup:
        warmup_model(model, config)

    return model, config

def run_inference_demo(config_path="config/config.yaml"):
    """è¿è¡Œæ¨ç†æ¼”ç¤º"""
    try:
        # åŠ è½½é…ç½®ç®¡ç†å™¨
        config_manager = get_config_manager(config_path)
        inference_config = config_manager.get_inference_config()
        
        # åŠ è½½æ¨¡å‹
        model, config = load_model_and_config(enable_warmup=inference_config.get('enable_warmup', True))
        
        # å‡†å¤‡æ•°æ®ï¼ˆä¸»è¦æ˜¯ä¸ºäº†è·å–è¯è¡¨ï¼‰
        print("å‡†å¤‡è¯è¡¨...")
        data_config = config_manager.get_data_config()
        
        # æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œç¡®ä¿ä½¿ç”¨ç›¸åŒçš„åˆ†è¯å™¨
        use_bpe = data_config.get('use_bpe_tokenizer', False)  # é»˜è®¤falseç¡®ä¿å…¼å®¹æ€§
        print(f"ğŸ”¤ ä½¿ç”¨åˆ†è¯å™¨ç±»å‹: {'BPE' if use_bpe else 'å­—ç¬¦çº§'}")
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨ä»¥ä¾¿æ­£ç¡®è§£ç 
        tokenizer_type = "chinese_bpe" if use_bpe else "char_level"
        vocab_path = data_config.get('vocab_cache_path', 'chinese_tokenizer_vocab.json') if use_bpe else None
        data_processor = DataProcessor(tokenizer_type=tokenizer_type, vocab_path=vocab_path)
        
        dataset, vocab, itos, stoi = data_processor.download_and_prepare_data(
            data_file=data_config.get('data_file', 'xiyouji.txt'),
            force_download=data_config.get('force_download', False)
        )

        # æ£€æŸ¥è¯è¡¨å¤§å°æ˜¯å¦åŒ¹é…
        model_vocab_size = config.get('vocab_size', 0)
        data_vocab_size = len(vocab)
        print(f"ğŸ“Š æ¨¡å‹è¯è¡¨å¤§å°: {model_vocab_size}")
        print(f"ğŸ“Š æ•°æ®è¯è¡¨å¤§å°: {data_vocab_size}")
        
        if model_vocab_size != data_vocab_size:
            print(f"âš ï¸ è­¦å‘Š: è¯è¡¨å¤§å°ä¸åŒ¹é…! æ¨¡å‹={model_vocab_size}, æ•°æ®={data_vocab_size}")
            print("è¿™å¯èƒ½ä¼šå¯¼è‡´è§£ç é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ†è¯å™¨ç±»å‹")
        
        # è¿›è¡Œæ¨ç†
        print("å¼€å§‹æ¨ç†...")
        print(f"ğŸ“‚ ä½¿ç”¨é…ç½®æ–‡ä»¶: {config_path}")
        
        # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤æ¨ç†å‚æ•°
        default_max_tokens = inference_config.get('max_new_tokens', 100)
        default_temperature = inference_config.get('temperature', 0.8)
        default_top_k = inference_config.get('top_k', 50)
        default_top_p = inference_config.get('top_p', 0.9)

        # æµ‹è¯•ä¸åŒçš„é‡‡æ ·ç­–ç•¥ - å¢åŠ è¾“å‡ºé•¿åº¦
        sampling_configs = [
            {"name": "è´ªå©ªé‡‡æ ·", "temperature": 0.1, "top_k": 1, "top_p": 1.0, "max_tokens": 80},
            {"name": "é…ç½®æ–‡ä»¶é»˜è®¤", "temperature": default_temperature, "top_k": default_top_k, "top_p": default_top_p, "max_tokens": default_max_tokens},
            {"name": "åˆ›æ„é‡‡æ ·", "temperature": 1.2, "top_k": 100, "top_p": 0.95, "max_tokens": 120},
            {"name": "éšæœºé‡‡æ ·", "temperature": 1.5, "top_k": 0, "top_p": 1.0, "max_tokens": 80}
        ]

        for config_item in sampling_configs:
            print(f"\n=== {config_item['name']} ===")
            output = generate(
                model, itos, config, 
                max_new_tokens=config_item['max_tokens'],
                temperature=config_item['temperature'],
                top_k=config_item['top_k'],
                top_p=config_item['top_p'],
                data_processor=data_processor
            )
            print("ç”Ÿæˆç»“æœ:")
            for i, text in enumerate(output):
                print(f"åºåˆ— {i+1}: {text}")
            print("-" * 50)
            
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        print("ğŸ’¡ è¯·ç¡®ä¿å·²ç»è®­ç»ƒäº†æ¨¡å‹å¹¶ä¿å­˜åœ¨ ./model_save/ ç›®å½•ä¸‹")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è¿è¡Œæ¨¡å‹æ¨ç†æ¼”ç¤º')
    parser.add_argument('--config', default='config/config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨æ¨¡å‹æ¨ç†æ¼”ç¤º...")
    print(f"ğŸ“‚ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    run_inference_demo(config_path=args.config)
    print("ğŸ‰ æ¨ç†æ¼”ç¤ºå®Œæˆï¼")
