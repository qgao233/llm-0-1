#!/usr/bin/env python3
"""
3Bå‚æ•°LLMæ¨¡å‹æ ¸å¿ƒå®ç°
åŸºäºTransformeræ¶æ„ï¼Œé’ˆå¯¹30äº¿å‚æ•°è§„æ¨¡ä¼˜åŒ–
"""

import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
import math
from collections import OrderedDict
from typing import Optional, Tuple

def get_device(force_cpu=False, device_config=None):
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„è®¾å¤‡ï¼Œæ”¯æŒå¤šGPU - 3Bæ¨¡å‹ç‰ˆæœ¬"""
    if force_cpu:
        device = torch.device("cpu")
        print("âš ï¸  å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œè®¡ç®— - æ³¨æ„ï¼š3Bæ¨¡å‹åœ¨CPUä¸Šä¼šéå¸¸æ…¢")
        return device, False, []
    
    if not torch.cuda.is_available():
        device = torch.device("cpu")
        print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œè®¡ç®— - è­¦å‘Šï¼š3Bæ¨¡å‹éœ€è¦å¤§é‡å†…å­˜å’Œæ—¶é—´")
        return device, False, []
    
    # è·å–å¯ç”¨çš„GPUæ•°é‡å’Œä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªå¯ç”¨GPU:")
    
    available_gpus = []
    total_memory = 0
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        available_gpus.append(i)
        total_memory += gpu_memory
    
    print(f"ğŸ“Š æ€»GPUå†…å­˜: {total_memory:.1f} GB")
    
    # 3Bæ¨¡å‹å†…å­˜éœ€æ±‚æ£€æŸ¥
    estimated_memory_gb = 12  # 3Bæ¨¡å‹å¤§çº¦éœ€è¦12GBå†…å­˜ï¼ˆFP16ï¼‰
    if total_memory < estimated_memory_gb:
        print(f"âš ï¸  è­¦å‘Š: 3Bæ¨¡å‹é¢„è®¡éœ€è¦çº¦{estimated_memory_gb}GBå†…å­˜ï¼Œå½“å‰æ€»å†…å­˜{total_memory:.1f}GBå¯èƒ½ä¸è¶³")
        print("ğŸ’¡ å»ºè®®å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒå’Œæ¢¯åº¦æ£€æŸ¥ç‚¹æ¥èŠ‚çœå†…å­˜")
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šGPUè®­ç»ƒ
    use_multi_gpu = gpu_count > 1
    if device_config and 'multi_gpu' in device_config:
        use_multi_gpu = device_config['multi_gpu'] and gpu_count > 1
    
    # é€‰æ‹©ä¸»GPUè®¾å¤‡
    primary_device = torch.device("cuda:0")
    
    if use_multi_gpu:
        print(f"ğŸš€ å¯ç”¨å¤šGPUè®­ç»ƒï¼Œä½¿ç”¨ {gpu_count} ä¸ªGPU - æ¨èç”¨äº3Bæ¨¡å‹")
        return primary_device, True, available_gpus
    else:
        print(f"ğŸ“± ä½¿ç”¨å•GPUè®­ç»ƒ: {torch.cuda.get_device_name(0)}")
        return primary_device, False, [0]


class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization - 3Bæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬"""
    def __init__(self, layer_shape, eps=1e-8, bias=False):
        super(RMSNorm, self).__init__()
        self.register_parameter("scale", nn.Parameter(torch.ones(layer_shape)))
        self.eps = eps

    def forward(self, x):
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„RMSNormå®ç°
        ffn_layernorm = x.pow(2).mean(-1, keepdim=True)
        ffn_layernorm = ffn_layernorm + self.eps
        ffn_layernorm = ffn_layernorm.sqrt()
        ffn_layernorm = x / ffn_layernorm
        return self.scale * ffn_layernorm


class RotaryPositionalEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç  - æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡çª—å£"""
    def __init__(self, d_model, max_seq_len=8192):
        super().__init__()
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # é¢„è®¡ç®—æ—‹è½¬çŸ©é˜µ
        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))
        self.register_buffer('inv_freq', inv_freq)
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        self._build_cache(max_seq_len)
    
    def _build_cache(self, max_seq_len):
        seq = torch.arange(max_seq_len, dtype=torch.float)
        freqs = torch.outer(seq, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer('cos_cached', emb.cos(), persistent=False)
        self.register_buffer('sin_cached', emb.sin(), persistent=False)
    
    def forward(self, x, seq_len=None):
        if seq_len is None:
            seq_len = x.shape[-2]
        
        if seq_len > self.max_seq_len:
            self._build_cache(seq_len)
            
        return (
            self.cos_cached[:seq_len].to(x.device),
            self.sin_cached[:seq_len].to(x.device)
        )


def apply_rotary_pos_emb(q, k, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    def rotate_half(x):
        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
        return torch.cat((-x2, x1), dim=-1)
    
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


class RoPEMaskedAttentionHead(nn.Module):
    """å¸¦æ—‹è½¬ä½ç½®ç¼–ç çš„æ³¨æ„åŠ›å¤´ - 3Bæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬"""
    def __init__(self, config):
        super().__init__()
        
        self.d_model = config['d_model']
        self.n_heads = config['n_heads']
        self.head_dim = self.d_model // self.n_heads
        
        assert self.d_model % self.n_heads == 0, "d_model must be divisible by n_heads"
        
        # çº¿æ€§å˜æ¢å±‚ - æ¯ä¸ªå¤´åªå¤„ç†è‡ªå·±çš„ç»´åº¦
        self.w_q = nn.Linear(self.d_model, self.head_dim, bias=False)
        self.w_k = nn.Linear(self.d_model, self.head_dim, bias=False)
        self.w_v = nn.Linear(self.d_model, self.head_dim, bias=False)
        
        # æ—‹è½¬ä½ç½®ç¼–ç 
        self.rope = RotaryPositionalEmbedding(self.head_dim, config['context_window'])
        
        # ç¼©æ”¾å› å­
        self.scale = 1.0 / math.sqrt(self.head_dim)
        
        # æ³¨å†Œå› æœæ©ç 
        self.register_buffer(
            "mask",
            torch.tril(torch.ones(config['context_window'], config['context_window']))
            .view(1, 1, config['context_window'], config['context_window'])
        )

    def forward(self, x, use_kv_cache=False, past_kv=None):
        batch_size, seq_len, d_model = x.shape
        
        # çº¿æ€§å˜æ¢ - æ¯ä¸ªå¤´è¾“å‡ºhead_dimç»´åº¦
        q = self.w_q(x).view(batch_size, seq_len, self.head_dim).unsqueeze(1)  # (B, 1, S, head_dim)
        k = self.w_k(x).view(batch_size, seq_len, self.head_dim).unsqueeze(1)  # (B, 1, S, head_dim)
        v = self.w_v(x).view(batch_size, seq_len, self.head_dim).unsqueeze(1)  # (B, 1, S, head_dim)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        cos, sin = self.rope(x, seq_len)
        q, k = apply_rotary_pos_emb(q, k, cos, sin)
        
        # KVç¼“å­˜æ”¯æŒï¼ˆç”¨äºæ¨ç†åŠ é€Ÿï¼‰
        if use_kv_cache and past_kv is not None:
            past_k, past_v = past_kv
            k = torch.cat([past_k, k], dim=-2)
            v = torch.cat([past_v, v], dim=-2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        
        # åº”ç”¨å› æœæ©ç 
        mask_len = scores.shape[-1]
        scores = scores.masked_fill(
            self.mask[:, :, :seq_len, :mask_len] == 0, float('-inf')
        )
        
        # åº”ç”¨softmax
        attn_weights = F.softmax(scores, dim=-1)
        
        # è®¡ç®—è¾“å‡º
        out = torch.matmul(attn_weights, v)  # (B, 1, S, head_dim)
        out = out.squeeze(1)  # (B, S, head_dim)
        
        # è¿”å›è¾“å‡ºå’ŒKVç¼“å­˜
        if use_kv_cache:
            return out, (k, v)
        return out


class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ› - 3Bæ¨¡å‹ç‰ˆæœ¬"""
    def __init__(self, config):
        super().__init__()
        self.heads = nn.ModuleList([
            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])
        ])
        # ä¿®æ­£ï¼šæ¯ä¸ªå¤´çš„è¾“å‡ºç»´åº¦æ˜¯ d_model // n_headsï¼Œä¸æ˜¯ d_model
        head_dim = config['d_model'] // config['n_heads']
        self.linear = nn.Linear(config['n_heads'] * head_dim, config['d_model'])

    def forward(self, x, use_kv_cache=False, past_kvs=None):
        if use_kv_cache and past_kvs is not None:
            head_outputs = []
            new_kvs = []
            for i, head in enumerate(self.heads):
                out, kv = head(x, use_kv_cache=True, past_kv=past_kvs[i] if past_kvs else None)
                head_outputs.append(out)
                new_kvs.append(kv)
            return self.linear(torch.cat(head_outputs, dim=-1)), new_kvs
        else:
            head_outputs = [head(x) for head in self.heads]
            return self.linear(torch.cat(head_outputs, dim=-1))


class SwiGLU(nn.Module):
    """SwiGLUæ¿€æ´»å‡½æ•° - æ›´é€‚åˆå¤§æ¨¡å‹"""
    def __init__(self, d_model, ffn_dim):
        super().__init__()
        self.w_gate = nn.Linear(d_model, ffn_dim, bias=False)
        self.w_up = nn.Linear(d_model, ffn_dim, bias=False)
        self.w_down = nn.Linear(ffn_dim, d_model, bias=False)

    def forward(self, x):
        gate = F.silu(self.w_gate(x))  # SiLUæ¿€æ´»
        up = self.w_up(x)
        return self.w_down(gate * up)


class LlmpvqBlock(nn.Module):
    """Transformerå— - 3Bæ¨¡å‹ç‰ˆæœ¬"""
    def __init__(self, config):
        super().__init__()
        
        # æ³¨æ„åŠ›å±‚
        self.attention = MultiHeadAttention(config)
        self.rms_1 = RMSNorm(config['d_model'])
        
        # FFNå±‚
        ffn_dim = config.get('ffn_dim', config['d_model'] * 4)
        self.ffn = SwiGLU(config['d_model'], ffn_dim)
        self.rms_2 = RMSNorm(config['d_model'])

        # Dropout
        self.dropout = nn.Dropout(config.get('dropout', 0.1))

    def forward(self, x, use_kv_cache=False, past_kv=None):
        # æ³¨æ„åŠ›å±‚ï¼ˆPre-Normï¼‰
        normed_x = self.rms_1(x)
        if use_kv_cache:
            attn_out, new_kv = self.attention(normed_x, use_kv_cache=True, past_kvs=past_kv)
        else:
            attn_out = self.attention(normed_x)
            new_kv = None
        
        x = x + self.dropout(attn_out)
        
        # FFNå±‚ï¼ˆPre-Normï¼‰
        normed_x = self.rms_2(x)
        ffn_out = self.ffn(normed_x)
        x = x + self.dropout(ffn_out)
        
        if use_kv_cache:
            return x, new_kv
        return x


class Llmpvq3B(nn.Module):
    """3Bå‚æ•°çš„LLMæ¨¡å‹"""
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # åµŒå…¥å±‚
        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])

        # Transformerå±‚
        self.layers = nn.ModuleDict(
            OrderedDict([(f"Llmpvq3B_{i}", LlmpvqBlock(config)) for i in range(config['n_layers'])])
        )
        
        # æœ€ç»ˆå½’ä¸€åŒ–å±‚
        self.final_norm = RMSNorm(config['d_model'])

        # è¾“å‡ºå±‚
        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)

        # æƒé‡å…±äº«ï¼ˆå¯é€‰ï¼‰
        if config.get('tie_weights', True):
            self.lm_head.weight = self.embeddings.weight

        # åˆå§‹åŒ–æƒé‡
        self.apply(self._init_weights)
        # è®¡ç®—å‚æ•°é‡
        self.print_model_info()

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ– - é’ˆå¯¹å¤§æ¨¡å‹ä¼˜åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        print(f"\nğŸ—ï¸  3Bæ¨¡å‹æ¶æ„ä¿¡æ¯:")
        print(f"  ğŸ“Š æ€»å‚æ•°é‡: {total_params:,} ({total_params/1e9:.2f}B)")
        print(f"  ğŸ¯ å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/1e9:.2f}B)")
        print(f"  ğŸ”§ æ¨¡å‹é…ç½®:")
        print(f"    - d_model: {self.config['d_model']}")
        print(f"    - n_heads: {self.config['n_heads']}")
        print(f"    - n_layers: {self.config['n_layers']}")
        print(f"    - vocab_size: {self.config['vocab_size']}")
        print(f"    - context_window: {self.config['context_window']}")
        print(f"    - ffn_dim: {self.config.get('ffn_dim', self.config['d_model'] * 4)}")

    def forward(self, idx, use_kv_cache=False, past_kvs=None):
        batch_size, seq_len = idx.shape
        
        # åµŒå…¥
        x = self.embeddings(idx)  # (batch_size, seq_len, d_model)
        
        # Transformerå±‚
        new_kvs = [] if use_kv_cache else None
        for i, (name, layer) in enumerate(self.layers.items()):
            if use_kv_cache:
                x, new_kv = layer(x, use_kv_cache=True, 
                                past_kv=past_kvs[i] if past_kvs else None)
                new_kvs.append(new_kv)
            else:
                x = layer(x)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        x = self.final_norm(x)
        
        # è¾“å‡ºæŠ•å½±
        logits = self.lm_head(x)  # (batch_size, seq_len, vocab_size)
        
        if use_kv_cache:
            return logits, new_kvs
        return logits

    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None, use_kv_cache=True):
        """ç”Ÿæˆæ–‡æœ¬ - æ”¯æŒKVç¼“å­˜çš„é«˜æ•ˆæ¨ç†"""
        self.eval()
        past_kvs = None
        
        with torch.no_grad():
            for _ in range(max_new_tokens):
                # å¦‚æœä½¿ç”¨KVç¼“å­˜ï¼Œåªéœ€è¦å¤„ç†æœ€åä¸€ä¸ªtoken
                if use_kv_cache and past_kvs is not None:
                    idx_input = idx[:, -1:]
                else:
                    idx_input = idx[:, -self.config['context_window']:]
                
                # å‰å‘ä¼ æ’­
                if use_kv_cache:
                    logits, past_kvs = self(idx_input, use_kv_cache=True, past_kvs=past_kvs)
                else:
                    logits = self(idx_input)
                
                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                logits = logits[:, -1, :] / temperature
                
                # Top-ké‡‡æ ·
                if top_k is not None:
                    v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                    logits[logits < v[:, [-1]]] = -float('Inf')
                
                # Top-pé‡‡æ ·
                if top_p is not None:
                    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
                    cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    logits[indices_to_remove] = -float('Inf')
                
                # é‡‡æ ·
                probs = F.softmax(logits, dim=-1)
                idx_next = torch.multinomial(probs, num_samples=1)
                
                # æ·»åŠ åˆ°åºåˆ—
                idx = torch.cat((idx, idx_next), dim=1)
        
        return idx


def create_3b_model(config):
    """åˆ›å»º3Bæ¨¡å‹å®ä¾‹"""
    return Llmpvq3B(config)


if __name__ == "__main__":
    # æµ‹è¯•3Bæ¨¡å‹
    config = {
        'context_window': 2048,
        'vocab_size': 50000,
        'd_model': 2304,
        'n_heads': 32,
        'n_layers': 32,
        'ffn_dim': 9216,
        'dropout': 0.1,
        'tie_weights': True
    }
    
    print("ğŸš€ åˆ›å»º3Bæ¨¡å‹...")
    model = create_3b_model(config)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size, seq_len = 2, 512
    test_input = torch.randint(0, config['vocab_size'], (batch_size, seq_len))
    
    print(f"\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    print(f"è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    
    with torch.no_grad():
        output = model(test_input)
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"âœ… 3Bæ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
