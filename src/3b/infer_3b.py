#!/usr/bin/env python3
"""
3Bå‚æ•°LLMæ¨¡å‹æ¨ç†è„šæœ¬
æ”¯æŒé«˜æ•ˆæ¨ç†ã€KVç¼“å­˜ã€æ‰¹é‡ç”Ÿæˆç­‰åŠŸèƒ½
"""

import torch
import torch.nn.functional as F
import os
import sys
import time
import yaml
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple
import numpy as np
from llmcore_3b import create_3b_model, get_device
from data_processor import DataProcessor


def sample_next_token(logits: torch.Tensor, temperature: float = 1.0, 
                     top_k: Optional[int] = None, top_p: Optional[float] = None) -> torch.Tensor:
    """é‡‡æ ·ä¸‹ä¸€ä¸ªtoken - 3Bæ¨¡å‹ä¼˜åŒ–ç‰ˆæœ¬"""
    if temperature <= 0:
        return torch.argmax(logits, dim=-1, keepdim=True)
    
    logits = logits / temperature
    
    # Top-ké‡‡æ ·
    if top_k is not None and top_k > 0:
        top_k = min(top_k, logits.size(-1))
        values, indices = torch.topk(logits, top_k)
        logits = torch.full_like(logits, float('-inf'))
        logits.scatter_(-1, indices, values)
    
    # Top-pé‡‡æ ·
    if top_p is not None and 0 < top_p < 1:
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # åˆ›å»ºæ©ç 
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
    
    # é‡‡æ ·
    probs = F.softmax(logits, dim=-1)
    return torch.multinomial(probs, num_samples=1)


class LLMInference3B:
    """3Bæ¨¡å‹æ¨ç†å™¨"""
    
    def __init__(self, model_dir: str = "model_save_3b", config_path: str = "config/config.yaml"):
        self.model_dir = Path(model_dir)
        self.config_path = config_path
        self.model = None
        self.config = None
        self.data_processor = None
        self.device = None
        self.itos = None
        self.stoi = None
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # è®¾ç½®è®¾å¤‡
        self._setup_device()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
        
        # å‡†å¤‡åˆ†è¯å™¨
        self._setup_tokenizer()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_device(self):
        """è®¾ç½®æ¨ç†è®¾å¤‡"""
        device_config = self.config.get('device', {})
        self.device, is_multi_gpu, available_gpus = get_device(
            force_cpu=device_config.get('force_cpu', False),
            device_config=device_config
        )
        print(f"ğŸ¯ æ¨ç†è®¾å¤‡: {self.device}")
    
    def _load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        print("ğŸ”„ åŠ è½½3Bæ¨¡å‹...")
        
        # æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶
        best_model_path = self.model_dir / "best_model.pt"
        if not best_model_path.exists():
            # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
            checkpoints = list(self.model_dir.glob("checkpoint_*.pt"))
            if not checkpoints:
                raise FileNotFoundError(f"åœ¨ {self.model_dir} ä¸­æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œé€‰æ‹©æœ€æ–°çš„
            best_model_path = max(checkpoints, key=lambda x: x.stat().st_mtime)
            print(f"ğŸ“ ä½¿ç”¨æœ€æ–°æ£€æŸ¥ç‚¹: {best_model_path}")
        else:
            print(f"ğŸ’ ä½¿ç”¨æœ€ä½³æ¨¡å‹: {best_model_path}")
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        try:
            checkpoint = torch.load(best_model_path, map_location=self.device)
            
            # è·å–æ¨¡å‹é…ç½®
            if 'config' in checkpoint:
                model_config = checkpoint['config']['model']
            else:
                model_config = self.config['model']
            
            # åˆ›å»ºæ¨¡å‹
            self.model = create_3b_model(model_config)
            
            # åŠ è½½æƒé‡
            if 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
                
                # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                if any(key.startswith('module.') for key in state_dict.keys()):
                    state_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}
                
                self.model.load_state_dict(state_dict)
            else:
                self.model.load_state_dict(checkpoint)
            
            # ç§»åŠ¨åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model = self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            if 'epoch' in checkpoint:
                print(f"ğŸ“Š è®­ç»ƒè½®æ¬¡: {checkpoint['epoch']}")
            if 'loss' in checkpoint:
                print(f"ğŸ“‰ è®­ç»ƒæŸå¤±: {checkpoint['loss']:.4f}")
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _setup_tokenizer(self):
        """è®¾ç½®åˆ†è¯å™¨"""
        print("ğŸ”¤ å‡†å¤‡åˆ†è¯å™¨...")
        
        data_config = self.config['data']
        tokenizer_type = "chinese_bpe" if data_config.get('use_bpe_tokenizer', True) else "char_level"
        vocab_path = data_config.get('vocab_cache_path', 'chinese_tokenizer_vocab_50k.json')
        
        from data_collectors import DataProcessor
        
        self.data_processor = DataProcessor(
            tokenizer_type=tokenizer_type,
            vocab_path=vocab_path,
            vocab_size=self.config['model']['vocab_size']
        )
        
        # å‡†å¤‡è¯æ±‡è¡¨ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
        try:
            # å°è¯•ä»ç°æœ‰çš„è¯æ±‡è¡¨æ–‡ä»¶åŠ è½½
            if os.path.exists(vocab_path):
                print(f"âœ… åˆ†è¯å™¨å‡†å¤‡å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {self.data_processor.vocab_size or 'Unknown'}")
            else:
                print("âš ï¸ è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹")
            
            self.itos = self.data_processor.itos
            self.stoi = self.data_processor.stoi
        except Exception as e:
            print(f"âŒ åˆ†è¯å™¨å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def encode_text(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºtokenåºåˆ—"""
        if self.data_processor:
            return self.data_processor.encode_text(text)
        else:
            return [self.stoi.get(ch, 0) for ch in text if ch in self.stoi]
    
    def decode_tokens(self, tokens: List[int]) -> str:
        """è§£ç tokenåºåˆ—ä¸ºæ–‡æœ¬"""
        if self.data_processor:
            return self.data_processor.decode_text(tokens)
        else:
            return ''.join([self.itos.get(idx, '') for idx in tokens])
    
    def generate(self, prompt: str, max_new_tokens: int = 512, 
                temperature: float = 0.7, top_k: int = 50, top_p: float = 0.9,
                use_kv_cache: bool = True, show_progress: bool = False) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        # ç¼–ç è¾“å…¥
        input_tokens = self.encode_text(prompt)
        if not input_tokens:
            return "æŠ±æ­‰ï¼Œæ— æ³•ç†è§£è¾“å…¥ã€‚"
        
        # è½¬æ¢ä¸ºå¼ é‡
        input_ids = torch.tensor([input_tokens], dtype=torch.long, device=self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            if use_kv_cache and hasattr(self.model, 'generate'):
                # ä½¿ç”¨æ¨¡å‹å†…ç½®çš„é«˜æ•ˆç”Ÿæˆæ–¹æ³•
                output_ids = self.model.generate(
                    input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    use_kv_cache=True
                )
            else:
                # ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•
                output_ids = self._generate_traditional(
                    input_ids, max_new_tokens, temperature, top_k, top_p, show_progress
                )
        
        # è§£ç è¾“å‡ºï¼ˆåªè¿”å›æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        generated_tokens = output_ids[0, len(input_tokens):].tolist()
        return self.decode_tokens(generated_tokens)
    
    def _generate_traditional(self, input_ids: torch.Tensor, max_new_tokens: int,
                            temperature: float, top_k: int, top_p: float, 
                            show_progress: bool = False) -> torch.Tensor:
        """ä¼ ç»Ÿç”Ÿæˆæ–¹æ³•ï¼ˆä¸ä½¿ç”¨KVç¼“å­˜ï¼‰"""
        generated_ids = input_ids.clone()
        context_window = self.config['model']['context_window']
        
        for step in range(max_new_tokens):
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            current_context = generated_ids[:, -context_window:]
            
            # å‰å‘ä¼ æ’­
            logits = self.model(current_context)
            next_token_logits = logits[:, -1, :]
            
            # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
            next_token = sample_next_token(
                next_token_logits,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p
            )
            
            # æ·»åŠ åˆ°åºåˆ—
            generated_ids = torch.cat([generated_ids, next_token], dim=-1)
            
            # æ˜¾ç¤ºè¿›åº¦
            if show_progress and step % 10 == 0:
                print(f"ç”Ÿæˆè¿›åº¦: {step}/{max_new_tokens}")
        
        return generated_ids
    
    def batch_generate(self, prompts: List[str], **kwargs) -> List[str]:
        """æ‰¹é‡ç”Ÿæˆ"""
        results = []
        for prompt in prompts:
            result = self.generate(prompt, **kwargs)
            results.append(result)
        return results
    
    def interactive_chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("\n" + "="*60)
        print("ğŸ¤– é€šç”¨å¯¹è¯å°åŠ©æ‰‹å·²å¯åŠ¨ï¼")
        print("ğŸ’¡ æ‚¨å¯ä»¥å’Œæˆ‘èŠå¤©ï¼Œæˆ‘ä¼šç”¨è¥¿æ¸¸è®°çš„é£æ ¼å›å¤æ‚¨")
        print("ğŸ“ è¾“å…¥ 'quit'ã€'exit' æˆ– 'bye' é€€å‡º")
        print("ğŸ›ï¸ è¾“å…¥ 'settings' æŸ¥çœ‹ç”Ÿæˆå‚æ•°")
        print("="*60)
        
        # é»˜è®¤ç”Ÿæˆå‚æ•°
        settings = {
            'max_new_tokens': 512,
            'temperature': 0.7,
            'top_k': 50,
            'top_p': 0.9,
            'use_kv_cache': True
        }
        
        conversation_history = []
        
        while True:
            try:
                user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
                
                if not user_input:
                    continue
                
                # å¤„ç†ç‰¹æ®Šå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                    break
                
                if user_input.lower() == 'settings':
                    print("\nâš™ï¸ å½“å‰ç”Ÿæˆå‚æ•°:")
                    for key, value in settings.items():
                        print(f"  {key}: {value}")
                    continue
                
                # æ„å»ºä¸Šä¸‹æ–‡
                context_parts = []
                for item in conversation_history[-3:]:  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
                    context_parts.append(f"ç”¨æˆ·ï¼š{item['user']}")
                    context_parts.append(f"åŠ©æ‰‹ï¼š{item['bot']}")
                
                context_parts.append(f"ç”¨æˆ·ï¼š{user_input}")
                context_parts.append("åŠ©æ‰‹ï¼š")
                context_prompt = ''.join(context_parts)
                
                # ç”Ÿæˆå›å¤
                print("ğŸ¤” æ€è€ƒä¸­...", end="", flush=True)
                start_time = time.time()
                
                response = self.generate(
                    context_prompt,
                    max_new_tokens=settings['max_new_tokens'],
                    temperature=settings['temperature'],
                    top_k=settings['top_k'],
                    top_p=settings['top_p'],
                    use_kv_cache=settings['use_kv_cache']
                )
                
                generation_time = time.time() - start_time
                
                # æ¸…é™¤"æ€è€ƒä¸­..."å¹¶æ˜¾ç¤ºå›å¤
                print(f"\r{'':20}\r", end="")
                print(f"ğŸ¤– åŠ©æ‰‹: {response}")
                print(f"â±ï¸  ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
                
                # æ·»åŠ åˆ°å†å²
                conversation_history.append({
                    'user': user_input,
                    'bot': response
                })
                
                # ä¿æŒå†å²è®°å½•ä¸è¶…è¿‡10è½®
                if len(conversation_history) > 10:
                    conversation_history.pop(0)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ å†è§ï¼æ„Ÿè°¢ä½¿ç”¨é€šç”¨å¯¹è¯å°åŠ©æ‰‹ï¼")
                break
            except Exception as e:
                print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
                print("ğŸ”„ è¯·é‡è¯•...")


def warmup_model(model, device, warmup_steps=3):
    """é¢„çƒ­æ¨¡å‹ä»¥è·å¾—ç¨³å®šçš„æ¨ç†æ€§èƒ½"""
    print(f"ğŸ”¥ é¢„çƒ­æ¨¡å‹ ({warmup_steps} æ­¥)...")
    
    model.eval()
    dummy_input = torch.randint(0, 1000, (1, 64), device=device)
    
    with torch.no_grad():
        for i in range(warmup_steps):
            _ = model(dummy_input)
            print(f"  é¢„çƒ­æ­¥éª¤ {i+1}/{warmup_steps}")
    
    print("âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ")


def load_model_and_config(model_dir="model_save_3b", config_path="config/config.yaml", 
                         enable_warmup=True):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®çš„ä¾¿æ·å‡½æ•°"""
    inference = LLMInference3B(model_dir, config_path)
    
    if enable_warmup:
        warmup_model(inference.model, inference.device)
    
    return inference.model, inference.config


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨3Bæ¨¡å‹æ¨ç†...")
    
    try:
        inference = LLMInference3B()
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompt = "è¯è¯´å¤©ä¸‹å¤§åŠ¿ï¼Œåˆ†ä¹…å¿…åˆï¼Œåˆä¹…å¿…åˆ†"
        print(f"\nğŸ§ª æµ‹è¯•ç”Ÿæˆ:")
        print(f"è¾“å…¥: {test_prompt}")
        
        result = inference.generate(
            test_prompt,
            max_new_tokens=100,
            temperature=0.8,
            show_progress=True
        )
        print(f"è¾“å‡º: {result}")
        
        # å¯åŠ¨äº¤äº’å¼èŠå¤©
        inference.interactive_chat()
        
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
