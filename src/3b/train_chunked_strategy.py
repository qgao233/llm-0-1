#!/usr/bin/env python3
"""
3Bå‚æ•°LLMæ¨¡å‹åˆ†å—è®­ç»ƒç­–ç•¥è„šæœ¬
å®ç°æ–°çš„æ•°æ®è®­ç»ƒç­–ç•¥ï¼š
- æ¯10ä¸ªæ ·æœ¬ä¸€å—ï¼Œæ¯å—è®­ç»ƒ5è½®
- 20ä¸‡æ ·æœ¬éœ€è¦100ä¸‡è½®è®­ç»ƒ
- æ¯1000è½®ä¿å­˜checkpointï¼Œåªä¿ç•™æœ€æ–°çš„
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torch.cuda.amp import autocast, GradScaler
import os
import sys
import time
import json
import yaml
import math
import gc
import psutil
from pathlib import Path
import numpy as np
from tqdm import tqdm
import logging
from datetime import datetime
from llmcore_3b import create_3b_model, get_device
from data_collectors import create_data_collectors, MixedDataset, DataProcessor

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
os.makedirs('logs', exist_ok=True)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/train_chunked_strategy.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class ChunkedDataset(torch.utils.data.Dataset):
    """åˆ†å—æ•°æ®é›† - æ”¯æŒæŒ‰å—è®­ç»ƒ"""
    
    def __init__(self, original_dataset, chunk_size=10):
        """
        åˆå§‹åŒ–åˆ†å—æ•°æ®é›†
        
        Args:
            original_dataset: åŸå§‹æ•°æ®é›†
            chunk_size: æ¯å—çš„æ ·æœ¬æ•°é‡
        """
        self.original_dataset = original_dataset
        self.chunk_size = chunk_size
        self.total_samples = len(original_dataset)
        self.total_chunks = math.ceil(self.total_samples / chunk_size)
        
        logger.info(f"ğŸ“¦ åˆ†å—æ•°æ®é›†åˆå§‹åŒ–:")
        logger.info(f"  ğŸ“Š æ€»æ ·æœ¬æ•°: {self.total_samples:,}")
        logger.info(f"  ğŸ“¦ å—å¤§å°: {chunk_size}")
        logger.info(f"  ğŸ”¢ æ€»å—æ•°: {self.total_chunks:,}")
    
    def get_chunk(self, chunk_id):
        """è·å–æŒ‡å®šå—çš„æ•°æ®"""
        start_idx = chunk_id * self.chunk_size
        end_idx = min(start_idx + self.chunk_size, self.total_samples)
        
        # åˆ›å»ºå­é›†
        indices = list(range(start_idx, end_idx))
        chunk_dataset = Subset(self.original_dataset, indices)
        
        return chunk_dataset
    
    def __len__(self):
        return self.total_samples
    
    def __getitem__(self, idx):
        return self.original_dataset[idx]


class ChunkedTrainingStrategy:
    """åˆ†å—è®­ç»ƒç­–ç•¥ç®¡ç†å™¨"""
    
    def __init__(self, config_path="config/config.yaml"):
        self.config_path = config_path
        self.config = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.scaler = None
        self.device = None
        self.is_multi_gpu = False
        self.is_save_interval = False
        
        # è®­ç»ƒç­–ç•¥å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
        self.chunk_size = 10  # æ¯å—æ ·æœ¬æ•°
        self.epochs_per_chunk = 5  # æ¯å—è®­ç»ƒè½®æ•°
        self.save_interval = 1000  # æ¯1000è½®ä¿å­˜checkpoint
        self.current_global_step = 0  # å…¨å±€æ­¥æ•°
        
        # åŠ è½½é…ç½®
        self._load_config()
        
        # ä»é…ç½®æ–‡ä»¶æ›´æ–°è®­ç»ƒç­–ç•¥å‚æ•°
        self._update_strategy_params()
        
        # è®¾ç½®è®¾å¤‡
        self._setup_device()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self._create_directories()
        
        # åˆå§‹å†…å­˜æ¸…ç†
        self._cleanup_memory()
        
        logger.info("âœ… åˆ†å—è®­ç»ƒç­–ç•¥åˆå§‹åŒ–å®Œæˆ")
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _update_strategy_params(self):
        """ä»é…ç½®æ–‡ä»¶æ›´æ–°è®­ç»ƒç­–ç•¥å‚æ•°"""
        if not self.config:
            return
        
        chunked_config = self.config.get('chunked_training', {})
        
        # æ›´æ–°å‚æ•°
        self.chunk_size = chunked_config.get('chunk_size', self.chunk_size)
        self.epochs_per_chunk = chunked_config.get('epochs_per_chunk', self.epochs_per_chunk)
        self.save_interval = chunked_config.get('save_interval', self.save_interval)
        
        logger.info(f"ğŸ”§ åˆ†å—è®­ç»ƒç­–ç•¥å‚æ•°:")
        logger.info(f"  ğŸ“¦ å—å¤§å°: {self.chunk_size}")
        logger.info(f"  ğŸ”„ æ¯å—è½®æ•°: {self.epochs_per_chunk}")
        logger.info(f"  ğŸ’¾ ä¿å­˜é—´éš”: {self.save_interval}")
    
    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        device_config = self.config.get('device', {})
        self.device, self.is_multi_gpu, available_gpus = get_device(
            force_cpu=device_config.get('force_cpu', False),
            device_config=device_config
        )
        
        # è®¾ç½®æ··åˆç²¾åº¦
        if device_config.get('mixed_precision', True) and self.device.type == 'cuda':
            self.scaler = GradScaler()
            logger.info("ğŸ”¥ å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
        
        logger.info(f"ğŸ¯ è®­ç»ƒè®¾å¤‡: {self.device}")
    
    def _create_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        dirs = [
            self.config['checkpoint']['save_dir'],
            self.config['monitoring']['log_dir'],
            'logs'
        ]
        
        for dir_path in dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
    
    def _cleanup_memory(self):
        """æ¸…ç†å†…å­˜"""
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
    
    def _prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("ğŸ“š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        data_config = self.config['data']
        
        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        tokenizer_type = "chinese_bpe" if data_config.get('use_bpe_tokenizer', True) else "char_level"
        vocab_path = data_config.get('vocab_cache_path', 'chinese_tokenizer_vocab_50k.json')
        
        self.data_processor = DataProcessor(
            tokenizer_type=tokenizer_type,
            vocab_path=vocab_path,
            vocab_size=self.config['model']['vocab_size'],
            config_path=self.config_path
        )
        
        # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
        collectors, self.data_processor = create_data_collectors(self.config)
        
        if not collectors:
            raise ValueError("æœªèƒ½åˆ›å»ºä»»ä½•æ•°æ®æ”¶é›†å™¨")
        
        # æ›´æ–°è¯æ±‡è¡¨å¤§å°
        actual_vocab_size = self.data_processor.vocab_size
        if actual_vocab_size and actual_vocab_size != self.config['model']['vocab_size']:
            logger.info(f"ğŸ”„ åŠ¨æ€è°ƒæ•´è¯æ±‡è¡¨å¤§å°: {self.config['model']['vocab_size']} -> {actual_vocab_size}")
            self.config['model']['vocab_size'] = actual_vocab_size
        
        # åˆ›å»ºæ··åˆæ•°æ®é›†
        seq_length = self.config['model']['context_window']
        original_dataset = MixedDataset(
            collectors=collectors,
            seq_length=seq_length,
            config=data_config
        )
        
        # åˆ›å»ºåˆ†å—æ•°æ®é›†
        self.chunked_dataset = ChunkedDataset(original_dataset, self.chunk_size)
        
        logger.info(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ")
        logger.info(f"  ğŸ“Š åŸå§‹æ•°æ®é›†å¤§å°: {len(original_dataset):,} æ ·æœ¬")
        logger.info(f"  ğŸ“¦ åˆ†å—æ•°æ®é›†: {self.chunked_dataset.total_chunks:,} å—")
        logger.info(f"  ğŸ“ åºåˆ—é•¿åº¦: {seq_length}")
        
        return self.data_processor.itos if hasattr(self.data_processor, 'itos') else {}, \
               self.data_processor.stoi if hasattr(self.data_processor, 'stoi') else {}
    
    def _create_model(self):
        """åˆ›å»º3Bæ¨¡å‹"""
        logger.info("ğŸ—ï¸ åˆ›å»º3Bæ¨¡å‹...")
        
        model_config = self.config['model']
        
        with torch.device(self.device):
            self.model = create_3b_model(model_config)
        
        # ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
        if model_config.get('use_gradient_checkpointing', True):
            if hasattr(self.model, 'gradient_checkpointing_enable'):
                self.model.gradient_checkpointing_enable()
            logger.info("ğŸ’¾ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
        # å¤šGPUæ”¯æŒ
        if self.is_multi_gpu:
            self.model = nn.DataParallel(self.model)
            logger.info("ğŸš€ å¯ç”¨DataParallelå¤šGPUè®­ç»ƒ")
        
        return self.model
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        optimizer_config = self.config['optimizer']
        
        lr = float(optimizer_config['lr'])
        weight_decay = float(optimizer_config['weight_decay'])
        eps = float(optimizer_config['eps'])
        betas = [float(b) for b in optimizer_config['betas']]
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=betas,
            eps=eps
        )
        
        logger.info(f"ğŸ¯ ä¼˜åŒ–å™¨: AdamW, å­¦ä¹ ç‡: {lr}")
    
    def _save_checkpoint(self, global_step, loss, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹ - åªä¿ç•™æœ€æ–°çš„"""
        checkpoint_dir = Path(self.config['checkpoint']['save_dir'])
        
        # å‡†å¤‡æ£€æŸ¥ç‚¹æ•°æ®
        checkpoint = {
            'global_step': global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # åˆ é™¤æ—§çš„checkpoint
        old_checkpoints = list(checkpoint_dir.glob("checkpoint_step_*.pt"))
        for old_checkpoint in old_checkpoints:
            try:
                old_checkpoint.unlink()
                logger.debug(f"ğŸ—‘ï¸ åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {old_checkpoint}")
            except Exception as e:
                logger.warning(f"âš ï¸ åˆ é™¤æ—§æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        
        # ä¿å­˜æ–°çš„checkpoint
        checkpoint_path = checkpoint_dir / f"checkpoint_step_{global_step}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = checkpoint_dir / "best_model.pt"
            torch.save(checkpoint, best_path)
            logger.info(f"ğŸ’ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
        
        logger.info(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            logger.info(f"âš ï¸ æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
            return 0
        
        try:
            # ä¸ºäº†å…¼å®¹PyTorch 2.6çš„å®‰å…¨æœºåˆ¶ï¼Œæ·»åŠ å®‰å…¨çš„å…¨å±€å˜é‡
            import torch.serialization
            safe_globals = [
                'numpy.core.multiarray.scalar',
                'numpy.dtype',
                'numpy.ndarray',
                'collections.OrderedDict',
                'torch._utils._rebuild_tensor_v2',
                'torch._utils._rebuild_parameter',
                'torch.nn.parameter.Parameter',
                'builtins.dict',
                'builtins.list',
                'builtins.tuple',
                'builtins.int',
                'builtins.float',
                'builtins.str',
                'builtins.bool'
            ]
            
            # ä½¿ç”¨å®‰å…¨çš„å…¨å±€å˜é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            try:
                with torch.serialization.safe_globals(safe_globals):
                    checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=True)
            except Exception as safe_load_error:
                logger.warning(f"âš ï¸ å®‰å…¨åŠ è½½å¤±è´¥ï¼Œå°è¯•ä¼ ç»ŸåŠ è½½æ–¹å¼: {safe_load_error}")
                # å›é€€åˆ°ä¼ ç»ŸåŠ è½½æ–¹å¼ï¼ˆä¸æ¨èï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ï¼‰
                checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
                logger.warning("âš ï¸ ä½¿ç”¨äº†ä¸å®‰å…¨çš„åŠ è½½æ–¹å¼ï¼Œè¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶æ¥æºå¯ä¿¡")
            
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            if self.scaler and 'scaler_state_dict' in checkpoint:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            global_step = checkpoint.get('global_step', 0)
            logger.info(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ: {checkpoint_path}")
            logger.info(f"ğŸ“Š æ¢å¤åˆ°å…¨å±€æ­¥æ•°: {global_step}")
            
            return global_step
            
        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥ç‚¹åŠ è½½å¤±è´¥: {e}")
            return 0
    
    def calculate_training_plan(self, total_samples):
        """
        è®¡ç®—è®­ç»ƒè®¡åˆ’ - è‡ªåŠ¨è®¡ç®—ç›®æ ‡è®­ç»ƒæ­¥æ•°
        
        Args:
            total_samples: æ€»æ ·æœ¬æ•°
            
        Returns:
            è®­ç»ƒè®¡åˆ’å­—å…¸
        """
        # è·å–åˆ†å—è®­ç»ƒé…ç½®
        chunked_config = self.config.get('chunked_training', {})
        auto_calculate = chunked_config.get('auto_calculate_steps', True)
        target_multiplier = chunked_config.get('target_multiplier', 1000)
        
        total_chunks = math.ceil(total_samples / self.chunk_size)
        base_training_rounds = total_chunks * self.epochs_per_chunk
        
        if auto_calculate:
            # è‡ªåŠ¨è®¡ç®—ï¼šæ ¹æ®æ ·æœ¬æ•°é‡å’Œå€æ•°è®¡ç®—ç›®æ ‡æ­¥æ•°
            # ä¾‹å¦‚ï¼š20ä¸‡æ ·æœ¬ Ã— 1000å€æ•° = 2äº¿æ­¥
            # ä½†è¿™é‡Œæˆ‘ä»¬æŒ‰ç…§æ‚¨çš„é€»è¾‘ï¼š20ä¸‡æ ·æœ¬éœ€è¦100ä¸‡è½®
            # æ‰€ä»¥å€æ•°åº”è¯¥æ˜¯ï¼š100ä¸‡ Ã· 20ä¸‡ = 5
            # æ›´é€šç”¨çš„å…¬å¼ï¼štarget_steps = total_samples * (target_multiplier / 200)
            # è¿™æ ·å½“æœ‰20ä¸‡æ ·æœ¬æ—¶ï¼Œtarget_multiplier=1000 ä¼šå¾—åˆ° 20ä¸‡ * (1000/200) = 100ä¸‡æ­¥
            
            if total_samples >= 200000:  # 20ä¸‡æ ·æœ¬
                # æŒ‰ç…§æ‚¨çš„éœ€æ±‚ï¼š20ä¸‡æ ·æœ¬éœ€è¦100ä¸‡è½®
                target_total_steps = int(total_samples * (target_multiplier / 200))
            else:
                # å¯¹äºè¾ƒå°‘çš„æ ·æœ¬ï¼Œä½¿ç”¨æ›´åˆç†çš„å€æ•°
                # ç¡®ä¿è‡³å°‘æ¯ä¸ªæ ·æœ¬è¢«è®­ç»ƒè¶³å¤Ÿå¤šæ¬¡
                min_steps_per_sample = 5  # æ¯ä¸ªæ ·æœ¬è‡³å°‘è®­ç»ƒ5æ¬¡
                calculated_steps = total_samples * min_steps_per_sample
                target_total_steps = max(calculated_steps, base_training_rounds * 2)
            
            logger.info(f"ğŸ§® è‡ªåŠ¨è®¡ç®—ç›®æ ‡è®­ç»ƒæ­¥æ•°:")
            logger.info(f"  ğŸ“Š æ ·æœ¬æ•°é‡: {total_samples:,}")
            logger.info(f"  ğŸ”¢ ç›®æ ‡å€æ•°: {target_multiplier}")
            logger.info(f"  ğŸ¯ è®¡ç®—å¾—å‡º: {target_total_steps:,} æ­¥")
        else:
            # æ‰‹åŠ¨æŒ‡å®šï¼ˆå‘åå…¼å®¹ï¼‰
            target_total_steps = chunked_config.get('target_total_steps', 1000000)
            logger.info(f"ğŸ“‹ ä½¿ç”¨æ‰‹åŠ¨æŒ‡å®šçš„ç›®æ ‡æ­¥æ•°: {target_total_steps:,}")
        
        # è®¡ç®—éœ€è¦é‡å¤å¤šå°‘æ¬¡æ‰èƒ½è¾¾åˆ°ç›®æ ‡è½®æ•°
        repeat_cycles = math.ceil(target_total_steps / base_training_rounds)
        actual_total_steps = repeat_cycles * base_training_rounds
        
        plan = {
            'total_samples': total_samples,
            'chunk_size': self.chunk_size,
            'total_chunks': total_chunks,
            'epochs_per_chunk': self.epochs_per_chunk,
            'base_training_rounds': base_training_rounds,
            'target_total_steps': target_total_steps,
            'repeat_cycles': repeat_cycles,
            'actual_total_steps': actual_total_steps,
            'checkpoints_count': actual_total_steps // self.save_interval,
            'auto_calculated': auto_calculate
        }
        
        return plan
    
    def train_chunk(self, chunk_dataset, chunk_id, cycle_id):
        """è®­ç»ƒå•ä¸ªæ•°æ®å—"""
        self.model.train()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = self.config['training']['batch_size']
        chunk_loader = DataLoader(
            chunk_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=False,
            drop_last=True
        )
        
        chunk_losses = []
        
        # è®­ç»ƒæŒ‡å®šè½®æ•°
        for epoch in range(self.epochs_per_chunk):
            epoch_loss = 0
            num_batches = len(chunk_loader)
            chunk_dataset = self.chunked_dataset.get_chunk(chunk_id)
            pbar = tqdm(chunk_loader, 
                       desc=f"å‘¨æœŸ{cycle_id+1} å—{chunk_id+1}(æ ·æœ¬æ•°: {len(chunk_dataset)})/{self.chunked_dataset.total_chunks} è½®{epoch+1}/{self.epochs_per_chunk}")
            
            for batch_idx, (x, y) in enumerate(pbar):
                try:
                    # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                    x, y = x.to(self.device, non_blocking=True), y.to(self.device, non_blocking=True)
                    
                    # å‰å‘ä¼ æ’­
                    if self.scaler:
                        with autocast():
                            logits = self.model(x)
                            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
                    else:
                        logits = self.model(x)
                        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))
                    
                    # ç«‹å³åˆ é™¤logitsä»¥é‡Šæ”¾å†…å­˜
                    del logits
                    
                    # åå‘ä¼ æ’­
                    if self.scaler:
                        self.scaler.scale(loss).backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        self.scaler.unscale_(self.optimizer)
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                        
                        self.scaler.step(self.optimizer)
                        self.scaler.update()
                    else:
                        loss.backward()
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                        self.optimizer.step()
                    
                    self.optimizer.zero_grad()
                    
                    # ç»Ÿè®¡
                    loss_value = loss.item()
                    epoch_loss += loss_value
                    
                    # æ›´æ–°å…¨å±€æ­¥æ•°
                    self.current_global_step += 1
                    
                    # åˆ é™¤losså¼ é‡
                    del loss, x, y
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.set_postfix({
                        'loss': f'{loss_value:.4f}',
                        'global_step': self.current_global_step,
                        'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                    })
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if self.current_global_step % self.save_interval == 0:
                        self.is_save_interval = True
                        avg_loss = epoch_loss / (batch_idx + 1)
                        self._cleanup_memory()
                        self._save_checkpoint(self.current_global_step, avg_loss)
                        logger.info(f"ğŸ“Š å…¨å±€æ­¥æ•° {self.current_global_step:,}, æŸå¤±: {avg_loss:.4f}")
                    
                    # å®šæœŸå†…å­˜æ¸…ç†
                    if batch_idx % 10 == 0:
                        self._cleanup_memory()
                
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        logger.error(f"âŒ GPUå†…å­˜ä¸è¶³: {e}")
                        # ç´§æ€¥æ¸…ç†
                        if 'x' in locals():
                            del x
                        if 'y' in locals():
                            del y
                        if 'logits' in locals():
                            del logits
                        if 'loss' in locals():
                            del loss
                        self._cleanup_memory()
                        raise
                    else:
                        raise
            
            # è®¡ç®—epochå¹³å‡æŸå¤±
            avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0
            chunk_losses.append(avg_epoch_loss)
            
        return np.mean(chunk_losses) if chunk_losses else 0.0
    
    def train(self):
        """å¼€å§‹åˆ†å—è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹åˆ†å—è®­ç»ƒç­–ç•¥...")
        
        # å‡†å¤‡æ•°æ®
        itos, stoi = self._prepare_data()
        
        # è®¡ç®—è®­ç»ƒè®¡åˆ’
        plan = self.calculate_training_plan(len(self.chunked_dataset))
        
        logger.info("ğŸ“‹ è®­ç»ƒè®¡åˆ’:")
        logger.info(f"  ğŸ“Š æ€»æ ·æœ¬æ•°: {plan['total_samples']:,}")
        logger.info(f"  ğŸ“¦ æ€»å—æ•°: {plan['total_chunks']:,}")
        logger.info(f"  ğŸ”„ æ¯å—è®­ç»ƒè½®æ•°: {plan['epochs_per_chunk']}")
        logger.info(f"  ğŸ“ åŸºç¡€è®­ç»ƒè½®æ•°: {plan['base_training_rounds']:,}")
        if plan['auto_calculated']:
            logger.info(f"  ğŸ¯ ç›®æ ‡è®­ç»ƒæ­¥æ•°: {plan['target_total_steps']:,} (è‡ªåŠ¨è®¡ç®—)")
        else:
            logger.info(f"  ğŸ¯ ç›®æ ‡è®­ç»ƒæ­¥æ•°: {plan['target_total_steps']:,} (æ‰‹åŠ¨æŒ‡å®š)")
        logger.info(f"  ğŸ” é‡å¤å‘¨æœŸæ•°: {plan['repeat_cycles']}")
        logger.info(f"  ğŸ“ˆ å®é™…æ€»è®­ç»ƒæ­¥æ•°: {plan['actual_total_steps']:,}")
        logger.info(f"  ğŸ’¾ é¢„è®¡æ£€æŸ¥ç‚¹æ•°: {plan['checkpoints_count']:,}")
        
        # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
        self._create_model()
        self._create_optimizer()
        
        # å°è¯•åŠ è½½æœ€æ–°çš„æ£€æŸ¥ç‚¹
        checkpoint_dir = Path(self.config['checkpoint']['save_dir'])
        latest_checkpoint = None
        for checkpoint_file in checkpoint_dir.glob("checkpoint_step_*.pt"):
            if latest_checkpoint is None or checkpoint_file.stat().st_mtime > latest_checkpoint.stat().st_mtime:
                latest_checkpoint = checkpoint_file
        
        if latest_checkpoint:
            self.current_global_step = self._load_checkpoint(latest_checkpoint)
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        start_time = time.time()
        
        logger.info(f"ğŸ¯ å¼€å§‹è®­ç»ƒï¼Œå½“å‰å…¨å±€æ­¥æ•°: {self.current_global_step}")
        
        for cycle in range(plan['repeat_cycles']):
            logger.info(f"\nğŸ”„ å¼€å§‹è®­ç»ƒå‘¨æœŸ {cycle + 1}/{plan['repeat_cycles']}")
            
            cycle_losses = []
            
            # éå†æ‰€æœ‰æ•°æ®å—
            for chunk_id in range(plan['total_chunks']):
                # è·å–å½“å‰å—çš„æ•°æ®
                chunk_dataset = self.chunked_dataset.get_chunk(chunk_id)
                
                # è®­ç»ƒå½“å‰å—
                chunk_loss = self.train_chunk(chunk_dataset, chunk_id, cycle)
                cycle_losses.append(chunk_loss)
                
                # æ›´æ–°æœ€ä½³æŸå¤±
                if chunk_loss < best_loss and self.is_save_interval:
                    best_loss = chunk_loss
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    self._save_checkpoint(self.current_global_step, chunk_loss, is_best=True)
                
                # å†…å­˜æ¸…ç†
                self._cleanup_memory()
                self.is_save_interval = False
            
            # å‘¨æœŸç»“æŸç»Ÿè®¡
            avg_cycle_loss = np.mean(cycle_losses) if cycle_losses else 0.0
            elapsed_time = time.time() - start_time
            
            logger.info(f"âœ… å‘¨æœŸ {cycle + 1} å®Œæˆ")
            logger.info(f"  ğŸ“Š å¹³å‡æŸå¤±: {avg_cycle_loss:.4f}")
            logger.info(f"  ğŸ’ æœ€ä½³æŸå¤±: {best_loss:.4f}")
            logger.info(f"  â±ï¸ å·²ç”¨æ—¶é—´: {elapsed_time/3600:.2f}å°æ—¶")
            logger.info(f"  ğŸ“ˆ å…¨å±€æ­¥æ•°: {self.current_global_step:,}")
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ­¥æ•°
            if self.current_global_step >= plan['actual_total_steps']:
                logger.info("ğŸ¯ å·²è¾¾åˆ°ç›®æ ‡è®­ç»ƒæ­¥æ•°ï¼Œè®­ç»ƒå®Œæˆï¼")
                break
        
        # è®­ç»ƒå®Œæˆ
        total_time = time.time() - start_time
        logger.info("ğŸ‰ åˆ†å—è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ’ æœ€ä½³æŸå¤±: {best_loss:.4f}")
        logger.info(f"ğŸ“ˆ æ€»è®­ç»ƒæ­¥æ•°: {self.current_global_step:,}")
        logger.info(f"â±ï¸ æ€»è®­ç»ƒæ—¶é—´: {total_time/3600:.2f}å°æ—¶")
        
        # æœ€ç»ˆä¿å­˜
        self._save_checkpoint(self.current_global_step, best_loss, is_best=True)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='3B LLMåˆ†å—è®­ç»ƒç­–ç•¥è„šæœ¬')
    parser.add_argument('--config', default='config/config_chunked_strategy.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--chunk-size', type=int, default=10, help='æ¯å—æ ·æœ¬æ•°é‡')
    parser.add_argument('--epochs-per-chunk', type=int, default=5, help='æ¯å—è®­ç»ƒè½®æ•°')
    parser.add_argument('--save-interval', type=int, default=1000, help='ä¿å­˜é—´éš”')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨åˆ†å—è®­ç»ƒç­–ç•¥...")
    print(f"ğŸ“¦ å—å¤§å°: {args.chunk_size}")
    print(f"ğŸ”„ æ¯å—è½®æ•°: {args.epochs_per_chunk}")
    print(f"ğŸ’¾ ä¿å­˜é—´éš”: {args.save_interval}")
    
    try:
        trainer = ChunkedTrainingStrategy(args.config)
        
        # æ›´æ–°ç­–ç•¥å‚æ•°
        trainer.chunk_size = args.chunk_size
        trainer.epochs_per_chunk = args.epochs_per_chunk
        trainer.save_interval = args.save_interval
        
        trainer.train()
        
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
