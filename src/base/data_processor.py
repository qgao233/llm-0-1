#!/usr/bin/env python3
"""
æ•°æ®å¤„ç†æ¨¡å— - ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–åˆ†è¯å™¨
æ•´åˆtiktokenåˆ†è¯å™¨å’ŒåŸæœ‰çš„æ•°æ®å¤„ç†æµç¨‹
"""

import torch
import os
import urllib.request
import numpy as np
from typing import Dict, List, Tuple, Optional
from chinese_tokenizer import ChineseTokenizer, create_chinese_tokenizer

class DataProcessor:
    """
    æ•°æ®å¤„ç†å™¨ - æ”¯æŒå­—ç¬¦çº§å’ŒBPEåˆ†è¯
    """
    
    def __init__(self, tokenizer_type: str = "chinese_bpe", vocab_path: Optional[str] = None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            tokenizer_type: åˆ†è¯å™¨ç±»å‹ ("char_level" æˆ– "chinese_bpe")
            vocab_path: è¯æ±‡è¡¨è·¯å¾„
        """
        self.tokenizer_type = tokenizer_type
        self.vocab_path = vocab_path
        
        if tokenizer_type == "chinese_bpe":
            print("ğŸš€ ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–BPEåˆ†è¯å™¨")
            self.tokenizer = create_chinese_tokenizer()
            if vocab_path and os.path.exists(vocab_path):
                self.tokenizer.load_vocab(vocab_path)
        else:
            print("ğŸ“ ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯å™¨")
            self.tokenizer = None
        
        # æ•°æ®ç¼“å­˜
        self.dataset = None
        self.vocab = None
        self.itos = None
        self.stoi = None
        self.vocab_size = None
        
    def download_and_prepare_data(self, data_file: str = "xiyouji.txt", 
                                force_download: bool = False) -> Tuple[torch.Tensor, List, Dict, Dict]:
        """
        ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†
        
        Args:
            data_file: æ•°æ®æ–‡ä»¶å
            force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
            
        Returns:
            (dataset, vocab, itos, stoi)
        """
        # ä¸‹è½½æ•°æ®æ–‡ä»¶
        if not os.path.exists(data_file) or force_download:
            print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½æ•°æ®é›†åˆ° {data_file}...")
            url = "https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt"
            try:
                urllib.request.urlretrieve(url, data_file)
                print("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆ")
            except Exception as e:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
                # åˆ›å»ºç¤ºä¾‹æ•°æ®
                self._create_sample_data(data_file)
        
        # è¯»å–æ•°æ®
        with open(data_file, 'r', encoding='utf-8') as f:
            text_data = f.read()
        
        print(f"ğŸ“Š æ•°æ®æ–‡ä»¶å¤§å°: {len(text_data):,} å­—ç¬¦")
        
        # æ ¹æ®åˆ†è¯å™¨ç±»å‹å¤„ç†æ•°æ®
        if self.tokenizer_type == "chinese_bpe":
            return self._prepare_bpe_data(text_data)
        else:
            return self._prepare_char_data(text_data)
    
    def _create_sample_data(self, data_file: str):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®ï¼ˆå¦‚æœä¸‹è½½å¤±è´¥ï¼‰"""
        sample_text = """è¥¿æ¸¸è®°æ˜¯ä¸­å›½å¤ä»£ç¥è¯å°è¯´ï¼Œè®²è¿°äº†å­™æ‚Ÿç©ºã€çŒªå…«æˆ’ã€æ²™å’Œå°šä¿æŠ¤å”åƒ§è¥¿å¤©å–ç»çš„æ•…äº‹ã€‚
å­™æ‚Ÿç©ºæœ¬é¢†é«˜å¼ºï¼Œèƒ½ä¸ƒåäºŒå˜ï¼Œä¸€ä¸ªç­‹æ–—äº‘åä¸‡å…«åƒé‡Œã€‚
çŒªå…«æˆ’è´ªåƒæ‡’åšï¼Œä½†å¿ƒåœ°å–„è‰¯ã€‚æ²™å’Œå°šå¿ åšè€å®ï¼Œä»»åŠ³ä»»æ€¨ã€‚
å”åƒ§å¿ƒæ…ˆé¢å–„ï¼Œä¸€å¿ƒå‘ä½›ï¼Œä½†æœ‰æ—¶è¿‡äºä»æ…ˆã€‚
ä»–ä»¬å¸ˆå¾’å››äººå†ç»ä¹ä¹å…«åä¸€éš¾ï¼Œç»ˆäºå–å¾—çœŸç»ï¼Œä¿®æˆæ­£æœã€‚
è¿™æ˜¯ä¸€ä¸ªå…³äºå‹è°Šã€åšæŒå’Œæˆé•¿çš„æ•…äº‹ã€‚
äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿï¼Œæ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æˆä¸ºçƒ­é—¨é¢†åŸŸã€‚
è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚
å¤§æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡çš„æ•°æ®å’Œè®¡ç®—èµ„æºã€‚
åˆ†è¯å™¨æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„é‡è¦ç»„æˆéƒ¨åˆ†ã€‚"""
        
        with open(data_file, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        print(f"âœ… åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶: {data_file}")
    
    def _prepare_bpe_data(self, text_data: str) -> Tuple[torch.Tensor, List, Dict, Dict]:
        """ä½¿ç”¨BPEåˆ†è¯å™¨å‡†å¤‡æ•°æ®"""
        print("ğŸ”¨ ä½¿ç”¨BPEåˆ†è¯å™¨å¤„ç†æ•°æ®...")
        
        # ä½¿ç”¨åˆ†è¯å™¨ç¼–ç 
        tokens = self.tokenizer.encode(text_data)
        print(f"ğŸ“Š ç¼–ç ç»“æœ: {len(tokens):,} tokens")
        
        # è·å–è¯æ±‡è¡¨å¤§å°
        self.vocab_size = self.tokenizer.vocab_size
        
        # åˆ›å»ºæ­£ç¡®çš„è¯æ±‡è¡¨æ˜ å°„ - ç›´æ¥ä½¿ç”¨åˆ†è¯å™¨çš„è§£ç åŠŸèƒ½
        self.vocab = list(range(self.vocab_size))
        
        # ä¸ºBPEåˆ†è¯å™¨åˆ›å»ºç®€åŒ–çš„itosæ˜ å°„
        # æ³¨æ„ï¼šå¯¹äºBPEï¼Œæˆ‘ä»¬ä¸»è¦ä¾èµ–tokenizer.decode()è¿›è¡Œè§£ç 
        # itosä¸»è¦ç”¨äºå…¼å®¹æ€§å’Œè°ƒè¯•
        self.itos = {}
        print("ğŸ”¨ æ„å»ºBPEè¯æ±‡è¡¨æ˜ å°„ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰...")
        
        # åªä¸ºå‰1000ä¸ªtokenæ„å»ºæ˜ å°„ï¼Œç”¨äºè°ƒè¯•å’Œå…¼å®¹æ€§
        max_mapping = min(1000, self.vocab_size)
        for token_id in range(max_mapping):
            try:
                decoded = self.tokenizer.decode([token_id])
                self.itos[token_id] = decoded
            except:
                self.itos[token_id] = f'<UNK_{token_id}>'
        
        # ä¸ºå…¶ä½™tokenä½¿ç”¨é»˜è®¤æ˜ å°„
        for token_id in range(max_mapping, self.vocab_size):
            self.itos[token_id] = f'<UNK_{token_id}>'
        
        # åˆ›å»ºåå‘æ˜ å°„ï¼ˆè™½ç„¶åœ¨BPEä¸­ä¸å¸¸ç”¨ï¼‰
        self.stoi = {v: k for k, v in self.itos.items() if not v.startswith('<UNK_')}
        
        # è½¬æ¢ä¸ºtensor
        self.dataset = torch.tensor(tokens, dtype=torch.long)
        
        print(f"ğŸ“ˆ BPEè¯æ±‡è¡¨å¤§å°: {self.vocab_size:,}")
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.dataset.shape}")
        print(f"ğŸ“ itosæ˜ å°„æ ·ä¾‹: {list(self.itos.items())[:10]}")
        
        # æ‰“å°å‹ç¼©ç»Ÿè®¡
        self.tokenizer.print_stats()
        
        # ä¿å­˜è¯æ±‡è¡¨
        if self.vocab_path:
            self.tokenizer.save_vocab(self.vocab_path)
        
        return self.dataset, self.vocab, self.itos, self.stoi
    
    def _prepare_char_data(self, text_data: str) -> Tuple[torch.Tensor, List, Dict, Dict]:
        """ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯å™¨å‡†å¤‡æ•°æ®ï¼ˆåŸæœ‰æ–¹æ³•ï¼‰"""
        print("ğŸ”¨ ä½¿ç”¨å­—ç¬¦çº§åˆ†è¯å™¨å¤„ç†æ•°æ®...")
        
        # åˆ›å»ºå­—ç¬¦çº§è¯è¡¨
        self.vocab = sorted(list(set(text_data)))
        print(f"ğŸ“ˆ å­—ç¬¦çº§è¯è¡¨å¤§å°: {len(self.vocab)}")
        
        # åˆ›å»ºç¼–ç æ˜ å°„
        self.itos = {i: ch for i, ch in enumerate(self.vocab)}
        self.stoi = {ch: i for i, ch in enumerate(self.vocab)}
        
        # ç¼–ç æ•´ä¸ªæ•°æ®é›†
        self.dataset = torch.tensor([self.stoi[ch] for ch in text_data], dtype=torch.long)
        print(f"ğŸ“Š æ•°æ®é›†å½¢çŠ¶: {self.dataset.shape}")
        
        self.vocab_size = len(self.vocab)
        
        return self.dataset, self.vocab, self.itos, self.stoi
    
    def encode_text(self, text: str) -> List[int]:
        """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—"""
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            return self.tokenizer.encode(text)
        else:
            # å­—ç¬¦çº§ç¼–ç 
            return [self.stoi.get(ch, 0) for ch in text if ch in self.stoi]
    
    def decode_text(self, indices: List[int]) -> str:
        """è§£ç æ•°å­—åºåˆ—ä¸ºæ–‡æœ¬"""
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            # ç›´æ¥ä½¿ç”¨BPEåˆ†è¯å™¨è§£ç ï¼Œè¿™æ ·æ›´å‡†ç¡®
            try:
                return self.tokenizer.decode(indices)
            except Exception as e:
                print(f"âš ï¸ BPEè§£ç å¤±è´¥: {e}")
                # é™çº§åˆ°é€ä¸ªè§£ç 
                result = []
                for idx in indices:
                    if idx in self.itos:
                        result.append(self.itos[idx])
                    else:
                        result.append(f'<UNK_{idx}>')
                return ''.join(result)
        else:
            # å­—ç¬¦çº§è§£ç 
            return ''.join([self.itos.get(idx, '') for idx in indices])
    
    def get_batches(self, data: torch.Tensor, split: str, batch_size: int, 
                   context_window: int, device: torch.device = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """æ„å»ºæ‰¹æ¬¡æ•°æ®"""
        # åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸ºï¼Œè®­ç»ƒ80%ï¼ŒéªŒè¯10%ï¼Œæµ‹è¯•10%
        train = data[:int(0.8 * len(data))]
        val = data[int(0.8 * len(data)): int(0.9 * len(data))]
        test = data[int(0.9 * len(data)):]

        # é€‰æ‹©å¯¹åº”çš„æ•°æ®é›†
        batch_data = train
        if split == 'val':
            batch_data = val
        elif split == 'test':
            batch_data = test

        # å¦‚æœä½¿ç”¨GPUï¼Œå…ˆå°†æ•°æ®ç§»åŠ¨åˆ°GPUä¸Šï¼Œé¿å…é‡å¤ä¼ è¾“
        if device is not None and device.type == 'cuda' and batch_data.device != device:
            batch_data = batch_data.to(device)

        # åœ¨GPUä¸Šç”Ÿæˆéšæœºç´¢å¼•ï¼Œé¿å…CPUåˆ°GPUçš„ä¼ è¾“
        device_for_randint = device if device is not None else torch.device('cpu')
        ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,), device=device_for_randint)

        # ä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡åŒ–ç´¢å¼•æ–¹å¼ï¼Œé¿å…Pythonå¾ªç¯
        batch_indices = ix.unsqueeze(1) + torch.arange(context_window, device=ix.device).unsqueeze(0)
        target_indices = batch_indices + 1
        
        x = batch_data[batch_indices].long()
        y = batch_data[target_indices].long()

        # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if device is not None:
            x = x.to(device, non_blocking=True)
            y = y.to(device, non_blocking=True)

        return x, y
    
    def get_stats(self) -> Dict:
        """è·å–æ•°æ®å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'tokenizer_type': self.tokenizer_type,
            'vocab_size': self.vocab_size,
            'dataset_size': len(self.dataset) if self.dataset is not None else 0,
            'data_type': str(self.dataset.dtype) if self.dataset is not None else None
        }
        
        if self.tokenizer_type == "chinese_bpe" and self.tokenizer:
            stats.update(self.tokenizer.get_compression_ratio())
        
        return stats
    
    def print_stats(self):
        """æ‰“å°æ•°æ®å¤„ç†ç»Ÿè®¡"""
        stats = self.get_stats()
        
        print("\nğŸ“Š æ•°æ®å¤„ç†ç»Ÿè®¡:")
        print("=" * 40)
        print(f"åˆ†è¯å™¨ç±»å‹: {stats['tokenizer_type']}")
        print(f"è¯æ±‡è¡¨å¤§å°: {stats['vocab_size']:,}")
        print(f"æ•°æ®é›†å¤§å°: {stats['dataset_size']:,}")
        print(f"æ•°æ®ç±»å‹: {stats['data_type']}")
        
        if self.tokenizer_type == "chinese_bpe":
            print(f"å‹ç¼©æ¯”: {stats.get('total_compression_ratio', 0):.2f} å­—ç¬¦/token")
            print(f"ä¸­æ–‡å‹ç¼©æ¯”: {stats.get('chinese_compression_ratio', 0):.2f} å­—ç¬¦/token")


# ä¾¿åˆ©å‡½æ•°ï¼Œä¿æŒå‘åå…¼å®¹æ€§
def download_and_prepare_data(data_file: str = "xiyouji.txt", force_download: bool = False, 
                            use_bpe: bool = True) -> Tuple[torch.Tensor, List, Dict, Dict]:
    """
    ä¾¿åˆ©å‡½æ•°ï¼šä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†
    
    Args:
        data_file: æ•°æ®æ–‡ä»¶å
        force_download: æ˜¯å¦å¼ºåˆ¶é‡æ–°ä¸‹è½½
        use_bpe: æ˜¯å¦ä½¿ç”¨BPEåˆ†è¯å™¨
        
    Returns:
        (dataset, vocab, itos, stoi)
    """
    tokenizer_type = "chinese_bpe" if use_bpe else "char_level"
    vocab_path = "chinese_tokenizer_vocab.json" if use_bpe else None
    
    processor = DataProcessor(tokenizer_type=tokenizer_type, vocab_path=vocab_path)
    result = processor.download_and_prepare_data(data_file, force_download)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    processor.print_stats()
    
    return result


def encode_text(text: str, stoi: Dict = None, processor: DataProcessor = None) -> List[int]:
    """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ— - å…¼å®¹å‡½æ•°"""
    if processor:
        return processor.encode_text(text)
    elif stoi:
        return [stoi[ch] for ch in text if ch in stoi]
    else:
        raise ValueError("éœ€è¦æä¾› processor æˆ– stoi å‚æ•°")


def decode_text(indices: List[int], itos: Dict = None, processor: DataProcessor = None) -> str:
    """è§£ç æ•°å­—åºåˆ—ä¸ºæ–‡æœ¬ - å…¼å®¹å‡½æ•°"""
    if processor:
        # ä½¿ç”¨å¤„ç†å™¨çš„è§£ç æ–¹æ³•ï¼ˆæ¨èæ–¹å¼ï¼‰
        return processor.decode_text(indices)
    elif itos:
        # å­—ç¬¦çº§è§£ç çš„å…¼å®¹æ–¹å¼
        return ''.join([itos.get(i, f'<UNK_{i}>') for i in indices])
    else:
        raise ValueError("éœ€è¦æä¾› processor æˆ– itos å‚æ•°")


if __name__ == "__main__":
    print("ğŸ§ª æµ‹è¯•æ•°æ®å¤„ç†æ¨¡å—...")
    
    # æµ‹è¯•BPEåˆ†è¯å™¨
    print("\n1. æµ‹è¯•BPEåˆ†è¯å™¨:")
    bpe_dataset, bpe_vocab, bpe_itos, bpe_stoi = download_and_prepare_data(use_bpe=True)
    
    # æµ‹è¯•å­—ç¬¦çº§åˆ†è¯å™¨
    print("\n2. æµ‹è¯•å­—ç¬¦çº§åˆ†è¯å™¨:")
    char_dataset, char_vocab, char_itos, char_stoi = download_and_prepare_data(use_bpe=False)
    
    # æ¯”è¾ƒç»“æœ
    print("\nğŸ“Š åˆ†è¯å™¨æ¯”è¾ƒ:")
    print("=" * 40)
    print(f"BPE - è¯æ±‡è¡¨å¤§å°: {len(bpe_vocab):,}, æ•°æ®é›†å¤§å°: {len(bpe_dataset):,}")
    print(f"å­—ç¬¦çº§ - è¯æ±‡è¡¨å¤§å°: {len(char_vocab):,}, æ•°æ®é›†å¤§å°: {len(char_dataset):,}")
    print(f"å‹ç¼©æ•ˆæœ: BPEæ¯”å­—ç¬¦çº§å‡å°‘äº† {(1 - len(bpe_dataset)/len(char_dataset))*100:.1f}% çš„tokenæ•°é‡")
