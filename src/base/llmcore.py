import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from collections import OrderedDict

def get_device(force_cpu=False, device_config=None):
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„è®¾å¤‡ï¼Œæ”¯æŒå¤šGPU"""
    if force_cpu:
        device = torch.device("cpu")
        print("å¼ºåˆ¶ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        return device, False, []
    
    if not torch.cuda.is_available():
        device = torch.device("cpu")
        print("CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        return device, False, []
    
    # è·å–å¯ç”¨çš„GPUæ•°é‡å’Œä¿¡æ¯
    gpu_count = torch.cuda.device_count()
    print(f"ğŸ® æ£€æµ‹åˆ° {gpu_count} ä¸ªå¯ç”¨GPU:")
    
    available_gpus = []
    for i in range(gpu_count):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        available_gpus.append(i)
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šGPUè®­ç»ƒ
    use_multi_gpu = gpu_count > 1
    if device_config and 'multi_gpu' in device_config:
        use_multi_gpu = device_config['multi_gpu'] and gpu_count > 1
    
    # é€‰æ‹©ä¸»GPUè®¾å¤‡
    primary_device = torch.device("cuda:0")
    
    if use_multi_gpu:
        print(f"ğŸš€ å¯ç”¨å¤šGPUè®­ç»ƒï¼Œä½¿ç”¨ {gpu_count} ä¸ªGPU")
        return primary_device, True, available_gpus
    else:
        print(f"ğŸ“± ä½¿ç”¨å•GPUè®­ç»ƒ: {torch.cuda.get_device_name(0)}")
        return primary_device, False, [0]



class RMSNorm(nn.Module):
    def __init__(self, layer_shape, eps=1e-8, bias=False):
        super(RMSNorm, self).__init__()

        # torchä¸­register_parameter()åŠŸèƒ½ä¸ºï¼šå‘æˆ‘ä»¬å»ºç«‹çš„ç½‘ç»œmoduleæ·»åŠ parameter
        # å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹pytorchå®˜æ–¹å°è£…å¥½çš„RMSNormåŠŸèƒ½æ¨¡å—æ·»åŠ ä¸€ä¸ªå¯ä»¥è®­ç»ƒå‚æ•°çš„å±‚ï¼Œå‘½åä¸ºscaleï¼Œå¹¶åˆå§‹åŒ–ä¸ºå½¢çŠ¶ä¸ºlayer_shapeï¼Œæ‰€æœ‰å€¼ä¸º1çš„å¼ é‡çŸ©é˜µã€‚
        self.register_parameter("scale", nn.Parameter(torch.ones(layer_shape)))

    def forward(self, x):
        # è®¡ç®—FrobeniusèŒƒæ•°ï¼ˆçƒæŸä¸ªçŸ©é˜µä¸­æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œå†å¼€æ–¹å¾—åˆ°ï¼Œè¯¥èŒƒæ•°ç”¨æ¥è¡¡é‡çŸ©é˜µçš„å¤§å°ï¼Œè¯¦æƒ…è¯·ç™¾åº¦ï¼‰, RMS = 1/sqrt(N) * Frobenius
        # å…·ä½“æ¥è¯´ï¼Œtorch.linalg.norm(x, dim=(1, 2))è®¡ç®—äº†xåœ¨ç¬¬1å’Œç¬¬2ç»´åº¦ä¸Šçš„èŒƒæ•°ã€‚ç„¶åï¼Œå°†ç»“æœä¹˜ä»¥x[0].numel() ** -.5ã€‚x[0].numel()è¡¨ç¤ºxç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå³xçš„ç¬¬ä¸€è¡Œï¼‰çš„å…ƒç´ ä¸ªæ•°ï¼Œ** -.5è¡¨ç¤ºæ±‚å¹³æ–¹æ ¹çš„å€’æ•°ã€‚
        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5
        # print(ff_rms.shape)
        # å°†ff_rmsç®—å­åº”ç”¨äºè¾“å…¥çš„å¼ é‡xï¼Œä¾æ®å…¬å¼ï¼Œåšé™¤æ³•ï¼Œå› ä¸ºè¾“å…¥å‘é‡xæ˜¯ä¸‰ç»´çš„ï¼Œå› æ­¤éœ€è¦å¯¹ff_rmsè¿›è¡Œå‡ä¸¤ç»´ï¼Œä¹Ÿå˜æˆä¸‰ç»´çš„å¼ é‡ã€‚è¿™æ ·å¯ä»¥è¿›è¡Œå…ƒç´ ä¹‹é—´çš„è®¡ç®—ã€‚
        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)
        # print(raw.shape)
        # è¿”å›scaleç¼©æ”¾åå½’ä¸€åŒ–çš„å¼ é‡
        # print(self.scale[:x.shape[1], :].unsqueeze(0) * raw)
        return self.scale[:x.shape[1], :].unsqueeze(0) * raw

def get_rotary_matrix(context_window, embedding_dim, device=None):
    # åˆå§‹åŒ–ä¸€ä¸ª0å¡«å……ï¼Œå½¢çŠ¶ä¸ºï¼ˆcontext_window, embedding_dim, embedding_dimï¼‰çš„å¼ é‡çŸ©é˜µï¼Œå…¶ä¸­context_windowä¸ºtokenæ•°é‡ï¼Œåé¢ä¸¤ä¸ªembedding_dimç»„æˆæ­£æ–¹å½¢çŸ©é˜µï¼Œä¸åé¢çš„attentionè®¡ç®—å¯¹é½æ ¼å¼
    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False, device=device)

    # éå†æ¯ä¸€ä¸ªä½ç½®çš„token
    for position in range(context_window):
        # è¿˜è®°å¾—æˆ‘çš„ä¸Šä¸€ç¯‡æ–‡ç« ä¸­è¯´çš„ï¼Œå¯¹äºç‰¹å¾ï¼Œä¸¤ä¸¤ç»„åˆå—ï¼Œå› æ­¤éœ€è¦å¾ªç¯çš„æ¬¡æ•°ä¸ºembedding_dimé™¤ä»¥2
        for i in range(embedding_dim // 2):
            # è®¾ç½®Î¸å€¼ï¼Œé‡‡æ ·é¢‘ç‡ï¼Œæˆ–è€…è¯´æ—‹è½¬é¢‘ç‡ï¼Œæ—‹è½¬è§’éƒ½å¯ä»¥ï¼Œé™¤ä»¥embedding_dimé˜²æ­¢æ¢¯åº¦é—®é¢˜ã€‚
            theta = 10000. ** (-2. * (i - 1) / embedding_dim)
            # æ ¹æ®æ¬§æ‹‰å…¬å¼ï¼Œè®¡ç®—æ—‹è½¬çš„è§’åº¦ï¼Œåˆ†åˆ«æœ‰sin å’Œcosï¼Œå°†è®¡ç®—æ‹‰åˆ°å¤æ•°ç©ºé—´ï¼Œå¹¶å°†æ—‹è½¬è§’åº¦åº”ç”¨åœ¨ä¸Šé¢çš„0å¡«å……çš„çŸ©é˜µ
            m_theta = position * theta
            R[position, 2 * i, 2 * i] = np.cos(m_theta)
            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
            # å¾—åˆ°çš„ç»“æœæ˜¯æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µï¼Œåˆ°è¿™é‡Œè¿˜æ²¡è¦†ç›–åˆ°attention
    return R

# æ­¤ä¸ºå•å¤´æ³¨æ„åŠ›æœºåˆ¶
class RoPEMaskedAttentionHead(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # è®¡ç®—Qæƒé‡çŸ©é˜µ
        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # è®¡ç®—Kæƒé‡çŸ©é˜µ
        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)
        # è®¡ç®—Væƒé‡çŸ©é˜µ
        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)
        
        self.context_window = config['context_window']
        self.embedding_dim = config['d_model']
        
        # é¢„è®¡ç®—å¹¶ç¼“å­˜æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µ - ä¸ä½œä¸ºæ¨¡å‹å‚æ•°ä¿å­˜
        self.R_cache = None
        self.cache_device = None

    def _get_rotary_matrix(self, device):
        """è·å–æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µï¼Œä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è®¡ç®—"""
        # å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–è®¾å¤‡ä¸åŒ¹é…ï¼Œé‡æ–°åˆ›å»º
        if self.R_cache is None or self.cache_device != device:
            R = torch.zeros((self.context_window, self.embedding_dim, self.embedding_dim), 
                          requires_grad=False, device=device)
            
            # éå†æ¯ä¸€ä¸ªä½ç½®çš„token
            for position in range(self.context_window):
                # è®¡ç®—æ—‹è½¬è§’åº¦
                for i in range(self.embedding_dim // 2):
                    theta = 10000. ** (-2. * (i - 1) / self.embedding_dim)
                    m_theta = position * theta
                    R[position, 2 * i, 2 * i] = np.cos(m_theta)
                    R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)
                    R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)
                    R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)
            
            # ç¼“å­˜çŸ©é˜µå’Œè®¾å¤‡ä¿¡æ¯
            self.R_cache = R
            self.cache_device = device
            
        return self.R_cache

    def forward(self, x, return_attn_weights=False):
        # å‰å‘ä¼ æ’­æ—¶ï¼Œè¾“å…¥çŸ©é˜µçš„å½¢çŠ¶ä¸º(batch, sequence length, dimension)
        b, m, d = x.shape  # batch size, sequence length, dimension

        # çº¿æ€§å˜æ¢Q,K,V
        q = self.w_q(x)
        k = self.w_k(x)
        v = self.w_v(x)

        # è·å–ç¼“å­˜çš„æ—‹è½¬ä½ç½®ç¼–ç çŸ©é˜µ
        R = self._get_rotary_matrix(x.device)
        
        # å°†æ—‹è½¬ä½ç½®ç¼–ç åº”ç”¨äºQå’ŒKï¼Œåªä½¿ç”¨éœ€è¦çš„éƒ¨åˆ†
        q_rotated = (torch.bmm(q.transpose(0, 1), R[:m])).transpose(0, 1)
        k_rotated = (torch.bmm(k.transpose(0, 1), R[:m])).transpose(0, 1)

        # å¯¹æ³¨æ„åŠ›æœºåˆ¶ç‚¹ç§¯è¿›è¡Œç­‰æ¯”ä¾‹ç¼©æ”¾
        activations = F.scaled_dot_product_attention(
            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True
        )
        
        # å¦‚æœéœ€è¦è¿”å›æ³¨æ„åŠ›æƒé‡
        if return_attn_weights:
            # ç”Ÿæˆä¸Šä¸‰è§’å¸ƒå°”æ©ç ï¼ˆæœªæ¥ä½ç½®ä¸ºTrueï¼‰
            attn_mask = torch.triu(torch.ones(m, m, device=q_rotated.device), diagonal=1).bool()
            # è®¡ç®—åŸå§‹æ³¨æ„åŠ›åˆ†æ•°
            attn_scores = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d)
            # é®ç›–æœªæ¥ä½ç½®ï¼ˆè®¾ä¸º-1e9ï¼‰
            attn_scores = attn_scores.masked_fill(attn_mask, -1e9)
            # Softmaxå½’ä¸€åŒ–
            attn_weights = F.softmax(attn_scores, dim=-1)
            return activations, attn_weights

        return activations

# å•å¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°å®Œæ¯•ï¼Œä¸‹é¢å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
class RoPEMaskedMultiheadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        # ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´å¯¹è±¡æ„å»ºå®Œæ¯•äº†ï¼Œå¤šå¤´çš„ï¼Œé¦–å…ˆå¤šæ¬¡åˆ›å»ºè¿™ä¸ªå¯¹è±¡ã€‚ç”Ÿæˆå¤šä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œå¡åˆ°ä¸€ä¸ªåˆ—è¡¨é‡Œã€‚
        self.heads = nn.ModuleList([
            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])
        ])
        # åœ¨æ¨¡å‹ç»“æ„ä¸Šï¼Œåˆ›å»ºä¸€ä¸ªçº¿æ€§å±‚ï¼ˆéšè—å±‚ï¼‰ï¼Œç”¨äºçº¿å‹è¾“å‡ºæ³¨æ„åŠ›æœºåˆ¶å¤´è¾“å‡ºçš„å¼ é‡çŸ©é˜µï¼Œå¯»æ‰¾å¤šå¤´ä¹‹é—´çš„ç‰¹å¾ï¼Œä½†æ˜¯æ›´ä¸»è¦çš„æ˜¯ï¼Œxç»è¿‡å¤šå¤´è®¡ç®—åå½¢çŠ¶æ”¹å˜äº†ï¼Œåˆ›å»ºçº¿æ€§å±‚ï¼Œè®©å¼ é‡çŸ©é˜µå˜å›åŸæ¥è¾“å…¥çš„å½¢çŠ¶ã€‚
        # åŒæ—¶ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½¿ç”¨éšæœºç¥ç»å…ƒå¤±æ´»ï¼Œæ¯”ç‡0.1
        # çº¿æ€§å±‚è¾“å…¥å½¢çŠ¶ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„å¤´æ•°ï¼Œä¹˜ä»¥çŸ©é˜µçš„ç»´åº¦ï¼Œå…³è”åˆ°ä¿ºçš„ä¸Šä¸€ç¯‡æ–‡ç« ï¼Œå°±æ˜¯keyçŸ©é˜µï¼Œåœ¨å¤šå¤´ä¹‹é—´å…±äº«æƒé‡ï¼Œå‡å°‘è®¡ç®—çš„æ€ç»´ã€‚ è¾“å‡ºä¸ºï¼šæ¨¡å‹çš„embeddingç»´åº¦æ•°
        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])
        self.dropout = nn.Dropout(0.1)

    def forward(self, x):
        # è¾“å…¥çŸ©é˜µå½¢çŠ¶xï¼š (batch, sequence length, dimension)

        # æ¯ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶å¤´ï¼Œéƒ½ä¼ å…¥Xè¿›è¡Œè®¡ç®—ã€‚ï¼ˆè¿™ä¸ªåœ°æ–¹å¼€å¯å¹¶è¡Œæ‰§è¡Œä¼šä¸ä¼šå¿«ä¸€äº›ï¼Œä½†æ˜¯ä¸çŸ¥é“pytorchæ˜¯ä¸æ˜¯è‡ªåŠ¨è°ƒç”¨å¹¶è¡Œï¼‰
        heads = [h(x) for h in self.heads]  # ä¸è¿”å›æ³¨æ„åŠ›æƒé‡ï¼Œåªè¿”å›æ¿€æ´»å€¼
        # è¾“å…¥å¼ é‡xç»è¿‡å¤šä¸ªå¤´è®¡ç®—attentionï¼ˆåŒæ—¶ï¼Œattentionæ˜¯å·²ç»è¦†ç›–äº†RoPEçš„ï¼‰ï¼Œé‡æ–°æ‹¼æ¥æˆæ–°çš„çŸ©é˜µï¼Œé‡æ–°æ”¾å…¥å˜é‡xã€‚åˆ°è¿™é‡Œä½ åº”è¯¥è§‰å¾—ï¼šé‚£çŸ©é˜µå½¢çŠ¶ä¸å°±å˜äº†å—
        x = torch.cat(heads, dim=-1)

        # è¿™ä¸ï¼Œçº¿æ€§å±‚çš„ä½œç”¨æ¥äº†
        x = self.linear(x)

        # éšæœºå¤±æ´»ä¸€ä¸‹ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        x = self.dropout(x)
        return x





class SwiGLU(nn.Module):
    """SwiGLUæ¿€æ´»å‡½æ•° - æ”¹è¿›ç‰ˆæœ¬"""
    def __init__(self, input_dim, hidden_dim=None):
        super().__init__()
        if hidden_dim is None:
            hidden_dim = input_dim
            
        # é—¨æ§çº¿æ€§å±‚
        self.gate_proj = nn.Linear(input_dim, hidden_dim, bias=False)
        # ä¸ŠæŠ•å½±å±‚
        self.up_proj = nn.Linear(input_dim, hidden_dim, bias=False)
        # ä¸‹æŠ•å½±å±‚
        self.down_proj = nn.Linear(hidden_dim, input_dim, bias=False)

    def forward(self, x):
        # SwiGLU: swish(gate_proj(x)) * up_proj(x)
        gate = self.gate_proj(x)
        up = self.up_proj(x)
        # Swishæ¿€æ´»å‡½æ•°: x * sigmoid(x)
        swish_gate = gate * torch.sigmoid(gate)
        # é—¨æ§æœºåˆ¶
        hidden = swish_gate * up
        # ä¸‹æŠ•å½±
        output = self.down_proj(hidden)
        return output


# ç°åœ¨æˆ‘ä»¬æ‹¥æœ‰äº†æ‰€æœ‰çš„ç®—å­ï¼ŒRMSï¼ŒROPE,SWIGLUï¼Œæˆ‘ä»¬æ­å»ºæˆ‘ä»¬çš„Llmpvqï¼ é¦–å…ˆå®ç°Llmpvqçš„åŠŸèƒ½å—ï¼Œç„¶åå †å ã€‚
# åŠŸèƒ½æ²¡ä»€ä¹ˆå¥½è®²çš„ï¼Œå¦‚æœä»”ç»†çœ‹åˆ°äº†è¿™é‡Œï¼Œä¸‹é¢çš„æ¯ä¸€è¡Œä»£ç éƒ½éš¾ä¸ä½ä½ ã€‚
class LlmpvqBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.rms_1 = RMSNorm((config['context_window'], config['d_model']))
        self.attention = RoPEMaskedMultiheadAttention(config)
        self.rms_2 = RMSNorm((config['context_window'], config['d_model']))
        
        # æ”¹è¿›çš„FFNè®¾è®¡ï¼Œä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°
        ffn_dim = config.get('ffn_dim', config['d_model'] * 4)
        self.feedforward = nn.Sequential(
            SwiGLU(config['d_model'], ffn_dim),
            nn.Dropout(config.get('dropout', 0.1))
        )

    def forward(self, x):
        # Pre-normæ¶æ„ï¼šå…ˆå½’ä¸€åŒ–å†è®¡ç®—
        # æ³¨æ„åŠ›æœºåˆ¶çš„æ®‹å·®è¿æ¥
        x = x + self.attention(self.rms_1(x))
        
        # FFNçš„æ®‹å·®è¿æ¥
        x = x + self.feedforward(self.rms_2(x))
        
        return x

# ç°åœ¨ï¼Œæˆ‘ä»¬ç»„è£…Llmpvq - ä¼˜åŒ–ç‰ˆæœ¬
class Llmpvq(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        
        # TokenåµŒå…¥å±‚
        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])
        
        # ä½ç½®åµŒå…¥ï¼ˆå¯é€‰ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨RoPEï¼‰
        # self.pos_embeddings = nn.Embedding(config['context_window'], config['d_model'])
        
        # Dropoutå±‚
        self.dropout = nn.Dropout(config.get('dropout', 0.1))
        
        # Transformerå—å †å 
        self.Llmpvq_blocks = nn.Sequential(
            OrderedDict([(f"Llmpvq_{i}", LlmpvqBlock(config)) for i in range(config['n_layers'])])
        )
        
        # æœ€ç»ˆçš„å±‚å½’ä¸€åŒ–
        self.final_norm = RMSNorm((config['context_window'], config['d_model']))
        
        # è¾“å‡ºæŠ•å½±å±‚ï¼ˆè¯­è¨€æ¨¡å‹å¤´ï¼‰
        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'], bias=False)
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        print(f"æ¨¡å‹æ€»å‚æ•°é‡: {total_params/1e9:.2f}B ({total_params:,})")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params/1e9:.2f}B ({trainable_params:,})")
        print(f"æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, idx, targets=None):
        batch_size, seq_len = idx.shape
        
        # TokenåµŒå…¥
        x = self.embeddings(idx)  # (batch_size, seq_len, d_model)
        
        # æ·»åŠ dropout
        x = self.dropout(x)
        
        # é€šè¿‡Transformerå—
        x = self.Llmpvq_blocks(x)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        x = self.final_norm(x)
        
        # è¯­è¨€æ¨¡å‹å¤´
        logits = self.lm_head(x)  # (batch_size, seq_len, vocab_size)

        # è®¡ç®—æŸå¤±
        if targets is not None:
            # é‡å¡‘å¼ é‡ä»¥è®¡ç®—äº¤å‰ç†µæŸå¤±
            logits_flat = logits.view(-1, self.config['vocab_size'])
            targets_flat = targets.view(-1)
            loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-1, reduction='mean')
            # ç¡®ä¿æŸå¤±æ˜¯æ ‡é‡ - åœ¨å¤šGPUç¯å¢ƒä¸‹ç‰¹åˆ«é‡è¦
            if loss.dim() > 0:
                loss = loss.mean()
            return logits, loss
        else:
            return logits

# ä¾¿åˆ©å‡½æ•°
def create_model(config=None):
    """åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¤šGPU"""
    if config is None:
        raise ValueError("å¿…é¡»æä¾›é…ç½®å‚æ•°,configä¸èƒ½ä¸ºNone")
    
    device = config.get('device', torch.device('cpu'))
    multi_gpu = config.get('multi_gpu', False)
    gpu_ids = config.get('gpu_ids', [])
    
    # å¦‚æœä½¿ç”¨GPUï¼Œè¿›è¡Œä¼˜åŒ–è®¾ç½®
    if device.type == 'cuda':
        print("ğŸš€ é…ç½®GPUä¼˜åŒ–...")
        
        # å¯ç”¨TensorFloat-32 (TF32) åŠ é€Ÿ (A100/RTX30ç³»åˆ—åŠä»¥ä¸Š)
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        
        # å¯ç”¨èåˆå†…æ ¸ä¼˜åŒ–
        torch.backends.cuda.enable_flash_sdp(True)  # Flash Attention
        
        # é¢„åˆ†é…å†…å­˜é¿å…ç¢ç‰‡åŒ–
        print("é¢„åˆ†é…GPUå†…å­˜...")
        temp_tensor = torch.randn(1000, 1000, device=device)
        del temp_tensor
        torch.cuda.empty_cache()
        print("GPUä¼˜åŒ–é…ç½®å®Œæˆ")
    
    print("ğŸ—ï¸ åˆ›å»ºæ¨¡å‹...")
    model = Llmpvq(config)
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°ä¸»è®¾å¤‡
    model = model.to(device)
    print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°ä¸»è®¾å¤‡: {device}")
    
    # å¦‚æœå¯ç”¨å¤šGPUï¼Œä½¿ç”¨DataParallel
    if multi_gpu and len(gpu_ids) > 1:
        print(f"ğŸ¯ å¯ç”¨å¤šGPUå¹¶è¡Œè®­ç»ƒï¼Œä½¿ç”¨GPU: {gpu_ids}")
        model = nn.DataParallel(model, device_ids=gpu_ids)
        print(f"âœ… DataParallelå·²å¯ç”¨ï¼Œä¸»GPU: {device}")
        
        # è®¡ç®—å¤šGPUä¸‹çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
        effective_batch_size = config.get('batch_size', 32) * len(gpu_ids)
        print(f"ğŸ“Š æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {config.get('batch_size', 32)} Ã— {len(gpu_ids)} = {effective_batch_size}")
    
    # ç¡®ä¿æ‰€æœ‰GPUæ“ä½œå®Œæˆ
    if device.type == 'cuda':
        torch.cuda.synchronize()
    
    return model

# ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨å’Œè®­ç»ƒç¯å¢ƒåˆå§‹åŒ–å‡½æ•°å·²ç§»åŠ¨åˆ° train.py