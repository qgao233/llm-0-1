from llmcore import create_model
from config_manager import get_config_manager
import torch
import torch.nn.functional as F
import json
import os
import time
import pandas as pd
import numpy as np
import urllib.request

# ==================== æ•°æ®å‡†å¤‡å‡½æ•° ====================

def download_and_prepare_data(data_file="xiyouji.txt", force_download=False):
    """ä¸‹è½½å¹¶å‡†å¤‡æ•°æ®é›†"""
    if not os.path.exists(data_file) or force_download:
        print(f"æ­£åœ¨ä¸‹è½½æ•°æ®é›†åˆ° {data_file}...")
        url = "https://raw.githubusercontent.com/mc112611/PI-ka-pi/main/xiyouji.txt"
        urllib.request.urlretrieve(url, data_file)
        print("æ•°æ®é›†ä¸‹è½½å®Œæˆ")
    
    # è¯»å–æ•°æ®
    with open(data_file, 'r', encoding='utf-8') as f:
        lines = f.read()
    
    # åˆ›å»ºè¯è¡¨
    vocab = sorted(list(set(lines)))
    print(f'è¯è¡¨å¤§å°: {len(vocab)}')
    
    # åˆ›å»ºç¼–ç æ˜ å°„
    itos = {i: ch for i, ch in enumerate(vocab)}
    stoi = {ch: i for i, ch in enumerate(vocab)}
    
    # ç¼–ç æ•´ä¸ªæ•°æ®é›†
    dataset = torch.tensor([stoi[ch] for ch in lines], dtype=torch.int16)
    print(f'æ•°æ®é›†å½¢çŠ¶: {dataset.shape}')
    
    return dataset, vocab, itos, stoi

def encode_text(text, stoi):
    """ç¼–ç æ–‡æœ¬ä¸ºæ•°å­—åºåˆ—"""
    return [stoi[ch] for ch in text]

def decode_text(indices, itos):
    """è§£ç æ•°å­—åºåˆ—ä¸ºæ–‡æœ¬"""
    return ''.join([itos[i] for i in indices])

# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================

def get_batches(data, split, batch_size, context_window, device=None):
    """æ„å»ºæ‰¹æ¬¡æ•°æ®"""
    # åˆ‡åˆ†è®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ï¼Œæ¯”ä¾‹ä¸ºï¼Œè®­ç»ƒ80%ï¼ŒéªŒè¯10%ï¼Œæµ‹è¯•10%
    train = data[:int(0.8 * len(data))]
    val = data[int(0.8 * len(data)): int(0.9 * len(data))]
    test = data[int(0.9 * len(data)):]

    # å°†å…¨éƒ¨çš„è®­ç»ƒæ•°æ®ä½œä¸ºbatchï¼ŒéªŒè¯é›†ï¼Œæµ‹è¯•é›†ä¹Ÿæ¢ä¸ªå˜é‡å­˜å‚¨ï¼ˆå•çº¯ä¸ºäº†æ–¹ä¾¿çœ‹ï¼‰
    batch_data = train
    if split == 'val':
        batch_data = val
    if split == 'test':
        batch_data = test

    # å¦‚æœä½¿ç”¨GPUï¼Œå…ˆå°†æ•°æ®ç§»åŠ¨åˆ°GPUä¸Šï¼Œé¿å…é‡å¤ä¼ è¾“
    if device is not None and device.type == 'cuda' and batch_data.device != device:
        batch_data = batch_data.to(device)

    # åœ¨GPUä¸Šç”Ÿæˆéšæœºç´¢å¼•ï¼Œé¿å…CPUåˆ°GPUçš„ä¼ è¾“
    device_for_randint = device if device is not None else torch.device('cpu')
    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,), device=device_for_randint)

    # ä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡åŒ–ç´¢å¼•æ–¹å¼ï¼Œé¿å…Pythonå¾ªç¯
    # åˆ›å»ºç´¢å¼•çŸ©é˜µï¼Œä¸€æ¬¡æ€§è·å–æ‰€æœ‰batchæ•°æ®
    batch_indices = ix.unsqueeze(1) + torch.arange(context_window, device=ix.device).unsqueeze(0)
    target_indices = batch_indices + 1
    
    x = batch_data[batch_indices].long()
    y = batch_data[target_indices].long()

    # ç¡®ä¿æ•°æ®åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if device is not None:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)

    # è¿”å›ç‰¹å¾å€¼ï¼Œç›®æ ‡å€¼
    return x, y

# ==================== è¯„ä¼°å‡½æ•° ====================

@torch.no_grad()
def evaluate_loss(model, dataset, config):
    """æ„é€ ä¸€ä¸ªè¯„ä¼°å‡½æ•°"""
    # è¯„ä¼°ç»“æœå­˜å‚¨å˜é‡
    out = {}

    # å°†æ¨¡å‹ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()

    # åˆ†åˆ«ä¼šåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†é‡Œé€šè¿‡get_batchs()å‡½æ•°å–è¯„ä¼°æ•°æ®
    for split in ["train", "val"]:

        losses = []

        # è¯„ä¼°10ä¸ªbatch
        for _ in range(10):
            # æ‹¿åˆ°ç‰¹å¾å€¼ï¼ˆè¾“å…¥æ•°æ®ï¼‰ï¼Œä»¥åŠç›®æ ‡å€¼ï¼ˆè¾“å‡ºæ•°æ®ï¼‰
            xb, yb = get_batches(dataset, split, config['batch_size'], config['context_window'], config.get('device'))

            # æŠŠæ‹¿åˆ°çš„æ•°æ®ä¸¢è¿›æ¨¡å‹ï¼Œå¾—åˆ°losså€¼
            _, loss = model(xb, yb)

            # æ›´æ–°losså­˜å‚¨
            losses.append(loss.item())

        # è¿™é‡Œå°±æ˜¯å¤§å®¶ç»å¸¸åœ¨æ§åˆ¶å°çœ‹åˆ°çš„ "train_loss"  "valid_loss"ç”±æ¥
        out[split] = np.mean(losses)

    # è¯„ä¼°å®Œäº†ï¼Œåˆ«å¿˜äº†æŠŠæ¨¡å‹å†ç½®å›è®­ç»ƒçŠ¶æ€ï¼Œä¸‹ä¸€ä¸ªepochè¿˜è¦ç»§ç»­è®­ç»ƒå‘¢
    model.train()

    return out

# ==================== ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ====================

def create_optimizer(model, config_manager=None, **kwargs):
    """åˆ›å»ºä¼˜åŒ–å™¨ - ä½¿ç”¨é…ç½®æ–‡ä»¶æˆ–ä¼ å…¥å‚æ•°"""
    if config_manager is not None:
        opt_config = config_manager.get_optimizer_config()
    else:
        # é»˜è®¤é…ç½®
        opt_config = {
            'type': 'Adam',
            'lr': 0.001,
            'betas': [0.9, 0.999],
            'weight_decay': 0.1,
            'eps': 1e-8
        }
    
    # ç”¨ä¼ å…¥çš„å‚æ•°è¦†ç›–é…ç½®
    opt_config.update(kwargs)
    
    optimizer_type = opt_config.get('type', 'Adam')
    
    if optimizer_type.lower() == 'adamw':
        return torch.optim.AdamW(
            model.parameters(),
            lr=opt_config['lr'],
            betas=tuple(opt_config['betas']),
            weight_decay=opt_config['weight_decay'],
            eps=opt_config['eps']
        )
    else:  # é»˜è®¤ä½¿ç”¨ Adam
        return torch.optim.Adam(
            model.parameters(),
            lr=opt_config['lr'],
            betas=tuple(opt_config['betas']),
            weight_decay=opt_config['weight_decay'],
            eps=opt_config['eps']
        )

class WarmupCosineScheduler:
    """å¸¦é¢„çƒ­çš„ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ŒåŒ…å«è‡ªé€‚åº”è°ƒæ•´åŠŸèƒ½"""
    def __init__(self, optimizer, warmup_steps=100, max_steps=10000, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_steps = warmup_steps
        self.max_steps = max_steps
        self.min_lr = min_lr
        self.base_lr = optimizer.param_groups[0]['lr']
        self.current_step = 0
        
        # è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
        self.plateau_patience = 10  # lossåœæ»å®¹å¿æ­¥æ•°
        self.plateau_factor = 0.5   # lossåœæ»æ—¶çš„å­¦ä¹ ç‡è¡°å‡å› å­
        self.best_loss = float('inf')
        self.plateau_count = 0
        
    def step(self, val_loss=None):
        self.current_step += 1
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªé€‚åº”è°ƒæ•´å­¦ä¹ ç‡
        if val_loss is not None:
            if val_loss < self.best_loss:
                self.best_loss = val_loss
                self.plateau_count = 0
            else:
                self.plateau_count += 1
                
            # å¦‚æœlossåœæ»ï¼Œé™ä½å­¦ä¹ ç‡
            if self.plateau_count >= self.plateau_patience:
                self.base_lr *= self.plateau_factor
                self.plateau_count = 0
                print(f"  ğŸ“‰ æ£€æµ‹åˆ°lossåœæ»ï¼Œå­¦ä¹ ç‡é™ä½è‡³: {self.base_lr:.2e}")
        
        if self.current_step <= self.warmup_steps:
            # é¢„çƒ­é˜¶æ®µï¼šçº¿æ€§å¢é•¿
            lr = self.base_lr * (self.current_step / self.warmup_steps)
        else:
            # ä½™å¼¦é€€ç«é˜¶æ®µ
            progress = (self.current_step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
            progress = min(progress, 1.0)
            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + np.cos(np.pi * progress))
        
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
    
    def get_lr(self):
        return [group['lr'] for group in self.optimizer.param_groups]
    
    def state_dict(self):
        """è¿”å›è°ƒåº¦å™¨çš„çŠ¶æ€å­—å…¸"""
        return {
            'current_step': self.current_step,
            'base_lr': self.base_lr,
            'best_loss': self.best_loss,
            'plateau_count': self.plateau_count,
            'warmup_steps': self.warmup_steps,
            'max_steps': self.max_steps,
            'min_lr': self.min_lr,
            'plateau_patience': self.plateau_patience,
            'plateau_factor': self.plateau_factor
        }
    
    def load_state_dict(self, state_dict):
        """åŠ è½½è°ƒåº¦å™¨çš„çŠ¶æ€å­—å…¸"""
        self.current_step = state_dict['current_step']
        self.base_lr = state_dict['base_lr']
        self.best_loss = state_dict['best_loss']
        self.plateau_count = state_dict['plateau_count']
        self.warmup_steps = state_dict['warmup_steps']
        self.max_steps = state_dict['max_steps']
        self.min_lr = state_dict['min_lr']
        self.plateau_patience = state_dict['plateau_patience']
        self.plateau_factor = state_dict['plateau_factor']

def create_scheduler(optimizer, config_manager=None, max_steps=10000, **kwargs):
    """åˆ›å»ºå¸¦é¢„çƒ­çš„å­¦ä¹ ç‡è°ƒåº¦å™¨"""
    if config_manager is not None:
        sch_config = config_manager.get_scheduler_config()
    else:
        # é»˜è®¤é…ç½®
        sch_config = {
            'warmup_steps': 100,
            'min_lr': 1e-6,
            'plateau_patience': 10,
            'plateau_factor': 0.5
        }
    
    # ç”¨ä¼ å…¥çš„å‚æ•°è¦†ç›–é…ç½®
    sch_config.update(kwargs)
    
    return WarmupCosineScheduler(
        optimizer, 
        warmup_steps=sch_config['warmup_steps'],
        max_steps=max_steps,
        min_lr=sch_config['min_lr']
    )

# ==================== è®­ç»ƒå‡½æ•° ====================

def train(model, optimizer, dataset, config, scheduler=None, print_logs=False):
    """æ„å»ºè®­ç»ƒå‡½æ•°"""
    # losså­˜å‚¨
    losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 50  # æ—©åœè€å¿ƒå€¼

    # è®­ç»ƒæ—¶é—´è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # å¾ªç¯è®­ç»ƒæŒ‡å®šepochçš„è½®æ•°
    for epoch in range(config['epochs']):
        # ä¼˜åŒ–å™¨è¦åˆå§‹åŒ–å•Šï¼Œå¦åˆ™æ¯æ¬¡è®­ç»ƒéƒ½æ˜¯åŸºäºä¸Šä¸€æ¬¡è®­ç»ƒç»“æœè¿›è¡Œä¼˜åŒ–ï¼Œæ•ˆæœç”šå¾®
        optimizer.zero_grad()

        # è·å–è®­ç»ƒæ•°æ®
        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'], config.get('device'))

        # å‰å‘ä¼ æ’­è®¡ç®—æ¦‚ç‡çŸ©é˜µä¸loss
        logits, loss = model(xs, targets=ys)

        # åå‘ä¼ æ’­æ›´æ–°æƒé‡å‚æ•°ï¼Œæ›´æ–°å­¦ä¹ ç‡ä¼˜åŒ–å™¨
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        optimizer.step()

        # å¦‚æœæä¾›å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Œé‚£ä¹ˆå­¦ä¹ ç‡ä¼šé€šè¿‡è°ƒåº¦å™¨è¿›è¡Œä¿®æ”¹
        if scheduler:
            # æ¯ä¸ªbatchéƒ½æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

        # æ‰“å°log
        if epoch % config['log_interval'] == 0:
            # è®­ç»ƒæ—¶é—´
            batch_time = time.time() - start_time

            # æ‰§è¡Œè¯„ä¼°å‡½æ•°ï¼Œåœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šè®¡ç®—loss
            x = evaluate_loss(model, dataset, config)

            # Store the validation loss
            losses += [x]
            
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lr = optimizer.param_groups[0]['lr']

            # æ‰“å°è¿›åº¦æ—¥å¿—
            if print_logs:
                print(f"Epoch {epoch:5d} | train loss {x['train']:.4f} | val loss {x['val']:.4f} | lr {current_lr:.2e} | Time {batch_time:.2f}s | ETA {batch_time * (config['epochs'] - epoch)/config['log_interval']:.1f}s")

            # å°†éªŒè¯lossä¼ é€’ç»™è°ƒåº¦å™¨è¿›è¡Œè‡ªé€‚åº”è°ƒæ•´
            if scheduler and hasattr(scheduler, 'step'):
                # å¯¹äºè‡ªå®šä¹‰è°ƒåº¦å™¨ï¼Œä¼ é€’éªŒè¯loss
                if hasattr(scheduler, 'plateau_patience'):
                    scheduler.step(val_loss=x['val'])
            
            # æ—©åœæœºåˆ¶
            if x['val'] < best_val_loss:
                best_val_loss = x['val']
                patience_counter = 0
                if print_logs:
                    print(f"  âœ“ æ–°çš„æœ€ä½³éªŒè¯loss: {best_val_loss:.4f}")
            else:
                patience_counter += 1
                if patience_counter >= max_patience:
                    print(f"  âš  æ—©åœè§¦å‘ï¼éªŒè¯lossè¿ç»­{max_patience}è½®æœªæ”¹å–„")
                    break

            # é‡ç½®å¼€å§‹æ—¶é—´ï¼Œç”¨äºè®¡ç®—ä¸‹ä¸€è½®çš„è®­ç»ƒæ—¶é—´
            start_time = time.time()

    # ä¸Šé¢æ‰€æœ‰epochè®­ç»ƒç»“æŸï¼Œæ‰“å°æœ€ç»ˆçš„ç»“æœ
    print(f"\n=== è®­ç»ƒç»“æŸ ===")
    print(f"æœ€ä½³éªŒè¯loss: {best_val_loss:.4f}")
    print(f"æœ€ç»ˆéªŒè¯loss: {losses[-1]['val']:.4f}")

    # è¿”è¿˜æ¯ä¸€æ­¥losså€¼çš„åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬è¦ç”»å›¾ï¼Œè¿”è¿˜çš„æ˜¯lossè¿­ä»£çš„å›¾åƒ
    return pd.DataFrame(losses).plot()

# ==================== è®­ç»ƒç¯å¢ƒåˆå§‹åŒ– ====================

def initialize_training_environment(config_path="../config.yaml", config_manager=None):
    """åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒï¼Œè¿”å›æ‰€æœ‰å¿…è¦çš„ç»„ä»¶"""
    if config_manager is None:
        config_manager = get_config_manager(config_path)
    
    # è·å–é…ç½®
    config = config_manager.get_training_config()
    data_config = config_manager.get_data_config()
    
    # å‡†å¤‡æ•°æ®
    dataset, vocab, itos, stoi = download_and_prepare_data(
        data_file=data_config.get('data_file', 'xiyouji.txt'),
        force_download=data_config.get('force_download', False)
    )
    
    # æ›´æ–°é…ç½®ä¸­çš„è¯è¡¨å¤§å°
    config['vocab_size'] = len(vocab)
    
    # å¦‚æœä½¿ç”¨GPUï¼Œå°†æ•°æ®é›†ç§»åŠ¨åˆ°GPUä¸Šä»¥é¿å…é‡å¤ä¼ è¾“
    device = config.get('device', torch.device('cpu'))
    if device.type == 'cuda':
        print(f"å°†æ•°æ®é›†ç§»åŠ¨åˆ°GPU: {device}")
        dataset = dataset.to(device)
        # é¢„çƒ­GPUï¼Œå‡å°‘é¦–æ¬¡è¿è¡Œçš„å¼€é”€
        torch.cuda.synchronize()
        print("GPUé¢„çƒ­å®Œæˆ")
    
    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)
    
    return {
        'model': model,
        'dataset': dataset,
        'vocab': vocab,
        'itos': itos,
        'stoi': stoi,
        'config': config,
        'config_manager': config_manager
    }

# ==================== GPUä¼˜åŒ–å’Œè¾…åŠ©å‡½æ•° ====================

def setup_gpu_optimization():
    """è®¾ç½®GPUä¼˜åŒ–é…ç½®"""
if torch.cuda.is_available():
    print("æ­£åœ¨é…ç½®GPUä¼˜åŒ–è®¾ç½®...")
    # æ¸…ç©ºGPUç¼“å­˜
    torch.cuda.empty_cache()
    # è®¾ç½®ä½¿ç”¨95%çš„GPUå†…å­˜
    torch.cuda.set_per_process_memory_fraction(0.95)
    # å¯ç”¨cudnnè‡ªåŠ¨è°ƒä¼˜ï¼Œé¦–æ¬¡è¿è¡Œä¼šæ…¢ä½†åç»­ä¼šå¿«
    torch.backends.cudnn.benchmark = True
    # å¯ç”¨ç¡®å®šæ€§æ“ä½œï¼ˆå¯é€‰ï¼Œä¼šç¨å¾®å½±å“æ€§èƒ½ä½†ä¿è¯å¯é‡ç°æ€§ï¼‰
    # torch.backends.cudnn.deterministic = True
    print("GPUä¼˜åŒ–è®¾ç½®å®Œæˆ")
        return True
    return False

def print_gpu_memory_info(device):
    """æ‰“å°GPUå†…å­˜ä¿¡æ¯"""
    if device.type != 'cuda':
        return
        
        allocated = torch.cuda.memory_allocated(device) / 1024**3
        reserved = torch.cuda.memory_reserved(device) / 1024**3
        total = torch.cuda.get_device_properties(device).total_memory / 1024**3
        free = total - reserved
        print(f"GPUå†…å­˜çŠ¶æ€:")
        print(f"  å·²åˆ†é…: {allocated:.3f} GB")
        print(f"  å·²é¢„ç•™: {reserved:.3f} GB") 
        print(f"  å¯ç”¨: {free:.3f} GB")
        print(f"  æ€»è®¡: {total:.3f} GB")
        print(f"  ä½¿ç”¨ç‡: {reserved/total*100:.1f}%")
    
def print_device_info(device):
    """æ‰“å°è®¾å¤‡ä¿¡æ¯"""
    print(f"å½“å‰ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print_gpu_memory_info(device)

def save_model_and_config(model, config, optimizer, scheduler, save_dir="./model_save"):
    """ä¿å­˜æ¨¡å‹ã€é…ç½®å’Œè®­ç»ƒçŠ¶æ€"""
    os.makedirs(save_dir, exist_ok=True)
    
    print("ä¿å­˜æ¨¡å‹...")

# ä¿å­˜æ¨¡å‹æƒé‡
    model_save_path = os.path.join(save_dir, "pytorch_model.bin")
    state_dict = model.state_dict()
    # è¿‡æ»¤æ‰ç¼“å­˜ï¼Œåªä¿å­˜çœŸæ­£çš„æ¨¡å‹å‚æ•°
    filtered_state_dict = {k: v for k, v in state_dict.items() if not k.endswith('.R_cache')}
    torch.save(filtered_state_dict, model_save_path)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    config_save_path = os.path.join(save_dir, "config.json")
config_serializable = {k: v for k, v in config.items() if k != 'device'}
config_serializable['device_type'] = str(config['device'])  # ä¿å­˜è®¾å¤‡ç±»å‹çš„å­—ç¬¦ä¸²è¡¨ç¤º
    with open(config_save_path, 'w', encoding='utf-8') as f:
        json.dump(config_serializable, f, indent=2, ensure_ascii=False)

    # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
    optimizer_save_path = os.path.join(save_dir, "optimizer.pt")
torch.save(optimizer.state_dict(), optimizer_save_path)

    # ä¿å­˜è°ƒåº¦å™¨çŠ¶æ€
    if scheduler is not None:
        scheduler_save_path = os.path.join(save_dir, "scheduler.pt")
torch.save(scheduler.state_dict(), scheduler_save_path)

    print(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")

def print_final_gpu_stats(device):
    """æ‰“å°æœ€ç»ˆGPUç»Ÿè®¡ä¿¡æ¯"""
    if device.type != 'cuda':
        return
        
    print("\n=== è®­ç»ƒå®Œæˆåçš„GPUå†…å­˜çŠ¶æ€ ===")
    final_allocated = torch.cuda.memory_allocated(device) / 1024**3
    final_reserved = torch.cuda.memory_reserved(device) / 1024**3
    max_allocated = torch.cuda.max_memory_allocated(device) / 1024**3
    total = torch.cuda.get_device_properties(device).total_memory / 1024**3
    
    print(f"æœ€ç»ˆå·²åˆ†é…å†…å­˜: {final_allocated:.3f} GB")
    print(f"æœ€ç»ˆå·²é¢„ç•™å†…å­˜: {final_reserved:.3f} GB")
    print(f"æœ€å¤§å·²åˆ†é…å†…å­˜: {max_allocated:.3f} GB")
    print(f"GPUæ€»å†…å­˜: {total:.3f} GB")
    print(f"å³°å€¼ä½¿ç”¨ç‡: {max_allocated/total*100:.1f}%")

def run_training(config_path="../config.yaml", use_scheduler=None):
    """è¿è¡Œè®­ç»ƒæµç¨‹"""
    try:
        # è®¾ç½®GPUä¼˜åŒ–
        gpu_available = setup_gpu_optimization()
        
        # åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ
        print("æ­£åœ¨åˆå§‹åŒ–è®­ç»ƒç¯å¢ƒ...")
        env = initialize_training_environment(config_path)
        
        model = env['model']
        dataset = env['dataset']
        config = env['config']
        config_manager = env['config_manager']
        
        # æ˜¾ç¤ºè®¾å¤‡ä¿¡æ¯
        device = config['device']
        print_device_info(device)
        
        # æ‰“å°é…ç½®ä¿¡æ¯
        print("\nğŸ“‹ å½“å‰è®­ç»ƒé…ç½®:")
        config_manager.print_config()
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = create_optimizer(model, config_manager)
        
        # åˆ›å»ºè°ƒåº¦å™¨
        scheduler = None
        scheduler_config = config_manager.get_scheduler_config()
        
        if use_scheduler is None:
            use_scheduler = scheduler_config.get('enabled', False)
        
        if use_scheduler:
            # æ ¹æ®æ€»è®­ç»ƒæ­¥æ•°è®¾ç½®è°ƒåº¦å™¨å‚æ•°
            total_steps = config['epochs']
            scheduler = create_scheduler(optimizer, config_manager, max_steps=total_steps)
            
            print(f"è°ƒåº¦å™¨é…ç½®:")
            print(f"  é¢„çƒ­æ­¥æ•°: {scheduler_config.get('warmup_steps', 100)}")
            print(f"  æ€»è®­ç»ƒæ­¥æ•°: {total_steps}")
        
        print(f"\nä¼˜åŒ–å™¨é…ç½®:")
        print(f"  ç±»å‹: {config_manager.get_optimizer_config().get('type', 'Adam')}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.2e}")
        print(f"  ä½¿ç”¨è°ƒåº¦å™¨: {'æ˜¯' if scheduler else 'å¦'}")
        
        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹æ­£å¼è®­ç»ƒ...")
        start_training_time = time.time()
        train(model, optimizer, dataset, config, scheduler=scheduler, print_logs=True)
        training_time = time.time() - start_training_time
        
        print(f"\n=== è®­ç»ƒå®Œæˆ ===")
        print(f"æ€»è®­ç»ƒæ—¶é—´: {training_time:.2f}ç§’")
        if config['epochs'] > 0:
            print(f"å¹³å‡æ¯epochæ—¶é—´: {training_time/config['epochs']:.4f}ç§’")
        
        # ä¿å­˜æ¨¡å‹å’Œé…ç½®
        save_model_and_config(model, config, optimizer, scheduler)
        
        # æ˜¾ç¤ºæœ€ç»ˆGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        print_final_gpu_stats(device)
        
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨æ¨¡å‹è®­ç»ƒ...")
    print("=" * 60)
    
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æ§åˆ¶é…ç½®å’Œè°ƒåº¦å™¨
    import sys
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒLLMæ¨¡å‹')
    parser.add_argument('--config', default='../config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--scheduler', action='store_true', help='å¼ºåˆ¶å¯ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨')
    parser.add_argument('--no-scheduler', action='store_true', help='å¼ºåˆ¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨')
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨è°ƒåº¦å™¨
    use_scheduler = None
    if args.scheduler:
        use_scheduler = True
        print("ğŸ“Š å¼ºåˆ¶å¯ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    elif args.no_scheduler:
        use_scheduler = False
        print("ğŸ“Š å¼ºåˆ¶ç¦ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨")
    else:
        print("ğŸ“Š æ ¹æ®é…ç½®æ–‡ä»¶å†³å®šæ˜¯å¦ä½¿ç”¨è°ƒåº¦å™¨")
    
    print(f"ğŸ“‚ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    
    success = run_training(config_path=args.config, use_scheduler=use_scheduler)
    
    if success:
        print("\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("ğŸ’¡ ç°åœ¨å¯ä»¥è¿è¡Œä»¥ä¸‹å‘½ä»¤:")
        print("  - python src/infer.py    # æ¨ç†æ¼”ç¤º")
        print("  - python src/chat.py     # èŠå¤©æ¨¡å¼")
        print("ğŸ’¡ é…ç½®æ–‡ä»¶ç›¸å…³å‘½ä»¤:")
        print("  - python src/config_manager.py  # æŸ¥çœ‹é…ç½®")
        print("  - ç›´æ¥ç¼–è¾‘ config.yaml æ–‡ä»¶ä¿®æ”¹è¶…å‚æ•°")
    else:
        print("\nâŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        sys.exit(1)